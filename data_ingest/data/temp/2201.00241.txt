Batched Second-Order Adjoint Sensitivity for Reduced Space Methods François Pacaud∗Michel Schanen∗Daniel Adrian Maldonado∗Alexis Montoison† Valentin Churavy‡Julian Samaroo‡Mihai Anitescu∗ Abstract This paper presents an eﬃcient method for extracting the second-order sensitivities from a system of implicit nonlinearequationsonupcominggraphicalprocessingunits(GPU) dominated computer systems. We design a custom automatic diﬀerentiation (AutoDiﬀ) backend that targets highly parallel architectures by extracting the second-order information in batch. When the nonlinear equations are associated to a reduced space optimization problem, we leverage the parallel reverse-mode accumulation in a batched adjointadjoint algorithm to compute eﬃciently the reduced Hessian of the problem. We apply the method to extract the reduced Hessian associated to the balance equations of a power network, and show on the largest instances that a parallel GPU implementation is 30 times faster than a sequential CPU reference based on UMFPACK 1 Introduction System of nonlinear equations are ubiquitous in numerical computing. Solving such nonlinear systems typically depends on eﬃcient iterative algorithms, as for example Newton-Raphson. In this article, we are interested in the resolution of a parametric system of nonlinear equations, where the solution depends on a vector of parametersp2Rnp. These parametric systems are, in their abstract form, written as (1.1) Find xsuch thatg(x;p) = 0; where the (smooth) nonlinear function g:RnxRnp! Rnxdepends jointly on an unknown variable x2Rnx and the parameters p2Rnp. The solution x(p)of (1.1) depends implicitly on the parametersp: of particular interest are the sensitivities of the solution x(p)with relation to the parameters p. Indeed, these sensitivities can be embedded inside an optimization algorithm (if pis a design variable) or in an uncertainty quantiﬁcation scheme (if pencodes an uncertainty). It is well known that propagating the sensitivities in an iterative algorithm is nontrivial [12]. ∗Argonne National Laboratory †GERAD and Polytechnique Montréal ‡Massachusetts Institute of TechnologyNonlinear system Projection g(x;p) = 0 Reduced gradientrxg;rpg Reduced Hessian r2 xxg;r2 xpg;r2 ppgF rpF r2 ppF Figure 1: Reduced space algorithm. This article focuses on the last block, in red. If Fis an objective function, the reduced gradient rpFand the reduced Hessianr2 ppFcan be used in any nonlinear optimization algorithm. Fortunately, there is no need to do so, as we can exploitthemathematicalstructureof (1.1)andcompute directly the sensitivities of the solution x(p)using the Implicit Function Theorem . By repeating this process one more step, we are able to extract second-order sensitivities at the solution x(p). However, this operation is computationally more demanding and involves the manipulation of thirdorder tensorsr2 xxg;r2 xpg;r2 ppg. The challenge is to avoid forming explicitly such tensors by using reverse mode accumulation of second-order information, either explicitly by using the speciﬁc structure of the problem — encoded by the function g— or by using automatic diﬀerentiation. As illustrated in Figure 1, this paper covers the efﬁcient computation of the second-order sensitivities of a nonlinear system (1.1). The sparsity structure of the problemispassedtoacustomAutomaticDiﬀerentiation (AutoDiﬀ) backend that automatically generates all the intermediate sensitivities from the implementation of g(x;p). To get a tractable algorithm, we use an adjoint model implementation of the generated ﬁrst-order sensitivities to avoid explicitly forming third-order derivative tensors. As an application, we compute the reduced Hessian of the nonlinear equations corresponding to the Copyright ©2021 by SIAM Unauthorized reproduction of this article is prohibitedarXiv:2201.00241v1 [cs.MS] 1 Jan 2022power ﬂow balance equations of a power grid [29]. The problem has an unstructured graph structure, leading to some challenge in the automatic diﬀerentiation library, that we discuss extensively. We show that the reduced Hessian associated to the power ﬂow equations can be computed eﬃciently in parallel, by using batches of Hessian-vector products. The underlying motivation istoembedthereductionalgorithminareal-timetracking procedure [28], where the reduced Hessian updates have to be fast to track a suboptimal solution. In summary, we aim at devising a portable,eﬃcient, and easily maintainable reduced Hessian algorithm. To this end, we leverage the expressiveness oﬀered by the Julia programming language. Due to the algorithm’s design, the automatic diﬀerentiation backend and the reduction algorithm are transparently implemented on the GPU without any changes to the algorithm’s core implementation, thus realizing a composable software design. 1.1 Contributions Our contribution is a tractable SIMD algorithm and implementation to evaluate the reduced Hessian from a parametric system of nonlinear equations (1.1). This consists of three closely intertwined components. First, we implement the nonlinear function g(x;p)using the programming language Julia [6] and the portability layer KernelAbstractions.jl to generate abstract kernels working on various GPU architectures (CUDA, ROCm). Second, we develop a custom AutoDiﬀ backend on top of the portability layer to extract automatically the ﬁrst-order sensitivities rxg;rpgand the second-order sensitivities r2 xxg;r2 xpg;r2 ppg. Third, we combine these in an eﬃcient parallel accumulation of the reduced Hessian associated to a given reduced space problem. The accumulation involves both Hessian tensor contractions and two sparse linear solves with multiple right-hand sides. Glued together, the three components give a generic code able to extract the secondorder derivatives from a power grid problem, running in parallel on GPU architectures. Numerical experiments with Volta GPUs (V100) showcase the scalability of the approach, reaching a 30x faster computation on thelargestinstanceswhencomparedtoareferenceCPU implementation using UMFPACK. Current researches suggest that a parallel OpenMP power ﬂow implementation using multi-threading (on the CPU alone) potentially achieves a speed-up of 3 [2] or up to 7 and 70 speed-up for Newton-Raphson and batched NewtonRaphson [30], respectively. However, multi-threaded implementations are not the scope of this paper as we focus on architectures where GPUs are the dominant FLOP contributors for our speciﬁc application ofsecond-order space reduction. 2 Prior Art In this article we extract the second-order sensitivities from the system of nonlinear equations using automatic diﬀerentiation (AutoDiﬀ). AutoDiﬀ on Single Instruction, Multiple Data (SIMD) architectures alike the CUDA cores on GPUs is an ongoing research effort. Forward-mode AutoDiﬀ eﬀectively adds tangent components to the variables and preserves the computationalﬂow. Inaddition, avectormodecanbeappliedto propagatemultipletangentsordirectionalderivativesat once. The technique of automatically generating derivatives of function implementations has been investigated since the 1950s [22, 4]. Reverseor adjoint-mode AutoDiﬀ reverses the computational ﬂow and thus incurs a lot of access restrictions on the ﬁnal code. Every read of a variable becomes a write, and vice versa. This leads to application-speciﬁc solutions that exploit the structure of an underlying problem to generate eﬃcient adjoint code [8, 13, 15]. Most prominently, the reverse mode is currently implemented as backpropagation in machine learning. Indeed, the backpropagation has a long history (e.g., [9]) with the reverse mode in AutoDiﬀ being formalized for the ﬁrst time in [18]. Because of the limited size and single access pattern of neural networks, current implementations [24, 1, 16] reach a high throughput on GPUs. For the wide ﬁeld of numerical simulations, however, eﬃcient adjoints of GPU implementations remain challenging [20]. In this work we combine the advantages of GPU implementations of the gradient with the evaluation of Hessian-vector products ﬁrst introduced in [25]. Reduced-spacemethodshavebeenappliedwidelyin uncertainty quantiﬁcation and partial diﬀerential equation (PDE)-constrained optimization [7], and their applications in the optimization of power grids is known since the 1960s [11]. However, extracting the secondorder sensitivities in the reduced space has been considered tedious to implement and hard to motivate on classical CPU architectures (see [17] for a recent discussion about the computation of the reduced Hessian on the CPU). To the best of our knowledge, this paper is the ﬁrst to present a SIMD focused algorithm leveraging the GPU to eﬃciently compute the reduced Hessian of the power ﬂow equations. 3 Reduced space problem In Section 3.1 we brieﬂy introduce the power ﬂow nonlinear equations to motivate our application. We present in Section 3.2 the reduced space problem associated with the power ﬂow problem, and recall in SecCopyright ©2021 by SIAM Unauthorized reproduction of this article is prohibitedtion 3.3 the ﬁrst-order adjoint method, used to evaluate eﬃciently the gradient in the reduced space, and later applied to compute the adjoint of the sensitivities. 3.1 Presentation of the power ﬂow problem. We present a brief overview of the steady-state solution of the power ﬂow problem. The power grid can be described as a graph G=fV;Egwithnvvertices and ne edges. The steady state of the network is described by the following nonlinear equations, holding at all nodes i2V, (3.2)8 >>>< >>>:Pinj i=viX j2A(i)vj(gijcos (i j) +bijsin (i j)); Qinj i=viX j2A(i)vj(gijsin (i j) bijcos (i j)); where at node i,(Pinj iandQinj i)are respectively the active and reactive power injections; viis the voltage magnitude; ithe voltage angle; and A(i)Vis the set of adjacent nodes: for all j2A(i), there exists a line (i;j)connecting node iand nodej. The values gijand bijareassociatedwiththephysicalcharacteristicsofthe line(i;j). Generally, we distinguish the ( PV) nodes — associated to the generators — from the ( PQ) nodes comprising only loads. We note that the structure of the nonlinear equations (3.2) depends on the structure of the underlying graph through the adjacencies A( ). We rewrite the nonlinear equations (3.2) in the standard form (1.1). At all nodes the power injection Pinj ishould match the net production Pg iminus the loadPd i: (3.3) g(x;p) =2 4Pinj pv Pg+Pd pv Pinj pq+Pd pq Qinj pq+Qd pd3 5= 0withx=2 4pv pq vpq3 5: In (3.3), we have selected only a subset of the power ﬂow equations (3.2) to ensure that the nonlinear system g(x;p) = 0is invertible with respect to the state x. The unknown variable xcorresponds to the voltage angles at the PV and PQ nodes and the voltage magnitudes at the PQ nodes. However, in contrast to the variable x, we have some ﬂexibility in choosing the parameters p. In optimal power ﬂow (OPF) applications, we are looking at minimizing a given operating cost f:Rnx Rnp!R(associated to the active power generations Pg) while satisfying the power ﬂow equations (3.3). In that particular case, pis a design variable associated to the active power generations and the voltage magnitude at PV nodes: p= (Pg;vpv). We deﬁne the OPF problem as (3.4) min x;pf(x;p)subject tog(x;p) = 0:3.2 Projection in the reduced space. We note that in Equation (3.3), the functional gis continuous and that the dimension of the output space is equal to the dimension of the input variable x. Thanks to the particular network structure of the problem (encoded by the adjacencies A( )in (3.2)), the Jacobian rxgis sparse. Generally, the nonlinear system (3.3) is solved iteratively with a Newton-Raphson algorithm. If at a ﬁxed parameter pthe Jacobianrxgis invertible, we compute the solution x(p)iteratively, starting from an initial guess x0:xk+1=xk (rxgk) 1g(xk;p)for k= 1;:::;K. We know that if x0is close enough to the solution, then the convergence of the algorithm is quadratic. With the projection completed, the optimization problem (3.4) rewrites in the reduced space as (3.5) min pF(p) :=f  x(p);p  ; reducing the number of optimization variables from nx+nptonp, while at the same time eliminating all equality constraints in the formulation. 3.3 First-Order Adjoint Method. With the reduced space problem (3.5) deﬁned, we compute the reduced gradient rpFrequired for the reduced space optimization routine. By deﬁnition, as x(p)satisﬁes g(x(p);p) = 0, thechainruleyields rpF=rpf+rxf  rpxwithrpx=   rxg) 1rpg. However, evaluating the full sensitivity matrix rpxinvolves the resolution of nxlinear system. On the contrary, the adjoint method requires solving a singlelinear system. For every dual 2Rnx, we introduce a Lagrangian function deﬁned as (3.6)`(x;p;) :=f(x;p) +>g(x;p): Ifxsatisﬁesg(x;p) = 0, then the Lagrangian `(x;p;) does not depend on and we get `(x;p;) =F(p). By using the chain rule, the total derivative of `with relation to the parameter psatisﬁes dp`=  rxf rpx+rpf  +>  rxg rux+rpg  =  rpf+>rpg  +  rxf+>rxg  rpx: Weobservethatbysettingtheﬁrst-orderadjointto =  (rxg) >rxf>, the reduced gradient rpFsatisﬁes (3.7)rpF=rp`=rpf+>rpg; withevaluated by solving a single linear system. 4 Parallel reduction algorithm It remains now to compute the reduced Hessian. We present in Section 4.1 the adjoint-adjoint method and Copyright ©2021 by SIAM Unauthorized reproduction of this article is prohibiteddescribe in Section 4.2 how to evaluate eﬃciently the second-order sensitivities with Autodiﬀ. By combining together the Autodiﬀ and the adjoint-adjoint method, we devise in Section 4.3 a parallel algorithm to compute the reduced Hessian. 4.1 Second-Order Adjoint over Adjoint Method. Among the diﬀerent Hessian reduction schemes presented in [23] (direct-direct, adjoint-direct, direct-adjoint, adjoint-adjoint), the adjoint-adjoint method has two key advantages to evaluate the reduced Hessian on the GPU. First, it avoids forming explicitly the dense tensor r2 ppxand the dense matrix rpx, leading to important memory savings on the larger cases. Second, it enables us to compute the reduced Hessian slice by slice, in an embarrassingly parallel fashion. Conceptually, the adjoint-adjoint method extends the adjoint method (see Section 3.3) to compute the second-order derivatives r2f2Rnpnpof the objective function f((x(p);p). The adjoint-adjoint method computes the matrix r2fslice by slice, by using np Hessian-vector products (r2f)w(withw2Rnp). By deﬁnition of the ﬁrst-order adjoint , the derivative of the Lagrangian function (3.6) with respect to x is null: (4.8)rxf(x;p) +>rxg(x;p) = 0: Let^g(x;p;) :=rxf(x;p) +>rxg(x;p). We deﬁne a new Lagrangian associated with (4.8) by introducing two second-order adjoints z; 2Rnxand a vector w2Rnp: (4.9) ^`(x;p;w;;z; ) := (rp`)>w+ z>g(x;p) + >^g(x;p;): By computing the derivative of ^`and eliminating the terms corresponding to rxandrp, we get the following expressions for the second-order adjoints (z; ): (4.10)( (rxg)z=   rpg >w (rxg)> = (r2 xp`)w (r2 xx`)z: Then, the reduced-Hessian-vector product reduces to (4.11)  r2f  w= (r2 pp`)w+ (r2 px`)>z+ (rpg)> : Asr2`=r2f+>r2g, we observe that both Equations (4.10) and (4.11) require evaluating the product of the three tensors r2 xxg;r2 xpg;andr2 ppg, on the left with the adjoint and on the right with the vector w. Evaluating the Hessian-vector products (r2 xxf)w, (r2 xpf)wand (r2 ppf)wis generally easier, as fis a real-valued function.4.2 Second-order derivatives. To avoid forming the third-order tensors r2gin the reduction procedure presented previously in Section 4.1, we exploit the particular structure of Equations (4.10) and (4.11) to implement with automatic diﬀerentiation an adjointtangent accumulation of the derivative information. For any adjoint2Rnxand vectorw2Rnp, we build a tangentv= (z;w)2Rnx+np, withz2Rnxsolution of the ﬁrst system in Equation (4.10). Then, the adjointforward accumulation evaluates a vector y2Rnx+npas (4.12)y=>r2 xxg>r2 xpg >r2 pxg>r2 ppg v; (the tensor projection notation will be introduced more thoroughly in Section 4.2.3). We detail next how to compute the vector yby using forward-over-reverse AutoDiﬀ. 4.2.1 AutoDiﬀ. AutoDiﬀtransformsacodethatimplementsamultivariatevectorfunction y=g(x);Rn7! Rmwith inputs xand outputs yinto its diﬀerentiated implementation. We distinguish two modes of AutoDiﬀ. Applying AutoDiﬀ in forward mode generates the code for evaluating the Jacobian vector product y(1)=rg(x) x(1), with the superscript(1)denoting ﬁrst-order tangents—also known as directional derivatives. The adjoint or reverse mode , or backpropagation in machine learning, generates the code of the transposed Jacobian vector product x(1)=y(1) rg(x)T, with the subscript (1)denoting ﬁrst-order adjoints. The adjoint mode is useful for computing gradients of scalar functions (m= 1) (such as Lagrangian) at a cost of O(cost(g)). 4.2.2 Sparse Jacobian Accumulation. To extract the full Jacobian from a tangent or adjoint AutoDiﬀ implementation, we have to let x(1)andy(1)go over the Cartesian basis of RnandRm, respectively. This incurs the diﬀerence in cost for the Jacobian accumulation:O(n) cost(g)for the tangent Jacobian model and O(m) cost(g)for the adjoint Jacobian model. In our case we need the full square ( m=n) Jacobianrxgof thenonlinearfunction(1.1)toruntheNewton–Raphson algorithm. The tangent model is preferred whenever mn. Indeed, the adjoint model incurs a complete reversal of the control ﬂow and thus requires storing intermediate variables, leading to high cost in memory. Furthermore, SIMD architectures are particularly well suited for propagating the nindependent tangent Jacobian vector products in parallel [26]. Ifnbecomes larger (»1000), however, the memory requirement of all ntangents may exceed the GPU’s Copyright ©2021 by SIAM Unauthorized reproduction of this article is prohibitedFigure 2: Jacobian compression via column coloring. On the left, the original Jacobian. On the right, the compressed Jacobian. memory. Since our Jacobian is sparse, we apply the techniqueofJacobiancoloringthatcompressesindependentcolumns oftheJacobianandreducesthenumberof required seedingtangent vectors from nto the number of colorsc(see Figure 2). 4.2.3 Second-Order Derivatives. For higherorder derivatives that involve derivative tensors (e.g., Hessianr2g2Rmnn) we introduce the projection notation<   >introduced in [21] and illustrated in Figure 3 with <x(1);r2g(x);x(1)>, whereby adjoints are projected from the left to the Jacobian and tangents from the right. To compute second-order derivatives and the Hessian projections in Equation (4.12), we use the adjoint model implementation given by (4.13) y=g(x);x(1)=<y(1);rg(x)>=y(1) rg(x)T; and we apply over it the tangent model given by (4.14) y=g(x);y(1)=<rg(x);x(1)>=rg(x) x(1); yielding (4.15) y =g(x); y(2)=<rg(x);x(2)>; and x(1)=<y(1);rg(x)>; x(2) (1)=<y(1);r2g(x);x(2)>+<y(2) (1);rg(x)> : Notice that every variable has now a value component and three derivative components denoted by (1),(2), and(2) (1)amounting to ﬁrst-order adjoint, second-order tangent, and second-order tangent over adjoint, respectively. In Section 4.3, we compute the term x(2) (1)on the GPU by setting y(2) (1)= 0and extracting the result from x(2) (1)2Rn. X X = n nmFigure 3: Hessian derivative tensor projection < y(1);r2g(x);x(2)>. Notice that the Hessian slices along thendirections are symmetric. 4.3 Reduction Algorithm. We are now able to write down the reduction algorithm to compute the Hessian-vector products r2F w. We ﬁrst present a sequential version of the algorithm, and then detail how to design a parallel variant of the reduction algorithm. Algorithm 1: Reduction algorithm Data:Vectorw2Rnp SpMul:b=  rug  w; SparseSolve :(rxg)z= b; TensorProjection : Compute (yx;yp) with (4.12) and v= (z;w); SparseSolve :(rxg)> = yx; MulAdd:(r2F)w=yp+ (rpg)> ; 4.3.1 Sequential algorithm. We observe that by default the Hessian reduction algorithm encompasses four sequential steps: 1.SparseSolve : Get the second-order adjoint zby solving the ﬁrst linear system in (4.10). 2.TensorProjection : Deﬁne the tangent v:= (z;w), and evaluate the second-order derivatives using (4.12). TensorProjection returns a vectory= (yx;yp), with (4.16)8 >>>>< >>>>:yx=<>;r2 xxg;z>+<>;r2 xpg;w>+ <r2 xxf;z>+<r2 xpf;w>; yp=<>;r2 pxg;z>+<>;r2 ppg;w>+ <r2 pxf;z>+<r2 ppf;w>; with “<>" denoting the derivative tensor projection introduced in Section 4.2.3 (and illustrated in Figure 3). 3.SparseSolve : Get the second-order adjoint by solvingthesecondlinearsysteminEquation(4.10): Copyright ©2021 by SIAM Unauthorized reproduction of this article is prohibited(rxg)> = yx. 4.SpMulAdd : Compute the reduced Hessian-vector product with Equation (4.11). The ﬁrst SparseSolve diﬀers from the second SparseSolve since the left-hand side is diﬀerent: the ﬁrst system considers the Jacobian matrix (rxg), whereas the second system considers its transpose (rxg)>. To compute the entire reduced Hessian r2F, we have to letwgo over all the Cartesian basis vectors ofRnp. The parallelization over these basis vectors is explained in the next paragraph. 4.3.2 Parallel Algorithm. Instead of computing the Hessian vector products (r2F)w1;   ;(r2F)wn one by one, the parallel algorithm takes as input abatchofNvectorsW =  w1;   ;wN  and evaluates the Hessian-vector products  (r2F)w1;   ;(r2F)wN  in a parallel fashion. By replacing respectively the SparseSolve and TensorProjection blocks by BatchSparseSolve andBatchTensorProjection , we get the parallel reduction algorithm presented in Algorithm 2 (and illustrated in Figure 4). On the contrary to Algorithm 1, the block BatchSparseSolve solves a sparse linear system with multiple right-hand-sides B= (rpg)W, and the block BatchTensorProjection runs the Autodiﬀ algorithm introduced in Section 4.2 in batch. As explained in the next section, both operations are fully amenable to the GPU. Algorithm 2: Parallel reduction algorithm Data:Nvectorsw1;   ;wN2Rnp BuildW= (w1;   ;wN);W2RnpN; SpMul:B=  rpg  W ;B2RnxN, rpg2Rnxnp; BatchSparseSolve :(rxg)Z= B; BatchTensorProjection : Compute (Yx;Yp)withV= (Z;W ); BatchSparseSolve :(rxg)>	 = Yx; SpMulAdd :(r2F)W=Yp+ (rpg)>	; 5 GPU Implementation In the previous section, we have devised a parallel algorithm to compute the reduced Hessian. This algorithm involves two key ingredients, both running in parallel: BatchSparseSolve and BatchTensorProjection . We present in Section 5.1 how to implement BatchTensorProjection on GPU by leveraging the Julia language. Then, wefocus on the parallel resolution of BatchSparseSolve in Section 5.2. The ﬁnal implementation is presented in Section 5.3. 5.1 Batched AutoDiﬀ. 5.1.1 AutoDiﬀ on GPU. Our implementation attempts to be architecture agnostic, and to this end we rely heavily on the just-in-time compilation capabilities of the Julia language. Julia has two key advantages for us: (i) it implements state-of-the-art automatic diﬀerentiation libraries and (ii) its multiple dispatch capability allows to write code in an architecture agnostic way. Combined together, this allows to run AutoDiﬀ on GPU accelerators. On the architecture side we rely on the array abstraction implemented by the packageGPUArrays.jl [5] and on the kernel abstraction layer KernelAbstractions.jl . The Julia community provides three GPU backends for these two packages: NVIDIA, AMD, and Intel oneAPI. Currently, CUDA.jl is the most mature package, and we are leveragingthisinfrastructuretorunourcodeonanx64/PPC CPU and NVIDIA GPU. In the future our solution will be rolled out transparently onto AMD and Intel accelerators with minor code changes. 5.1.2 Forward Evaluation of Sparse Jacobians. The reduction algorithm in Section 4.3 requires (i) the Jacobianrxgto form the linear system in (4.10) and (ii) the Hessian vector product of >r2gin (4.16). We use the Julia package ForwardDiff.jl [27] to apply the ﬁrst-order tangent model (4.14) by instantiating every variable as a dual type deﬁned as T1S{T,C} = ForwardDiff.Dual{T, C}} , where Tis the type ( double orfloat) and Cis the number of directions that are propagated together in parallel. This allows us to apply AutoDiﬀ both on the CPU and on the GPU in a vectorized fashion, through a simple type change: for instance, Array{TIS{T, C}}(undef, n) instantiates a vector of dual numbers on the CPU, whereas CuArray{TIS{T, C}}(undef, n) does the same on a CUDA GPU. (Julia allows us to write code where all the types are abstracted away). This, combined with KernelAbstractions.jl , allows us to write a portable residual kernel for g(x;p)that is both differentiable and architecture agnostic. By setting the number of Jacobian colors cto the parameter Cof type T1S{T,C} we leverage the GPUs by propagating the tangents in a SIMD way. 5.1.3 Forward-over-Reverse Hessian Projections.As opposed to the forward mode, generating efCopyright ©2021 by SIAM Unauthorized reproduction of this article is prohibitedW B (r2f)W z1 y1 1z2 y2 2z3 y3 3z4 y4 4 sync sync SpMulB= (rpg)W BatchSparseSolvezi= (rxg) 1bi BatchAutoDiff BatchSparseSolve i= (rxg) >yi SpMullAdd Figure 4: Parallel computation of the reduced Hessian vector products on the GPU ﬁcient adjoint code for GPUs is known to be hard. Indeed, adjoint automatic diﬀerentiation implies a reversal of the computational ﬂow, and in the backward pass every read of a variable translates to a write adjoint, and vice versa. The latter is particularly complex for parallelized algorithms, especially as the automatic parallelization of algorithms is hard. For example, an embarrassinglyparallelalgorithmwhereeachprocessreads the data of all the input space leads to a challenging race condition in its adjoint. Current state-of-the-art AutoDiﬀ tools use specialized workarounds for certain cases. However, a generalized solution to this problem does not exist. The promising AutoDiﬀ tool Enzyme [19] is able to diﬀerentiate CUDA kernels in Julia, but it is currently not able to digest all of our code. To that end, we hand diﬀerentiate our GPU kernels for the forward-over-reverse Hessian projection. We then apply ForwardDiff to these adjoint kernels to extract second-order sensitivities according to the forward-over-reverse model. Notably, our test case (see Section 3.1) involves reversing a graph-based problem (with vertices Vand edgesE). The variables of the equations are deﬁned on the vertices. To adjoin or reverse these kernels, we pre-accumulate the adjoints ﬁrst on the edges and then on the nodes, thus avoiding a race condition on the nodes. This process yields a fully parallelizable adjoint kernel. Unfortunately, current AutoDiﬀ tools are not capable of detecting such structural properties. Outside the kernels we use a tape (or stack) structure to store the values computed in the forward pass and to reuse them in the reverse (split reversal). The kernels themselves are written in joint reversal, meaning that the forward and reverse passes are implemented in one function evaluation without intermediate storage of variables in a data structure. For a more detailed introduction to writing adjoint code we recommend [14]. 5.2 Batched Sparse Linear Algebra. The block BatchSparseSolve presented in Section 4.3 requiresthe resolution of two sparse linear systems with multiple right-hand sides, as illustrated in Equation (4.10). This part is critical because in practice a majority of the time is spent inside the linear algebra library in the parallel reduction algorithm. To this end, we have wrapped the library cuSOLVER_RF in Julia to get an eﬃcient LU solver on the GPU. For any sparse matrix A2Rnn, the library cuSOLVER_RF takes as input an LU factorization of the matrix Aprecomputed on the host, and transfers it to the device. cuSOLVER_RF has two key advantages to implement the resolution of the two linear systems in BatchSparseSolve . (i) If a new matrix ~Aneeds to be factorized and has the same sparsity pattern as the original matrix A, the refactorization routine proceeds directly on the device, without any data transfer with the host (allowing to match the performance of the state-of-the-art CPU sparse library UMFPACK [10]). (ii) Once the LU factorization has been computed, the forward and backward solves for diﬀerent right-hand sides b1;   ;bNcan be computed in batch mode. 5.3 Implementation of the Parallel Reduction. By combining the batch AutoDiﬀ with the batch sparse linear solves of cuSOLVER_RF , we get a fully parallel algorithm to compute the reduced Hessian projection. We compute the reduced Hessian r2F2Rnpnpby blocks ofNHessian-vector products. If we have enough memory to set N=np, we can compute the full reduced Hessian in one batch reduction. Otherwise, we setN < n pand compute the full reduced Hessian in Nb=div(n;N) + 1batch reductions. Tuning the number of batch reductions Nis nontrivial and depends on two considerations. How eﬃcient is the parallel scaling when we run the two parallel blocks BatchTensorProjection and BatchSparseSolve ? and Are we ﬁtting into the device memory? This second consideration is indeed one of the bottlenecks of the algorithm. In fact, if we look more closely at the memory usage of the parallel reCopyright ©2021 by SIAM Unauthorized reproduction of this article is prohibitedduced Hessian, we observe that the memory grows linearly with the number of batches N. First, in the block BatchTensorProjection , we need to duplicate N times the tape used in the reverse accumulation of the Hessian in Section 5.1, leading to memory increase from O(MT)toO(MTN), withMTthe memory of the tape. The principle is similar in SparseSolve , since the second-order adjoints zand are also duplicated in batch mode, leading to a memory increase from O(2nx) toO(2nxN). This is a bottleneck on large cases when the number of variables nxis large. The other bottleneck arises when we combine together the blocks BatchSparseSolve and BatchTensorProjection . Indeed, BatchTensorProjection should wait for the ﬁrst block BatchSparseSolve to ﬁnish its operations. The same issue arises when passing the results of BatchTensorProjection to the second BatchSparseSolve block. As illustrated by Figure 4, we need to add two explicit synchronizations in the algorithm. Allowing the algorithm to run the reduction algorithm in a purely asynchronous fashion would require a tighter integration with cuSOLVER_RF . 6 Numerical experiments In this section we provide extensive benchmarking results that investigate whether the computation of the reduced Hessian r2fwith Algorithm 2 is well suited for SIMD on GPU architectures. As a comparison, we use a CPU implementation based on the sparse LU solver UMFPACK, with iterative reﬁnement disabled1 (it yields no numerical improvement, however, considerably speeds up the computation). We show that on the largest instances our GPU implementation is 30 times faster than its sequential CPU equivalent and provide a path forward to further improve our implementation. Then, we illustrate that the reduced Hessian computed is eﬀective to track a suboptimal in a real-time setting. 6.1 Experimental Setup 6.1.1 Hardware. Our workstation Moonshot is provided by Argonne National Laboratory. All the experiments run on a NVIDIA V100 GPU (with 32GB of memory) and CUDA 11.3 . The system is equipped with a Xeon Gold 6140, used to run the experiments on the CPU (for comparison). For the software, the workstation works with Ubuntu 18.04, and we use Julia 1.6 for our implementation. We rely on our packageKernelAbstractions.jl andGPUArrays.jl to generate parallel GPU code. 1We set the parameter UMFPACK_IRSTEP to 0.All the implementation is open-sourced, and an artifact is provided to reproduce the numerical results2. 6.1.2 Benchmark library. The test data represents various case instances (see Table 1) in the power grid community obtained from the open-source benchmark library PGLIB [3]. The number in the case name indicates the number of buses (graph nodes) nvand the number of lines (graph edges) nein the power grid: nxis the number of variables, while npis the number of parameters (which is also equal to the dimension of the reduced Hessian and the parameter space Rnp). Case nvnenxnp IEEE118 118 186 181 107 IEEE300 300 411 530 137 PEGASE1354 1,354 1,991 2,447 519 PEGASE2869 2,869 4,582 5,227 1,019 PEGASE9241 9,241 16,049 17,036 2,889 GO30000 30,000 35,393 57,721 4,555 Table 1: Case instances obtained from PGLIB 6.2 Numerical Results CasesDimensions W2RnpNB2RnxNr2f2Rnpnp IEEE118 107N 181N 107107 IEEE300 137N 530N 137137 PEGASE1354 519N 2;447N 519519 PEGASE2869 1;019N 5;227N 1;0191;019 PEGASE9241 17;036N 17;036N 2;8892;889 GO30000 30;000N 35;393N 4;5554;555 Table 2: Size of key matrices (seed matrix W, multiple right-hand sides B, and ﬁnal reduced Hessian r2F) for a batch size of N. On GO30000, instantiating the three matricesW;B;r2FforN= 256already takes 286MB in the GPU memory. . 6.2.1 Benchmark reduced Hessian evaluation. For the various problems described in Table 1, we benchmarked the computation of the reduced Hessian r2Ffor diﬀerent batch sizes N. Each batch computes Ncolumns of the reduced Hessian (which has a ﬁxed size ofnpnp). Hence, the algorithm requires Nb= div(np;N) + 1number of batches to evaluate the full Hessian. In Figure 5, we compare on various instances (see Table 2) the reference CPU implementation together with the full reduced Hessian computation r2Fon the GPU (with various batch sizes N). The ﬁgure is 2available on https://github.com/exanauts/Argos.jl/ tree/master/papers/pp2022 Copyright ©2021 by SIAM Unauthorized reproduction of this article is prohibiteddisplayed in log-log scale, to better illustrate the linear scaling of the algorithm. In addition, we scale the time takenbythealgorithmontheGPUbythetimetakento compute the full reduced Hessian on the CPU: a value below 1 means that the GPU is faster than the CPU. We observe that the larger the number of batches N, the faster the GPU implementation is. This proves that the GPU is eﬀective at parallelizing the reduction algorithm, withascalingalmostlinearwhenthenumber of batches is small ( N < 32 = 25). However, we reach the scalability limit of the GPU as we increase the number of batches N(generally, when N256 = 28). Comparing to the CPU implementation, the speed-up is not large on small instances ( 2 for IEEE118 and IEEE300), but we get up to a 30 times speed-up on the largest instance GO30000, when using a large number of batches. 2223242526272829 Batch size N102 101 100Ratio (GPU / CPU) case30000 case1354 case9241pegase case2869 case118 case300 Figure 5: Parallel scaling of the total reduced Hessian accumulationr2Fwith batch size N: A ratio value <1indicates a faster runtime compared with that of UMFPACK and AutoDiﬀ on the CPU in absolute time. The dotted lines indicate the linear scaling reference. Lower values imply a higher computational intensity. Figure 6 shows the relative time spent in the linear algebra and the automatic diﬀerentiation backend. On theCPU,weobservethatUMFPACKisveryeﬃcientto perform the linear solves (once the iterative reﬁnement is deactivated). However, a signiﬁcant amount of the total running time is spent inside the AutoDiﬀ kernel. We get a similar behavior on the GPU: the batched automaticdiﬀerentiationbackendleadstoasmallerspeedup than the linear solves, increasing the fraction of the total runtime spent in the block BatchAutoDiff . 6.2.2 Discussion. Our analysis shows that the reduced Hessian scales with the batch size, while hitting an utilization limit for larger test cases. Our kernels may still have potential for improvement, thus further 010203040Absolute time (ms)Time decomposition for batch of size N Forward solve AutoDiff Backward solve CPU (1) 4 8 16 32 64 128 256 512 Batch size N020406080100Relative time (%)Figure 6: Decomposition of the runtime against the number of batch N, on case PEGASE 9241. N= 1corresponds to the CPU implementation. The derivative computation is the dominant kernel. improving utilization scaling as long as we do not hit the memory capacity limit. However, the sparsity of the powerﬂowproblemsrepresentsaworst-caseproblemfor SIMD architectures, common in graph-structured applications. Indeed, in contrast to PDE-structured problems, graphs are diﬃcult to handle in SIMD architectures because of their unstructured sparsity patterns. 6.3 Real-time tracking algorithm. Finally, we illustrate the beneﬁts of our reduced Hessian algorithm by embedding it in a real-time tracking algorithm. Letwt= (Pd t;Qd t)be the loads in (3.3), indexed by timetand updated every minute. In that setting, the reducedspaceproblemisparameterizedbytheloads wt: (6.17) min ptF(pt;wt) :=f  x(pt);pt;wt  : For all time t, the real-time algorithm aims at tracking the optimal solutions p? tassociated with the sequence of problems (6.17). To achieve this, we update the tracking point ptat every minute, by exploiting the curvature information provided by the reduced Hessian. The procedure is the following: •Step 1: For new loads wt= (Pd t;Qd t), compute the reduced gradient gt=rpF(pt;wt)and the reduced Hessian Ht=r2 ppF(pt;wt)using Algorithm 2. •Step 2: Update the tracking control ptwith pt+1=pt+dt, wheredtis a descent direction computed as solution of the dense linear system (6.18) Htdt= gt: Copyright ©2021 by SIAM Unauthorized reproduction of this article is prohibitedIn practice, we use the dense Cholesky factorization implemented in cuSOLVER to solve the dense linear system (6.18) eﬃciently on the GPU. We compare the tracking controls fptgt=1;   ;Twith the optimal solutions fp? tgt=1;   ;Tassociated to the sequence of optimization problems (6.17). Note that solving each (6.17) to optimality is an expensive operation, involvingcallinganonlinearoptimizationsolver. Onthe contrary, the real-time tracking algorithm involves only (i) updating the gradient and the Hessian for the new loadswtand (ii) solving the dense linear system (6.18). Figure 7: Performance of the real-time tracking algorithm on PEGASE1354, compared with the optimal solutions. The real-time algorithm is applied every minute, during one hour. The ﬁrst plot shows the evolution of the operating cost along time, whereas the second plot shows the evolution of the absolute diﬀerence between the tracking control ptand the optimum p? t. We depict in Figure 7 the performance of the realtime tracking algorithm, compared with an optimal solution computed by a nonlinear optimization solver. In the ﬁrst subplot, we observe that the operating cost associated tofptgtis close to the optimal cost associated tofp? tgt. The second subplot depicts the evolution of the absolute diﬀerence jpt p? tj, component by component. We observe that the diﬀerence remains tractable: themedian(Quantile50%)isalmostconstant, andclose to10 2(which in our case is not a large deviation from the optimum) whereas the maximum diﬀerence remains below 0:5. At each time t, the real-time algorithm takes inaverage 0:10stoupdateptontheGPU(with N= 256 batches), comparing to 2:22s on the CPU (see Table 3). We achieve such a 20 times speed-up on the GPU as (i) the evaluation of the reduced Hessian is faster on the GPU (ii) we do not have any data transfer betweenthe host and the device to perform the dense Cholesky factorization with cusolver . Hence, this real-time use case leverages the high parallelism of our algorithm to evaluate the reduced Hessian. Step 1 (s) Step 2 (s) Total (s) CPU 1.41 0.81 2.22 GPU 0.05 0.05 0.10 Table 3: Time to update the tracking point ptfor case1354pegase with the real-time algorithm, on the CPU and on the GPU. 7 Conclusion In this paper we have devised and implemented a practical batched algorithm (see Algorithm 2) to extract on SIMDarchitecturesthesecond-ordersensitivitiesfroma system of nonlinear equations. Our implementation on NVIDIAGPUsleveragestheprogramminglanguageJulia to generate portable kernels and diﬀerentiated code. We have observed that on the largest cases the batch code is 30x faster than a reference CPU implementation using UMFPACK. This is important for upcoming large-scale computer systems where availability of general purpose CPUs is very limited. We have illustrated the interest of the reduced Hessian when used inside a real-time tracking algorithm. Our solution adheres to the paradigm of diﬀerential and composable programming, leveragingthebuilt-inmetaprogrammingcapabilities of Julia. In the future, we will investigate extending the method to other classes of problems (such as uncertainty quantiﬁcation, optimal control, trajectory optimization, or PDE-constrained optimization). Acknowledgments This research was supported by the Exascale Computing Project (17-SC-20-SC), a joint project of the U.S. Department of Energy’s Oﬃce of Science and National Nuclear Security Administration, responsible for delivering a capable exascale ecosystem, including software, applications, and hardware technology, to support the nation’s exascale computing imperative. References [1] M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro, G. S. Corrado, A. Davis, J. Dean, M. Devin, S. Ghemawat, I. Goodfellow, A. Harp, G. Irving, M. Isard, Y.Jia, R.Jozefowicz, L.Kaiser, M.Kudlur, J.Levenberg, D. Mané, R. Monga, S. Moore, D. Murray, C.Olah, M.Schuster, J.Shlens, B.Steiner, I.Sutskever, K. Talwar, P. Tucker, V. Vanhoucke, V. Vasudevan, F. Viégas, O. Vinyals, P. Warden, M. Wattenberg, Copyright ©2021 by SIAM Unauthorized reproduction of this article is prohibitedM. Wicke, Y. Yu, and X. Zheng. TensorFlow: Largescale machine learning on heterogeneous systems, 2015. [2] A. Ahmadi, S. Jin, M. C. Smith, E. R. Collins, and A. Goudarzi. Parallel power ﬂow based on openmp. In2018 North American Power Symposium (NAPS) , pages 1–6. IEEE, 2018. [3] S. Babaeinejadsarookolaee, A. Birchﬁeld, R. D. Christie, C. Coﬀrin, C. DeMarco, R. Diao, M. Ferris, S. Fliscounakis, S. Greene, R. Huang, et al. The power grid library for benchmarking AC optimal power ﬂow algorithms. arXiv preprint arXiv:1908.02788 , 2019. [4] L.Bedaetal. Programsforautomaticdiﬀerentiationfor themachinebesm. Precise Mechanics and Computation Techniques, Academy of Science, Moscow , 1959. [5] T. Besard, C. Foket, and B. De Sutter. Eﬀective extensible programming: unleashing Julia on GPUs. IEEE Transactions on Parallel and Distributed Systems, 30(4):827–841, 2018. [6] J.Bezanson, A.Edelman, S.Karpinski, andV.B.Shah. Julia: A fresh approach to numerical computing. SIAM Rev., 59(1):65–98, 2017. [7] L. T. Biegler, O. Ghattas, M. Heinkenschloss, and B. van Bloemen Waanders. Large-scale PDEconstrained optimization: an introduction. In LargeScale PDE-Constrained Optimization , pages 3–13. Springer, 2003. [8] J. Blühdorn, N. R. Gauger, and M. Kabel. AutoMat – automatic diﬀerentiation for generalized standard materials on gpus. arXiv preprint arXiv:2006.04391 , 2020. [9] A. E. Bryson and W. F. Denham. A steepest-ascent method for solving optimum programming problems. Journal of Applied Mechanics , 29:247, 1962. [10] T. A. Davis. Algorithm 832: UMFPACK v4.3— an unsymmetric-pattern multifrontal method. ACM Trans. Math. Software , 30(2):196–199, 2004. [11] H. W. Dommel and W. F. Tinney. Optimal power ﬂow solutions. IEEE Transactions on power apparatus and systems, (10):1866–1876, 1968. [12] J. C. Gilbert. Automatic diﬀerentiation and iterative processes. Optimization methods and software , 1(1):13– 21, 1992. [13] M. Grabner, T. Pock, T. Gross, and B. Kainz. Automatic diﬀerentiation for GPU-accelerated 2d/3d registration. In Advances in automatic diﬀerentiation , pages 259–269. Springer, 2008. [14] A. Griewank and A. Walther. Evaluating derivatives: principles and techniques of algorithmic diﬀerentiation . SIAM, 2008.[15] J. C. Hückelheim, P. D. Hovland, M. M. Strout, and J.D. Müller. Parallelizable adjoint stencil computations using transposed forward-mode algorithmic diﬀerentiation.Optimization Methods and Software , 33(4-6):672– 693, 2018. [16] M. Innes. Flux: Elegant machine learning with julia. Journal of Open Source Software , 3(25):602, 2018. [17] J. Kardos, D. Kourounis, and O. Schenk. Reducedspace interior point methods in power grid problems. arXiv preprint arXiv:2001.10815 , 2020. [18] S. Linnainmaa. Taylor expansion of the accumulated rounding error. BIT Numer. Math. , 16(2):146–160, 1976. [19] W. S. Moses and V. Churavy. Instead of rewriting foreign code for machine learning, automatically synthesize fast gradients. In Advances in Neural Information Processing Systems , volume 33, pages 12472– 12485. 2020. [20] W. S. Moses, V. Churavy, L. Paehler, J. Hückelheim, S. H. K. Narayanan, M. Schanen, and J. Doerfert. Reverse-mode automatic diﬀerentiation and optimization of GPU kernels via Enzyme. In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis , SC ’21, New York, NY, USA, 2021. Association for Computing Machinery. [21] U.Naumann. The Art of Diﬀerentiating Computer Programs: An Introduction to Algorithmic Diﬀerentiation . SIAM, 2012. [22] J. F. Nolan. Analytical diﬀerentiation on a digital computer . PhD thesis, Massachusetts Institute of Technology, 1953. [23] D. Papadimitriou and K. Giannakoglou. Direct, adjoint and mixed approaches for the computation of hessian in airfoil design problems. International journal for numerical methods in ﬂuids , 56(10):1929–1943, 2008. [24] A. Paszke et al. PyTorch: An imperative style, highperformance deep learning library. In Advances in Neural Information Processing Systems 32 , pages 8024– 8035. Curran Associates, Inc., 2019. [25] B. A. Pearlmutter. Fast exact multiplication by the hessian. Neural computation , 6(1):147–160, 1994. [26] J. Revels, T. Besard, V. Churavy, B. D. Sutter, and J. P. Vielma. Dynamic automatic diﬀerentiation of GPU broadcast kernels. CoRR, abs/1810.08297, 2018. [27] J. Revels, M. Lubin, and T. Papamarkou. Forwardmode automatic diﬀerentiation in Julia. arXiv preprint arXiv:1607.07892 , 2016. Copyright ©2021 by SIAM Unauthorized reproduction of this article is prohibited[28] Y. Tang, K. Dvijotham, and S. Low. Real-time optimal power ﬂow. IEEE Transactions on Smart Grid , 8(6):2963–2973, 2017. [29] W. F. Tinney and C. E. Hart. Power ﬂow solution by Newton’s method. IEEE Transactions on Power Apparatus and systems , (11):1449–1460, 1967. [30] Z. Wang, S. Wende-von Berg, and M. Braun. Fast parallel Newton–Raphson power ﬂow solver for large number of system calculations with CPU and GPU. Sustainable Energy, Grids and Networks , 27:100483, Sep 2021. The submitted manuscript has been created by UChicago Argonne, LLC, Operator of Argonne National Laboratory (“Argonne"). Argonne, a U.S. Department of Energy Oﬃce of Science laboratory, is operated under Contract No. DE-AC02-06CH11357. The U.S. Government retains for itself, and others acting on its behalf, a paid-up nonexclusive, irrevocable worldwide license in said article to reproduce, prepare derivative works, distribute copies to the public, and perform publicly and display publicly, by or on behalf of the Government. The Department of Energy will provide public access to these results of federally sponsored research in accordance with the DOE Public Access Plan. http://energy.gov/downloads/doe-public-access-plan. Copyright ©2021 by SIAM Unauthorized reproduction of this article is prohibited