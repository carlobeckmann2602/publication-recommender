arXiv:2312.11540v1 [cs.LG] 16 Dec 2023On the Trade-oﬀ between the Number of Nodes and the Number of Trees in a Random Forest Tatsuya Akutsu∗1, Avraham A. Melkman2, and Atsuhiro Takasu3 1Bioinformatics Center, Institute for Chemical Research, Kyoto U niversity, Japan 2Department of Computer Science, Ben-Gurion University of the Ne gev, Israel 3National Institute of Informatics, Chiyoda-ku, Tokyo, Japan January 5, 2024 Abstract In this paper, we focus on the prediction phase of a random forest and study the problem of representing a bag of decision trees using a smaller bag of decision tr ees, where we only consider binary decision problems on the binary domain and simple decision trees in which an internal node is limited to querying the Boolean value of a single variable. As a main result, we show that the majority function of nvariables can be represented by a bag of T(< n) decision trees each with polynomial size if n−Tis a constant, where nandTmust be odd (in order to avoid the tie break). We also show that a bag of ndecision trees can be represented by a bag of T decision trees each with polynomial size if n−Tis a constant and a small classiﬁcation error is allowed. A related result on the k-out-of-nfunctions is presented too. Keywords: Random forest, decision tree, majority function, Boolean functio n. 1 Introduction Oneof the major machine learning models is a random forest. T he model was proposedby Breiman in 2001 [4], and is represented as a bag of trees. When used for classiﬁcation tasks its decision is the majority vote of the outputs of classiﬁcation trees, wherea s for regression tasks its prediction is the average over the outputs of regression trees. In spite of the simplicity of the model, its predictive accuracy is high in practice. Consequently, it has been appl ied to a number of classiﬁcation and regression tasks [6], and it remains a strong option even as v arious kinds of deep neural networks (DNN) have been put forward for these tasks. For example, Xu e t al. compared DNNs and decision forests and reported that deep forests achieved better resu lts than DNNs when the training data were limited [17]. Random forests are also used as a building block for deep networks. Zhou and Feng proposed a method to exploit random forests in deep netw orks [18]. Because of its strong predictive power, the learnability of the random forest has been extensively studied. For example, Breiman showed that the generalization error of a random for est depends on the strength of the individual trees in the forest and their correlations [4]. L orenzen et al. analyzed the generalization error [11] from a PAC (Probably Approximately Correct) lear ning viewpoint [16]. Biau showed that the rate of convergence depends only on the number of str ong features [3]. Oshiro et al. empirically studied the relations between the number of tre es and the prediction accuracy [13]. ∗Corresponding author. Partially supported by JSPS KAKENHI #JP22H00532 and #JP22K19830. 1Recently, Audemard et al. studied the complexity of ﬁnding s mall explanations on random forests [2]. From a theoretical computer science perspective, extensiv e studies have been doneon the circuit complexity [1, 7, 8, 9, 14, 15] and the decision tree complexi ty [5, 12] for realizing the majority function, andmoregenerally, linearthresholdfunctions. For example, ChistopolskayaandPodolskii studied lower bounds on the size of the decision tree for repr esenting the threshold functions and the parity function [5]. In particular, they showed an n−o(1) lower bound when each node in a decision tree is a Boolean function of two input variables, wherenis the number of variables. Extensive studies have also been done on representing major ity functions of larger fan-in by the majority of majority functions of lower fan-in [1, 7, 9]. Since it is empirically known that the number of trees aﬀects t he prediction performance [13], it is of interest to theoretically study the trade-oﬀ between t he number of trees and the size of trees. However, to our knowledge, there is almost no theoretical st udy on the size and number of decision trees in a random forest. Recently, Kumano and Akutsu studied embedding of random for ests and binary decision diagrams into layered neural networks with sigmoid, ReLU, and similar activation functions [10]. They also showed that Ω/parenleftBigg/parenleftbigg2n √n/parenrightbigg2/(T+1)/parenrightBigg nodes are needed to represent the majority function ofnvariables using a random forest consisting of Tdecision trees. However, deriving an upper bound on the number of nodes in a random forest was left as an op en problem, without specifying the target function. The main purpose of this paper is to part ially answer this (a bit ambiguous) question. In this paper, we focus on the prediction phase of r andom forests for binary classiﬁcation problems with binary input variables. That is, we consider b inary classiﬁcation problems for which the decision is made by the majority vote of a bag of decision t rees. Furthermore, we consider a simple decision tree model in which each internal node asks w hether some input variable has value 1 or 0. Since we focus on the prediction phase of a random fores t, we use “random forest” to mean a classiﬁer whose decision is made according to the majority vote of the decision trees in the forest, as in [2]. We show that the majority function on n= 2m−1 variables can be represented by a bag of T (< n) trees in which the maximum size of each tree is O(n((n−T)/2)+1), where n−Tis assumed to be a constant. This result means that the size of the transforme d trees is polynomially bounded. We also consider more generally the problem of transforming a g iven bag of ntrees to a bag of T(< n) trees. However, the approach that was used for the majority f unction cannot be directly applied to the general problem. We, therefore, modify the general pr oblem to the task of transforming a given forest of ntrees to a forest with fewer trees that is allowed to make smal l errors, in the sense that the total weight of the input vectors that give inconsis tent outputs divided by the total weight of all input vectors is small (i.e., we consider an arbitrary probability distribution on samples). Assuming that candKare ﬁxed positive integers, we show that for a bag of 2 m−1 decision trees each with size at most r, there exists a bag of 2 m−1−2cdecision trees such that the size of each tree isO(r(2K+11)c) and the error is at mostc 2K. In addition, as part of the derivation of our results on the majority function, we show that the k-out-of-n(majority) function, which is often referred to as a threshold function, can be represented by a bag of ndecision trees each of size O(n|m−k|+1). 2 Preliminaries In this paper we consider simple decision trees over binary d omains and bags of simple decision trees, deﬁned as follows (see also Fig. 1(A) and (B)). 2x1=1 x1=0 x2=1 1x2=0 x3=0 x3=1 1 0x2=1 x2=0 0 1x3=1 0x3=0 1x1=1 0x1=0 1x2=1 0x2=0 1x3=1 0x3=0 1x4=1 0x4=0 1x5=1 0x5=0(A) (B)T1 T2 T3 T4 T5 Figure 1: (A) Decision tree representing the majority funct ion on 3 variables, and (B) Random forest (bag of trees) representing the majority function on 5 variables. Deﬁnition 1. Adecision tree is a rooted tree in which a query is assigned to each internal n ode. The tree is simple if all queries are limited to asking the valu e of one input variable from a set of Boolean input variables X={x1,...,x n}, and the leaves carry labels that are either 0 or 1. The output for a given sample x= (x1,...,x n)is given by the label of the leaf that is reached from the root by answering the queries. The size of a decision tree is th e number of nodes in the tree. Deﬁnition 2. Abag of decision trees is a collection of N= 2M−1decision trees T1,...,T Non a set of Boolean variables X={x1,...,x n}which on a given input returns the decision arrived at by a majority of the trees. In the coming two sections we focus on the representation of t he majority function, Maj, which returns 1 if the majority of its n= 2m−1 (Boolean) variables has value 1, by a bag of trees. That task is very easy when using a bag of ntrees: simply set Ti=xi. Representing it with a bag of less than ntrees takes more doing, which is one of the main subjects of th is paper. A typeof bagthat will beespecially useful inthe following i s one that uses Ntrees to implement thek-out-of-nfunction Maj(k;x1,...,x n): the bag returns 1 if and only if at least kamong its ninput values are 1. We denote this type of bag by CN(k;x1,...,x n), or more concisely where warranted by CN(k;x).Cis mnemonic for Choose bag , as it returns 1 if and only if it is possible to choose kinput variables that have the value 1. For literals z1,z2···,zn,z1z2···znrepresents z1∧z2∧···∧zn. 3 Bag of trees for the k-out-of-nfunction In this section, we present a method for constructing a bag of decision trees for representing the k-out-of-nfunction. This method will be used as a kind of subroutine for the main problem. We begin by deﬁning two types of (sub)trees that will appear i n the implementation. We deﬁne these trees by describing which inputs they decide to accept , or by the corresponding Boolean formula. 1. The (sub)tree Lℓ(i) accepts bif and only if bi= 0 and it is the leftmost, or second leftmost, or ..., the ℓ-th leftmost 0 in b. I.e., there are at most 1 ≤p≤ℓindicesi1≤... < i p=i such that bi1=...=bip= 0. 3For example, suppose that n= 3 (or larger). Then, we have L1(3) = x1∧x2∧x3, L2(3) = ( x1∧x2∧x3)∨(x1∧x2∧x3)∨(x1∧x2∧x3). Note that all of x1,...,x biappear in each conjunction. Then, we can construct a decisio n tree by simply branching on x1at the root, on x2at the depth 1 nodes, on x3at the depth 2 nodes, and so on. Since it is enough to consider at most ℓ−1/summationdisplay i=0/parenleftbiggn i/parenrightbigg ≤O(nℓ−1) combinations and the number of nodes from the root to each lea f is at most n+1.Lℓ(i) can be represented by a decision tree of size O(nℓ), where the size of a decision tree is the number of its nodes. 2.Iℓ(i) is a (sub)tree which accepts an input bif and only if bi= 1 and bcontains at least ℓ values 1 after bi, i.e.bi1=...=biℓ= 1 for some i < i1< ... < i ℓ≤2m−1 andbj= 0 for the other js such that i < j < i ℓ. It will be helpful to visualize the interval [ ij,ij+1] as a “ones-interval”, with Iℓ(i) verifying that, starting at i, there are ℓcontiguous ones-intervals in the input. We will call these ℓcontiguous ones-intervals a “ones-interval of length ℓ”. For example, suppose that n= 5. Then, we have I1(2) = ( x2∧x3)∨(x2∧x3∧x4)∨(x2∧x3∧x4∧x5), I2(2) = ( x2∧x3∧x4)∨(x2∧x3∧x4∧x5)∨(x2∧x3∧x4∧x5). Note that each term contains consecutive variables (i.e., xi,xi+1,...,x jfor some j) beginning from the same variable xi. Then, we can construct a decision tree by simply branching o nxi at the root, on xi+1at the depth 1 nodes, on xi+2at the depth 2 nodes, and so on. Since it is enough to consider at most/parenleftbign ℓ/parenrightbig terms and the number of nodes from the root to each leaf is at most n+1.Iℓ(i) can be represented by a decision tree of size O(nℓ+1). As a function of k, the trees of Cn(k;x) are as follows: 1. Fork=m−ℓwith 1≤ℓ≤m−1, T1···TℓTℓ+1 ···T2m−1 1···1xℓ+1∨Lℓ(ℓ+1)···x2m−1∨Lℓ(2m−1) Note that Tifori≤ℓcorresponds to xi∨Lℓ(i). However, each of them is always 1, and thus 1 is given in this table. 2. Fork=m, T1···T2m−1 x1···x2m−1 3. Fork=m+ℓwith 1≤ℓ≤m−1, T1···T2m−ℓ−1 T2m−ℓ···T2m−1 Iℓ(1)···Iℓ(2m−ℓ−1)0···0 4For the ﬁrst case, Tifori≥ℓ+1 has the form xi∨Lℓ(i). To represent this Boolean function by a decision tree, it is enough to add a new root (corresponding toxi) to the decision tree for Lℓ(i), where the new root has two children: one is a leaf with label 1, the other is the root of the tree for Lℓ(i). Theorem 3. For ﬁxed k,Cn(k;x)can be represented by a bag of decision trees each of which has sizeO(n|m−k|+1). Proof.LetCn(k;x) be the bag of decision trees constructed as above. For simpl icity, we also represent the result of the majority vote of this bag on input bbyCn(k;b). We need therefore to prove that Cn(k;b) = 1 if and only there are at least k1’s inb. This is obviously true in case k=m. 1.k=m−ℓwith 1≤ℓ≤m−1. Suppose ﬁrst that bcontains at least k1’s. Ifbcontains at least m1’s thenCn(k;b) returns 1 per its deﬁnition. Suppose then that bcontains m−p1’s, with 1 ≤p≤ℓ. Noting that b contains more than ℓ0’s, adjoin the indices of the ℓleftmost ones to the set of of indices of them−p1’s. The resulting set has size m−p+ℓ≥m. Moreover, for each index iin the set xi∨Lℓ(i) has value 1 so that Tireturns 1. Hence Cn(k;b) = 1. To prove the converse assume that mof the trees in Cn(k;b) return 1 on input b. There are therefore at least m−ℓindicesij> ℓsuch that bij∨Lℓ(ij) forij. Denote by p0andq0the number of indices isuch that Lℓ(i) = 1 for i≤ℓandi > ℓ, respectively, and by p1andq1 the number of indices isuch that bi= 1 fori≤ℓandi > ℓ, respectively. Then p0+p1=ℓ, p0+q0≤ℓ(by the deﬁnition of L), andq0+q1≥m−ℓ. Hence the number of 1’s in b, p1+q1, satisﬁes p1+q1=ℓ−p0+q1≥q0+q1≥m−ℓ=k. 2.k=m+ℓwith 1≤ℓ≤m−1. Ifbcontains at least m+ℓ1’s at indices i1< ... < i m+ℓ, then each ij, 1≤j≤m, is the left endpoint of a ones-interval of length ℓinb. Therefore, for 1 ≤j≤m,Iℓ(ij) = 1 and Tij returns 1. It follows that Cn(k;b) = 1. To prove the converse assume that mof the trees in Cn(k;b) return 1 on input b. There are therefore i1<···< imsuch that Iℓ(ij) = 1, and in particular bij= 1. These, together with theℓadditional bits that make Iℓ(im) = 1 show that bcontains k1’s. Finally, it is seen from the discussions before this theorem that the size of each decision tree is O(nℓ) =O(nm−k) for 1 ≤k < m. O(1) for k=m, O(nℓ+1) =O(nk−m+1) form < k≤n. Therefore, for each k, the size of each decision tree is O(n|m−k|+1). Note that for small k, the size given by this theorem is not better than that by a nai ve construction method, in which all kcombinations of variables are represented by T1(as in Figure 1(A)), each ofT2,...,T mrepresents 1, each of Tm+1,...,T 2m−1represents 0. Analogously, for large k (close to n), the size given by this theorem is not better than that by a na ive construction method. It should also be noted that the case of k=m−ℓand the case of k=m+ℓare not symmetric even if we switch 0 and 1. 0 and 1 are interchanged. Suppose m= 3 (i.e., n= 2m−1 = 5) and 5k=m+1 = 4. In order to get a positive output, at least four input va riables should have value 1. Here, we switch 0 and 1 in the input variables and consider t he case of k=m−1 = 2. In order to get a positive output, at least 2 variables should ha ve the switched value 1. This means that at most 3 variables have the switched value 0, correspon ding to that at most 3 variables have the original value 1. This example suggests that the cases of k=m−ℓandk=m+ℓare not symmetric. 4 Majority function on 2m−1variables by 2m−1−2ctrees In this section, we present one of our main results. Theorem 4. For any positive integer constant c, the majority function on n= 2m−1variables can be represented by a bag of 2m−1−2cdecision trees where the maximum size of each tree is O(nc+1). Proof.To illustrate the construction we look ﬁrst at the simplest c ase,c= 1. Extending our notation a bit, the majority function on n= 2m−1 variables can be represented by a bag B2m−3 containing n−2 = 2m−3 trees as follows: B2m−3(x1,x2,...,x n) = ((x1x2)∧C2m−3(m−2;x3,...,x n))∨ ((x1x2)∧C2m−3(m−1;x3,...,x n))∨ ((x1x2)∧C2m−3(m−1;x3,...,x n))∨ ((x1x2)∧C2m−3(m;x3,...,x n)). (1) To describe B2m−3explicitly let T−1 i,T0 i, andT+1 ibe theith tree in the bags of 2 m−3 trees representingC2m−3(m−2;x3,...,x n),C2m−3(m−1;x3,...,x n), andC2m−3(m;x3,...,x n), respectively. We also use T−1 i,T0 i, andT+1 ito denote the Boolean functions corresponding to these tree s. Then the ith tree of B2m−3,Ti, is Ti= ((x1x2)∧T−1 i)∨((x1x2)∧T0 i)∨((x1x2)∧T0 i)∨((x1x2)∧T+1 i). For example, in case n= 5, we get 3 decisions trees whose Boolean functions are: ((x1x2)∧1)∨((x1x2)∧x3)∨((x1x2)∧x3)∨((x1x2)∧(x3x4∨x3x4x5)), ((x1x2)∧(x4∨x3x4))∨((x1x2)∧x4)∨((x1x2)∧x4)∨((x1x2)∧(x4x5)), ((x1x2)∧(x5∨x3x4x5)∨((x1x2)∧x5)∨((x1x2)∧x5)∨((x1x2)∧0). The ﬁrst two levels of the tree Ti, therefore, branch on the values of x1andx2, respectively, with the 4 trees T−1 i,T0 i,T0 i, andT+1 igrafted onto the 4 outcomes. According to Section 3 the sizes of T−1 i,T0 i, andT+1 iareO(n),O(1), and O(n2), respectively, so that the theorem holds in this case. In order to generalize the above discussion to a general cons tantcwe deﬁne P(r,s) forr≥s and given variables x1,...,x nby P(r,s) =/logicalordisplay {z1z2···zr|(zi=xiorzi=xifor alli) and (exactly sof the literals ziare of the form zi=xi)}, wherePis mnemonic for preﬁx. To illustrate, P(2,0) =x1x2,P(2,1) =x1x2∨x1x2, andP(2,2) = x1x2. Using this notation to generalize the discussion above, we see that the majority function on n= 2m−1 variables can be represented by 62c/logicalordisplay s=0/parenleftBig P(2c,s)∧C2m−1−2c(m−2c+s;x2c+1,x2c+2,...,x n)/parenrightBig . Here, we note: •C2m−1−2c(m−c−ℓ;x2c+1,...,x n) corresponds to the case of k=m−ℓof Section 3, •C2m−1−2c(m−c;x2c+1,...,x n) corresponds to the case of k=mof Section 3, •C2m−1−2c(m−c+ℓ;x2c+1,...,x n) corresponds to the case of k=m+ℓof Section 3. It is seen from Theorem 3 that each C2m−1−2c(m−2c+s;x2c+1,...,x n) can be represented by a bag of 2 m−1−2ctrees each of which has O(nc+1) nodes. Let T−ℓ i,T0 i, andT+ℓ idenote the ith tree (and also the corresponding Boolean function) in the bag of 2m−1−2ctrees representingC2m−1−2c(m−c−ℓ;x2c+1,...,x n),C2m−1−2c(m−c;x2c+1,...,x n), andC2m−1−2c(m−c+ ℓ;x2c+1,...,x n), respectively. Then, as in the case of c= 1, the majority function on 2 m−1 variables can be represented by the majority function on 2 m−1−2cdecision trees in which the ith decision tree, Ti, represents the following Boolean function: Ti=c/logicalordisplay ℓ=−c/parenleftBig P(2c,c+ℓ)∧Tℓ i/parenrightBig . Constructing Tias in the case c= 1 yields a tree of size O(22cnc+1) which is O(nc+1) for constant c. 5 Reducing the number of trees in a general bag of trees while allowing small errors Unfortunately, the construction given in Section 4 cannot b e used to reduce the number of trees in a general random forest (while keeping the tree size polynom ial) because conjunction or disjunction ofndecision trees each withsize smay need adecision treeof size Ω( sn), as discussedinSection 5.1. Therefore, we modify the problem by still asking for a reduct ion in the number of decision trees in a forest but permitting the transformed forest to make a sm all number of classiﬁcation errors, all the while keeping the size of each tree polynomial in n. 5.1 Basic idea Before delving into the details of the construction, we put o ur ﬁnger on the reason why extending Theorem 4 to general randomforests is diﬃcult, and then pres ent thecentral idea for circumventing this diﬃculty. It is tempting to deal with the problem of reducing the number of trees in a general bag of trees by modifying the approach that worked for the majority function. After all, the function implemented by the bag is a majority function albeit of trees rather than of variables. Consider, for example, the case c= 1. Given a bag of trees B2m−1(x) ={t1(x),...,t2m−1(x)}we would like to construct a bag with only 2 m−3 trees,B2m−3, using equation (1) while replacing the variables 7xion the right hand side by the trees ti(x): B2m−3(x) = ( (t1(x)t2(x))∧C2m−3(m−2;t3(x),...,t2m−1(x)) )∨ ( (t1(x)t2(x) )∧C2m−3(m−1;t3(x),...,t2m−1(x)) )∨ ( (t1(x)t2(x))∧C2m−3(m−1;t3(x),...,t2m−1(x)) )∨ ( (t1(x)t2(x))∧C2m−3(m;t3(x),...,t2m−1(x)) ). (2) Following the discussion of Section 3, a tree in the choose ba gC2m−3(m−2;t3(x),...,t2m−1(x)), for example, is of the form ti+2(x)∨L1(i+ 2) where L1(i+ 2) is a composition of itrees, L1(i+2) =∧i+1 j=3(tj(x)∧ti+2(x))1. However, the number of nodes in L1(i) can grow exponentially with the number of trees. Proposition 5. LetT1andT2be decision trees with s1ands2nodes, respectively. Then, the conjunction (resp., disjunction) of T1andT2can be represented by a decision tree with O(s1s2) nodes. Proof.To construct the decision tree for the conjunction of T1andT2replace each leaf of T1that has the label 1 by a copy of T2. Clearly the resulting decision tree represents the conjun ction ofT1 andT2, and it has O(s1s2) nodes. By repeatedly applying this proposition, the conjunction o fndecision trees each having O(s) nodescan berepresented by adecision tree of size O(sn). As far as we know thereis noconstruction method that requires o(sn) size. Therefore, a simple modiﬁcation of the construction would yield exponential size decision trees even for the case of c= 1. To prevent this exponential buildup in the size of the trees w e apply the construction of Section 4 to the ﬁrst Ktrees only, while the remaining trees correspond to just sin gle variables. To indicate this modiﬁcation we replace B2m−3andC2m−3byˆB2m−3andˆC2m−3: ˆB2m−3(x) = ( (t1(x)t2(x))∧ˆC2m−3(m−2;t3(x),...,t2m−1(x)) )∨ ( (t1(x)t2(x) )∧ˆC2m−3(m−1;t3(x),...,t2m−1(x)) )∨ ( (t1(x)t2(x))∧ˆC2m−3(m−1;t3(x),...,t2m−1(x)) )∨ ( (t1(x)t2(x))∧ˆC2m−3(m;t3(x),...,t2m−1(x)) ). (3) For example, if m= 6 and K= 3, then ˆC9(4;t3(x),...,t2m−1(x)) ={t3(x)∨L1(3),t4(x)∨L1(4),t5(x)∨L1(5),t6(x),...,t11(x)}. Continuing with this example, we ask for which xsuch that t1(x) =t2(x) = 1 does ˆB9(x) return the correct answer of the original bag B11? Since in this case ˆB9(x) =ˆC9(4;t3(x),...,t2m−1(x)) we see that if only 3 out of the trees t3(x),...,t11(x) return 1 then B11(x) =ˆB9(x) = 0, while if at least 5 return 1 then B11(x) =ˆB9(x) = 1 (taking into account that t1(x) =t2(x) = 1). However, if 4 trees return 1, so that B11(x) = 1, an error, i.e. ˆB9(x) = 0, can occur but only ift3(x) =t4(x) =t5(x) = 1. If ti(x) = 1 holds with probability 0.5 for each ti, and all tis are independet, the probability of t3(x) =t4(x) =t5(x) = 1 is (1 2)3and thus the average error (the ratio of the truth assignments on variables giving the incor rect output to the number of all truth assignments) is bounded by this value. Consequently, for ge neralK, the average error is bounded by(1 2)K. However, weneed tohandleageneral probability distribut ion. Therefore, in thefollowing, we give rigorous and detailed analysis of the error. 1The negation of a decision tree can be obtained by mutually ex changing the 0 and 1 labels assigned to leaves. 85.2 Details of the construction First we deﬁne our error measure. Let T= (T1,...,T n) be a bag of decision trees on the set of all binary vectors of length l,X={x1,...,xN},N= 2l, and denote by T(x) its decision on x. Let D(x) be an arbitrary probability distribution on X,/summationtext x∈XD(x) = 1. Given a Boolean function F onXwe deﬁne its error with respect to Tby err(F,T) =/summationdisplay x:F(x)/negationslash=T(x)D(x) Note that 0 ≤err(F,T)≤1. For a binary vector bof length Hand an index j∈ {1,2,...,H},b[j] denotes the jth element ofb. Furthermore, for a subset J⊆ {1,2,...,H},b[J] denotes the projection of bonto the coordinates given by J(i.e,b[J] is a bit vector of length |J|). For a set of Hdecision trees Tand an input x, letT(x) = (T1(x),T2(x),...,T H(x)). For a binary vector bof length H, we deﬁne w(b) by w(b) =/summationdisplay x∈X:T(x)=bD(x). Note that/summationtext bw(b) =/summationtext xD(x) = 1. For a binary vector b, #1(b) and #0( b) denote the number of elements with value 1 and 0, respectively. Given L≤H WL=/summationdisplay b:#1(b)=Lw(b). For an integer Land a subset J⊆ {1,2,...H}of size|J|=K≤Land a binary vector aof lengthKwe deﬁne WL J(a) by WL J(a) =/summationdisplay b:(b[J]=a)∧(#1(b)=L)w(b), respectively. Lemma 6. Leta= (1,1,...,1)be a vector of length K. Then, there exists J0⊆ {1,2,...H}of cardinality Ksuch that WL J0(a)≤/parenleftbigg/parenleftbiggL K/parenrightbigg //parenleftbiggH K/parenrightbigg/parenrightbigg WL. Proof.Let{b1,...,bF}be the set of all binary vectors of length Hsuch that #1( bi) =L, where F=/parenleftbigH L/parenrightbig . We deﬁne Wi,J(a) by WL i,J(a) =/braceleftbiggw(bi),ifbi[J] =a, 0,otherwise . Then, we have /summationdisplay J:|J|=KWL i,J(a) =/parenleftbiggL K/parenrightbigg w(bi) because #1( bi) =Land #1(a) =K. Thus, we have F/summationdisplay i=1/summationdisplay J:|J|=KWL i,J(a) =F/summationdisplay i=1/parenleftbiggL K/parenrightbigg w(bi) =/parenleftbiggL K/parenrightbiggF/summationdisplay i=1w(bi) =/parenleftbiggL K/parenrightbigg WL. 9Since there exist/parenleftbigH K/parenrightbig Js of cardinality K, there must exist J0with cardinality Ksuch that WL J0(a) =F/summationdisplay i=1WL i,J0(a)≤/parenleftbigL K/parenrightbig /parenleftbigH K/parenrightbigWL. Theorem 7. LetT= (t1,...,tn)be a bag of n= 2m−1decision trees on X, and let the size of the largest tiber. Then, for any positive integer constant K, there exists a bag of 2m−3decision trees on X,ˆB2m−3, such that each of its trees has size O(r2K+11)anderr(ˆB2m−3,T)≤1 2K. Proof.Letti(x) betheoutputof tiforan inputvector x. Toconstruct ˆB2m−3accordingtoequation (3), we need to deﬁne ﬁrst ˆC2m−3(k;t3(x),...,t2m−1(x)) fork=m−2,m−1,m. 1.ˆC2m−3(m−2;t3(x),...,t2m−1(x)) ={T− 1(x),...,T− 2m−3(x)}, where T− i(x) =/braceleftbiggt2+i(x)∨L1(2+i) ifi= 1,...,K t2+i(x) if i=K+1,...,2m−3and L1(j) =t3(x)∧t4(x)∧···∧tj−1(x)∧tj(x); 2.ˆC2m−3(m−1;t3(x),...,t2m−1(x)) ={t3(x),...,t2m−1(x)}; 3.ˆC2m−3(m;t3(x),...,t2m−1(x)) ={T+ 1(x),...T+ 2m−3(x)}, where T+ i(x) =/braceleftbiggI1(2+i) ifi= 1,...,K t2+i(x) ifi=K+1,...,2m−3and I1(j) =tj(x)∧(tj+1(x)∨···∨t2+K(x)). ThusˆB2m−3={T1(x),...,T 2m−3(x)}is deﬁned by Ti(x) = (t1(x)∧t2(x)∧T− i(x))∨(t1(x)∧t2(x)∧t2+i(x))∨(t1(x)∧t2(x)∧t2+i(x))∨ (t1(x)∧t2(x)∧T+ i(x)), and the value of ˆB2m−3on an input x,ˆB2m−3(x), is the majority vote of its decision trees, i.e. it is 1 if and only if m−1 or more of the Ti(x)s are satisﬁed. We turn next to the analysis of err(ˆB2m−3,T). To that end we introduce some notation. W(j1,j2)=/summationdisplay b:(b[1],b[2])=(j1,j2)w(b) wherew(b) =/summationtext x∈X:t(x)=bD(x). Note that W(0,0)+W(0,1)+W(1,0)+W(1,1)= 1. Leth(x) andˆh(x) be the number of decision trees in T, andˆB2m−3respectively, satisﬁed by x, i.e.h(x) =/summationtext2m−1 i=1ti(x), andˆh(x) =/summationtext2m−3 i=1Ti(x). Also, h−(x) andh+(x) are the number of decision trees T− i, andT+ i, respectively, which return 1 on x, i.e.h−(x) =/summationtext2m−1 i=1T− i(x), and h+(x) =/summationtext2m−3 i=1T+ i(x). We break our analysis into 3 cases. Caset1(x) =t2(x) = 1: Thenh(x) = 2+/summationtext2m−1 i=3ti(x), and ˆh(x) =h−(x) = min/parenleftBigg 1+2+K/summationdisplay i=3ti(x),K/parenrightBigg +2m−1/summationdisplay i=3+Kti(x) = min/parenleftBigg h(x)−1,h(x)−2+K−K+2/summationdisplay i=3ti(x)/parenrightBigg 10On the basis of these expressions we distinguish four possib ilities: •ifh(x)≥m+1 then ˆh(x)≥m−1, so that T(x) =ˆB2m−3(x) = 1. •ifh(x)≤m−1 thenˆh(x)≤m−2, so that T(x) =ˆB2m−3(x) = 0. •ifh(x) =m, and in particular T(x) = 1, then –if/summationtextK+2 i=3ti(x)< K, thenˆh(x) =m−1 so that also ˆB2m−3(x) = 1. –if/summationtextK+2 i=3ti(x) =K, thenˆh(x) =m−2 so that ˆB2m−3(x) = 0. Only the last case leads to an erroneous value of ˆB2m−3(x), and that error occurs only when |{i|(i≥3)∧(ti(x) = 1)}|=m−2 and (t3(x),t4(x),···,tK+2(x)) = (1,1,···,1), which corresponds to the case of L=m−2 andH= 2m−3 of Lemma 6. Therefore, the Lemma ensures that there exists a permutation of t3,...,tnsuch that the error is at most /parenleftbigg/parenleftbiggm−2 K/parenrightbigg //parenleftbigg2m−3 K/parenrightbigg/parenrightbigg W(1,1)=/parenleftbiggm−2 2m−3·m−3 2m−4···m−1−K 2m−2−K/parenrightbigg W(1,1) ≤1 2KW(1,1). Case (t1(x) = 1andt2(x) = 0) or (t1(x) = 0andt2(x) = 1): Thenh(x) = 1+ˆh(x) = 1+/summationtext2m−1 i=3ti(x), so that h(x)≥mif and only if ˆh(x)≥m−1. Caset1(x) =t2(x) = 0: Thenh(x) =/summationtext2m−1 i=3ti(x), and ˆh(x) =h+(x) = max/parenleftBigg −1+2+K/summationdisplay i=3ti(x),0/parenrightBigg +2m−1/summationdisplay i=3+Kti(x) = max/parenleftBigg h(x)−1,h(x)−K+2/summationdisplay i=3ti(x)/parenrightBigg . On the basis of these expressions we distinguish four possib ilities: •ifh(x)≥mthenˆh(x)≥m−1, so that T(x) =ˆB2m−3(x) = 1. •ifh(x)≤m−2 thenˆh(x)≤m−2, so that T(x) =ˆB2m−3(x) = 0. •ifh(x) =m−1, and in particular T(x) = 1, then –if/summationtextK+2 i=3ti(x) = 0, then ˆh(x) =m−1 so that also ˆB2m−3(x) = 1. –if/summationtextK+2 i=3ti(x)>0, thenˆh(x)< m−1 so that ˆB2m−3(x) = 0/ne}ationslash=T(x). In this case, therefore, an error occurs only when |{i|(i≥3)∧(ti(x) = 0)}|=m−2 and t3(x)t4(x)···t3+K−1(x) = 00···0. This corresponds to the case of L=m−2 andH= 2m−3 of Lemma 6, by exchanging the roles of 0 and 1. Hence, the Lemma ensures the existence of a permutation of t3,...,tnsuch that the error is at most /parenleftbigg/parenleftbiggm−2 K/parenrightbigg //parenleftbigg2m−3 K/parenrightbigg/parenrightbigg W(0,0)=/parenleftbiggm−2 2m−3·m−3 2m−4···m−1−K 2m−2−K/parenrightbigg W(0,0) ≤1 2KW(0,0). 11Since the three cases are mutually independent, the permuta tions can be done independently. We conclude that there exists a bag of 2 m−3 decision trees whose error is at most 1 2KW(1,1)+1 2KW(0,0)≤1 2K/parenleftBig W(1,1)+W(0,0)/parenrightBig ≤1 2K. Finally, we analyze the size of the resulting trees. The size can be upper-boundedas O(rY+1) if the number of ∧and∨operators for representing a Boolean function over t1,...,tncorresponding to each resulting tree is Y. For the case of t1t2, the number of operators is at most 2+ K. For the case oft1t2(resp.,t1t2), the number of operators is 2. For the case of t1t2, the number of operators is at most 1+ K. Hence, the total number of operators is (2+ K)+2+2+(1+ K)+3 = 2K+10. It is seen from Proposition 5 that the resulting upper bound of t he size of each tree is O(r2K+11). Example 8. We consider the case of m= 5andK= 3, in which In this case, the original number of trees is n= 2m−1 = 9, and the number of transformed trees is 2m−3 = 7. Then, the transformed trees are as in Table 1 (before permutations ). Note that t3∨L1(3)is always 1 and I1(5)is always 0. Table 1: Representation of majority function on 9 trees by 7 t rees with allowing errors, where K= 3. preﬁx T1T2T3T4T5T6T7 t1t2 t3∨t4∨t5∨t6t7t8t9 L1(3)L1(4)L1(5) t1t2(ort1t2)t3t4t5t6t7t8t9 t1t2 I1(3)I1(4)I1(5)t6t7t8t9 For example, an error for Case (A) occurs when t(x) = (1,1,1,1,1,0,0,0,0)because only three of the seven transformed trees are 1 (i.e., ˆT(x) = (1,1,1,0,0,0,0)), and an error for Case (B) occurs when t(x) = (0,0,0,0,0,1,1,1,1)because four of the seven transformed trees are 1 (i.e., ˆT(x) = (0,0,0,1,1,1,1)). It might be possible to develop a similar construction proce dure for the case of c >1 by modifying the construction in Theorem 4. However, probabil istic analysis would be quite diﬃcult. Instead, we use a simple recursive procedure. First, we redu ce the number of trees to 2 m−3 using Theorem 7. Then, we reduce the number of trees to 2 m−5 using Theorem 7 again. We repeat this procedure until the number of trees becomes 2 m−1−2c. Theorem 9. LetTbe a bag of n= 2m−1decision trees on a set of variables X={x1,...,x n′}, where the size of each decision tree is at most r. Then, for any positive integer constants candK, there exists a bag of 2m−1−2cdecision trees on Xsuch that the size of each bag is O(r(2K+11)c) and the error is at mostc 2K. Proof.First, we analyze the size of decision trees in a bag. At the ﬁr st iteration, the size increases fromrtoO(r2K+11) from Theorem 7. Then, this increased size corresponds to th e nextr. Since Kis a constant, at the second iteration, the size increases to O((r2K+11)2K+11) =O(r(2K+11)2). Since we repeat this procedure ctimes and cis a constant, the ﬁnal size is O(r(2K+11)c). 12Next, we analyze the error. At the ﬁrst iteration, the error i s at most1 2Kfrom Theorem 7. Then, we set the weight of the erroneous samples to be 0 and sca ling up the weights of the other samples so that the total weight becomes 1. Then, at the secon d iteration, the error caused by the second iteration is at most1 2K, where the actual error is not greater than this because the w eights are scaled up. Therefore, the total error is at most2 2K. Since we repeat this procedure ctime, the ﬁnal error is at mostc 2K. 6 Concluding remarks In this paper, we studied the trade-oﬀ between the number of n odes and the number of trees in a bag of decision trees for representing a given bag of ndecision trees. As a main result, we showed that ifn−Tis a constant, there exists a bag of Ttrees of polynomial size that represents the majority function of nvariables. If n−Tis not a constant, the derived size is exponential, which is consistent with an exponential lower bound given in [10]. However, the gap between the upper and lower bounds is still large. Therefore, shortening the g ap is left as an open problem. For the general case of representing a given bag of ndecision trees using a bag of Tdecision trees, we only showed an existence of a bag of polynomial size decision trees allowing a small classiﬁcation error for a constant n−T. It is unclear whether or not there exists such a bag with no error, and thus is left as an open problem. References [1] Kazuyuki Amano and Masafumi Yoshida. Depth two ( n-2)-majority circuits for n-majority. IEICE Trans. Fundam. Electron. Commun. Comput. Sci. , 101-A(9):1543–1545, 2018. [2] Gilles Audemard, Steve Bellart, Louenas Bounia, Fr´ ed´ eric Koriche, Jean-Marie Lagniez, and Pierre Marquis. Trading complexity for sparsity in random f orest explanations. In Proceedings of Thirty-Sixth AAAI Conference on Artiﬁcial Intelligence (AA AI-2022) , pages 5461–5469, 2022. [3] G´ erard Biau. Analysis of a random forests model. J. Mach. Learn. Res. , 13:1063–1095, 2012. [4] Leo Breiman. Random forests. Mach. Learn. , 45(1):5–32, 2001. [5] Anastasiya Chistopolskaya and Vladimir V. Podolskii. O n the decision tree complexity of threshold functions. Theory Comput. Syst. , 66(6):1074–1098, 2022. [6] Manuel Fern´ andez Delgado, Eva Cernadas, Sen´ en Barro, and Dinani Gomes Amorim. Do we need hundreds of classiﬁers to solve real world classiﬁcati on problems? J. Mach. Learn. Res. , 15(1):3133–3181, 2014. [7] Christian Engels, Mohit Garg, Kazuhisa Makino, and Anup Rao. On expressing majority as a majority of majorities. SIAM J. Discret. Math. , 34(1):730–741, 2020. [8] Mikael Goldmann, Johan H˚ astad, and Alexander A. Razbor ov. Majority gates VS. general weighted threshold gates. Comput. Complex. , 2:277–300, 1992. [9] Alexander S. Kulikov and Vladimir V. Podolskii. Computi ng majority by constant depth majority circuits with low fan-in gates. Theory Comput. Syst. , 63(5):956–986, 2019. 13[10] So Kumanoand Tatsuya Akutsu. Comparison of the represe ntational power of random forests, binary decision diagrams, and neural networks. Neural Comput. , 34(4):1019–1044, 2022. [11] Stephan Sloth Lorenzen, Christian Igel, and Yevgeny Se ldin. On pac-bayesian bounds for random forests. Mach. Learn. , 108(8-9):1503–1522, 2019. [12] Fr´ ed´ eric Magniez, Ashwin Nayak, Miklos Santha, Jona h Sherman, G´ abor Tardos, and David Xiao. Improved bounds for the randomized decision tree comp lexity of recursive majority. Random Struct. Algorithms , 48(3):612–638, 2016. [13] Thais Mayumi Oshiro, Pedro Santoro Perez, and Jos´ e Aug usto Baranauskas. How many trees in a random forest? In Proceedings of 8th International Conference on Machine Learn ing and Data Mining in Pattern Recognition (MLDM 2012) , volume7376 of Lecture Notes in Computer Science, pages 154–168, 2012. [14] Kai-Yeung Siu, Vwani Roychowdhury, and Thomas Kailath .Discrete Mathematics of Neural Networks, Selected Topics . Prentice Hall, 1995. [15] Eleonora Testa, Mathias Soeken, Luca Gaetano Amar` u, W inston Haaswijk, and Giovanni De Micheli. Mapping monotone boolean functions into majority .IEEE Trans. Computers , 68(5):791–797, 2019. [16] Leslie G. Valiant. A theory of the learnable. Commun. ACM , 27(11):1134–1142, 1984. [17] Haoyin Xu, Kaleab A. Kinfu, Will LeVine, Sambit Panda, J ayanta Dey, Michael Ainsworth, Yu-Chung Peng, Madi Kusmanov, Florian Engert, Christopher M. White, Joshua T. Vogelstein, and Carey E. Priebe. When are deep networks really bet ter than decision forests at small sample sizes, and how?, 2021. [18] Zhi-Hua Zhou and Ji Feng. Deep forest: Towards an altern ative to deep neural networks. InProceedings of the Twenty-Sixth International Joint Conferen ce on Artiﬁcial Intelligence, IJCAI-17 , pages 3553–3559, 2017. 14