Fair Data Representation for Machine Learning at the Pareto Frontier Shizhou Xu shzxu@ucdavis.edu Department of Mathematics University of California Davis Davis, CA 95616-5270, USA Thomas Strohmer strohmer@math.ucdavis.edu Department of Mathematics Center of Data Science and Artificial Intelligence Research University of California Davis Davis, CA 95616-5270, USA Abstract As machine learning powered decision-making becomes increasingly important in our daily lives, it is imperative to strive for fairness in the underlying data processing. We propose a pre-processing algorithm for fair data representation via which supervised learning results in estimations of the Pareto frontier between prediction error and statistical disparity. In particular, the present work applies the optimal affine transport to approach the post-processing Wasserstein barycenter characterization of the optimal fair L2-objective supervised learning via a pre-processing data deformation. Furthermore, we show that the Wasserstein geodesics from the conditional (on sensitive information) distributions of the learning outcome to their barycenter characterize the Pareto frontier between L2-loss and the average pairwise Wasserstein distance among sensitive groups on the learning outcome. Numerical simulations underscore the advantages: (1) the pre-processing step is compositive with arbitrary conditional expectation estimation supervised learning methods and unseen data; (2) the fair representation protects the sensitive information by limiting the inference capability of the remaining data with respect to the sensitive data; (3) the optimal affine maps are computationally efficient even for high-dimensional data. Keywords: statistical parity, equalized odds, Wasserstein barycenter, Wasserstein geodesics, conditional expectation estimation 1. Introduction Our society is increasingly influenced by artificial intelligence as (direct or indirect) decisionmaking processes become more reliant on statistical inference and machine learning. The potentially significant long-term impact from sequences of automated (facilitate of) decisionmaking has brought large concerns about bias and discrimination in machine learning [5, 38]. Machine learning based on unbiased algorithms can naturally inherit the historical biases that exist in data and hence reinforce the bias via automated decision-making process [12]. One straightforward partial remedy is to exclude the sensitive variables from the data set used in the learning and decision process. But such exclusion merely eliminates disparate treatment, which refers to direct discrimination, and leaves disparate impact, which refers to unintended or indirect discrimination, remaining in both data and learning outcome [23]. ©2023 Shizhou Xu and Thomas Strohmer. License: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/ .arXiv:2201.00292v4 [stat.ML] 24 Nov 2023Xu and Strohmer Examples of the legal doctrine of disparate impact include Griggs v. Duke Powers Co. [10] and Ricci v. DeStefano [1], where the decision is based on factors that are strongly correlated with race, such as intelligence qualification in the former and the racially disproportionate test result in the latter, are ruled illegal by the US supreme court. As a result, along with the trending development of automated decision making, the need for more sophisticated but practical techniques has made fairness in machine learning an important research area [33]. Two important but potentially conflicting goals of fair machine learning are group fairness, which aims to achieve similarity in predictions conditioned on sensitive information, andindividual fairness , which aims for similar treatment of similar individuals regardless of the sensitive information. The present work targets an important definition in group fairness: statistical parity [21], because it is closely related to disparate impact and hence long-term structural influence [45], while individual fairness focuses more on the short-term individual consequence. In the remainder of this paper, fairness and statistical parity are used interchangeably1. Before further discussing statistical parity, we note that fairness in machine learning should not be defined by a single condition without considering the application context. The goal of the present work is to provide theoretically reliable and explainable tools to help practitioners obtain the optimal (w.r.t. utility) solutions at any chosen statistical disparity level, provided one chooses to adopt statistical parity (or limited statistical dependence between the learning outcome and the sensitive information) as a meaningful fairness definition in one’s particular application context. Remark 1.1 below provides a more detailed discussion on statistical parity, namely how the utility optimization solves some major insufficiency of the original statistical parity definition and improves statistical parity to proportional equality, a fairness concept similar to equity in modern ethics which can be traced back to Aristotle and Plato [6, 19]. Remark 1.1 (Statistical parity enhanced by utility optimization) Statistical parity is one of the most important definitions of group fairness. It has advantages such as (1) legal support on mitigating adverse impact and (2) the long-term effect resulting from the enforced involvement of minority groups or diversity in learning outcome via affirmative action [27]. On the other hand, there are three major criticisms about statistical parity that are often mentioned, e.g. see [21, 26]: (1) reduced utility, (2) self-fulfilling prophecy, (3) subset targeting. However, we notice that the first two are insufficiencies with respect to utility. Therefore, the proposed method mitigates these two insufficiencies. 1 (Utility) The development of the Pareto frontier allows us to achieve a desirable statistical disparity level with theoretically provable minimum (hence necessary) utility sacrifice. Equivalently, practitioners can choose a tolerable utility sacrifice level so that the Pareto frontier will provide a learning outcome with the minimum statistical disparity while not violating the utility sacrifice tolerance. 2 (Self-fulfilling prophecy) As mentioned in [21, 26], self-fulfilling prophecy results from random, careless, or malicious selection in minority groups. But the barycenter char1. There are many other notions of fairness, such as equalized odds or equal opportunity, which all have their benefits and shortcomings [16]. A discussion of the advantages or disadvantages of the different concepts of fairness is beyond the scope of this paper. 2Fair Data Representation for Machine Learning at the Pareto Frontier acterization method guarantees the optimal fair model to make good selections in all sensitive groups to maximize utility. Section 1.2 contribution point 4 and Section 2.1 provides, respectively, the intuitive and technical explanation of how the utility maximization enforces the model to give similar learning outcomes to data points sharing relatively (within their sensitive groups) similar qualifications. For example, if race is the sensitive information and an admission test score is the only qualification variable, a barycenter-characterized optimal fair admission model would give admission to the same percentage of top-score students in each of their racial groups. Interestingly, the interpretation is consistent with the philosophical definition of fairness involving proportional equality: a model is fair (with respect to the sensitive information) if it distributes proportional chance or prediction to proportionally qualified independent variables within each of the sensitive groups. Beginning with [21], there is now a sizable body of research studying fair machine learning solutions. The resulting approaches can be categorized into the following: (1) preprocessing: deform data before training to mitigate sensitive information in the learning outcome [13, 29]; (2) in-processing: implement the definition of fairness in the training process by penalizing unfair outcome [8, 43]; (3) post-processing: enforce the definition of fairness directly on the learning outcome [26, 28]. In recent years, the post-processing approach has received significant attention due to the following remarkable result: the optimal fair distribution of supervised learning, such as classification [28] and regression [18, 24], can be characterized as the Fr´ echet mean of the learning outcome marginals on the Wasserstein space, which is also known as the Wasserstein barycenter in the optimal transport literature. (See Remark 2.2 for more details on learning outcome marginals.) The following remark provides an intuition of the Wasserstein ( W2) barycenter characterization, on which we develop our theoretical results and algorithms. Remark 1.2 (Intuition of Wasserstein barycenter characterization) The Fr´ echet mean is the closest point to a set of points in a metric space and, therefore, a generalization of the mean on the Euclidean space to general metric spaces such as the Wasserstein space. Intuitively, one can consider the barycenter (Fr´ echet mean in Wasserstein space) characterization of optimal fair learning outcome as an analog of representing a set of points by their average, which thereby optimally (with respect to total moving distance) removes the disparity among those points, except that each point is now in Wasserstein space, and hence a distribution. See Section 1.2 contribution point 4 below for more details. Despite the theoretical elegance of the post-processing barycenter characterization, challenges remain in theory and practice (see Section 1.2 for a detailed explanation of the challenges), especially compared to pre-processing or data representation methods. Fair machine learning using a pre-processing approach has been considered in [13, 23, 25, 37, 29]. While the Wasserstein barycenter provides a mathematically rigorous characterization of the post-processing optimal learning outcome, optimal fair data representation for general supervised learning models still lacks a theoretical characterization. See, for 3Xu and Strohmer example, [16, Section 3.4, 3.5] for more details on the current challenges in fair data representation design for general machine learning models beyond classification, not to mention data representations that provide the optimal trade-off between accuracy and fairness. The goal of the present work is to develop an optimal fair data representation characterization so that any supervised learning model, which aims to estimate the conditional expectation, trained via the fair data representation results in a fair estimation of the post-processing Wasserstein barycenter characterized optimal fair learning outcome. The ultimate goal is to develop a method that enjoys both the mathematically rigorous characterization of post-processing and the flexibility of pre-processing. 1.1 Optimization Problems with Sensitive Variable Independence Constraint The statistical parity constraint for supervised learning or data representation in a nutshell is a constraint on the dependence between the learning outcome and a chosen sensitive variable. Equivalently, the constraint limits the ability to access or reverse engineer the sensitive variable from the learning outcome or data representation. Therefore, although the theory and methods in the present work aim to solve current challenges in machine learning fairness, they can also be useful in other areas where sensitive or undesirable information needs to be eliminated within the existing learning outcome or data. One example of such an area other than fair machine learning is machine (feature) unlearning. It starts from [14] and now has a sizable body of research works. Here, we summarize the constrained optimization problems solved in the present work. We prove existence (and uniqueness, if possible) results via a constructive characterization approach so that an explicit formula of the solutions becomes available. Practitioners and researchers interested in limiting the statistical dependence between the learning outcome or data representation and certain feature variables can directly refer to the corresponding section for results. We leave the underlying motivations resulting from machine learning fairness to the following two subsections. In Section 3, we target the following problem: Problem 1 (Optimal fair L2-objective learning outcome) inf f∈L2(X×Z ,Y){||Y−f(X, Z)||2 2:f(X, Z)⊥Z} (1) Here, Yis the dependent variable, and f(X, Z) is an estimator that uses the independent variable Xand sensitive variable Zto estimate Y. The loss function aims to maximize utility by minimizing the L2-norm between Yandf(X, Z): ||Y−f(X, Z)||2 2=Z Ω||Y−f(X, Z)||2dP. (Ω,Σ,P) is a probability space. For S∈ {X, Y, Z },S: Ω→ S is a random variable (equivalently a measurable function) from Ω to the state space S.||·||denotes the Euclidean norm. The constraint f(X, Z)⊥Zguarantees that the final result is independent of the sensitive information Zand hence satisfies statistical parity. Finally, the admissible function space L2(X × Z ,Y) is the space of all square-integrable measurable functions from X × Z toY. (Our proof shows Problem 1 does not change if one allows all measurable functions 4Fair Data Representation for Machine Learning at the Pareto Frontier X ×Z toY.) The reason of allowing all measurable functions in our problem setting is due to the recent development of deep neural networks that are capable of estimating arbitrary measurable functions. In Section 4, we relax the above strict independence constraint by applying a quantification of statistical disparity: the Wasserstein disparity , which is the average pairwise Wasserstein distance among conditional (on Z) distributions of f(X, Z), denoted by D(f(X, Z), Z). It has the following desirable properties: (1) D(f(X, Z)) = 0 if and only if f(X, Z)⊥Z. (2) The larger Dis, the more disparities there are among the marginals (w.r.t. Z) off(X, Z). (3)Dhas a meaningful interpretation in physics as the minimum expected amount of work required to remove the distributional discrepancy between two randomly chosen sensitive groups on the learning outcome. Therefore, fixing a disparity tolerance level d∈[0,∞), Problem 2 (Optimal L2-objective learning Pareto frontier) inf f∈L2(X×Z ,Y){||Y−f(X, Z)||2 2:D(f(X, Z), Z)< d} (2) gives us the corresponding Pareto optimal solution. That is, if one wants a lower L2-loss than provided by the infimum in Problem 2, then it is necessary to increase the tolerance level d. Equivalently, if one wants to lower the tolerance level d, then it is necessary to sacrifice more L2-loss than the infimum. In Section 5, we provide a theoretical characterization of the solution to Problem 3 (Optimal fair data representation for conditional expectation estimation) inf (˜X,˜Y)∈D{||Y−E(˜Y|˜X)||2 2:˜X,E(˜Y|˜X, Z)⊥Z}, (3) where Dis the admissible data representation set we define later. Here, the objective function aims to maximize the potential utility remaining within the deformed data ( ˜X,˜Y) by minimizing the L2distance between the perfect estimator E(˜Y|˜X) on ( ˜X,˜Y) and the original Y, so that better estimation of E(˜Y|˜X) leads to better prediction of Y. The constraint ˜X,E(˜Y|˜X, Z)⊥Zguarantees: (1) f(˜X)⊥Zfor∀f:X → Y , such that any estimator of E(˜Y|˜X) is independent of Z; (2) The perfect adversarial estimator E(˜Y|˜X, Z) is independent of Z, so that a better estimation of E(˜Y|˜X, Z) leads to more independence of Z(alignment between the training objective and independence constraint). In addition, one may choose the following alternative constraints according to the application context: (1) ˜X⊥Z, which guarantees f(˜X)⊥Zfor all measurable fas mentioned above; (2) ( ˜X,˜Y)⊥Z, which guarantees any (adversarial) supervised or unsupervised learning on (˜X,˜Y) to be independent of Z. The first alternative is useful if only measurable functions ofXare allowed, whereas the second should be applied when one does not know which features are dependent or independent. See Section 1.3 for a more detailed derivation and explanation of the data representation objective function and constraints. 1.2 Challenges and Contributions in Machine Learning Fairness Now, we go back to the motivation behind the optimization problems listed above: fair machine learning. We first summarize the limitations of the current post-processing characterization and the current methods based on it to estimate the optimal fair learning outcome. 5Xu and Strohmer 1. The post-processing barycenter characterization lacks theoretical and computational generalization to high-dimensional data spaces, such as text or image spaces. From a theoretical perspective, the current works [18, 24, 37] focus on classification and 1-dimensional regression. From a computational perspective, the current works apply the coupling of cumulative distribution functions (cdf) of the learning outcome sensitive conditionals to find the barycenter and the inverse of the cdf to compute the optimal transport map. Both the coupling and the inverse of the cdf are computationally expensive. Furthermore, since the inverse of the cdf cannot be generalized to high-dimensional spaces, the current methods lack the generalization to supervised learning with high-dimensional dependent variables. Due to the recent development of generative AI models, it is now important to have fair machine learning methods for arbitrarily high-dimensional data. We hope the present work on the L2space can be a starting point for fair machine learning or data representation on more general spaces for high-dimensional data. 2. The current post-processing barycenter characterization lacks both theoretical and computational generalization to (an estimation of) the optimal trade-off, also known as the Pareto frontier, between prediction accuracy and fairness. In theory, there is a lack of characterization of the Pareto frontier (optimal trade-off) between utility and fairness. Current works on the Pareto frontier, such as [37], apply tight inequalities based on the convexity of distance metrics to suggest the optimal trade-off coincide with the Wasserstein geodesic path. While such inequalities are tight for a broad type of metrics on the space of probability measures, they are not tight for the Wasserstein metric. Hence, the inequalities are not able to extend the mathematically rigorous Wasserstein barycenter characterization of the optimal fair learning outcome to a Pareto frontier. From a computational perspective, current methods, such as [37], apply interpolation between the inverses of the sensitive conditional cdf’s (more specifically, interpolating the data points that share the same image under the sensitive conditional cdf’s) to estimate the geodesics. In addition to the drawbacks mentioned above, the inverse of the cdf also does not come with an explicit form, which makes the computation of an interpolation between two cdf inverses even more cumbersome. 3. The post-processing nature of the characterization requires explicit or implicit sensitive information in the training and decision-making process. More specifically, in order to apply the barycenter characterization to find the optimal fair learning outcome or to make predictions to newly incoming data, one needs the following steps: (1) Estimate the conditional expectation and obtain its conditional distributions with respect to the sensitive information; (2) Find the Wasserstein barycenter of the sensitive conditionals of the conditional expectation estimation or the learning outcome; (3) Compute the optimal transport maps from each sensitive conditional to the barycenter; (4) Apply each transport map to the conditional with the matched sensitive information. Here, not only does the trained model still inherit unfairness, but it is also clear that sensitive information needs to be attached to both the dependent variable or incoming data and its learning outcome or prediction, until the very last post-processing step of finding the barycenter comes to the rescue. Hence, we say that 6Fair Data Representation for Machine Learning at the Pareto Frontier the characterization has a post-processing nature. As a result, the user needs access to the sensitive information of each individual incoming data at every step during the learning process. Such a strong access to sensitive information makes the supervised learning process vulnerable to attack and sensitive information leakage. The post-processing nature of the characterization also suffers from the lack of flexibility in model selection, modification, and composition. For model selection and modification, a practitioner would have to perform the post-processing step for every model and every modification in order to compare the corresponding optimal fair learning outcomes. See Table 7 for more details on the additive computational cost of the post-processing approach compared to the one-time cost of the proposed pre-processing approach. For model composition, we consider the simple example task 2◦task 1where task i, i∈ {1,2}are trained supervised learning models. In practice, there is a good chance that task 1andtask 2belong to different practitioners or organizations, denoted by practitioners 1 and 2, respectively. Therefore, to protect sensitive information from practitioner 2, practitioner 1 will perform the post-processing step to obtain a fair learning outcome and provide it as an input variable for the training task of practitioner 2. But unless task 2needs no more input variables other than the dependent variables of task 1(in that case, task 1would be fair data representation design), still practitioner 2 needs full access to the sensitive variable attached to its input data, which includes the desensitized task 1output and other input variables. Such attachment makes the post-processing step performed by practitioner 1 meaningless. Considering the recent development of decentralized learning in practice, such drawback in model composition makes a model-independent fair data representation more applicable than a post-processing solution. 4. Many of the current fair machine learning methods are proposed without utility guarantee or explainability. Such a lack of utility guarantee or explainability prevents the study of fair machine learning from practical use. For instance, Wells Fargo [45] concluded recently that current fair machine learning methods are black-box methods, and hence they hesitate to adopt fair machine learning techniques. We provide a road map of the tools that we have developed in response to each of the listed challenges and how the present work combines all the tools to provide (exact solution and estimation of) the fair data representation at the Pareto frontier. 1. In response to the theoretical part of the first challenge, Lemma 3.1 in Section 3 provides a characterization (with explicit construction) of the exact solution to Problem 1 (the optimal fair L2-objective learning). The result shows that the infimum loss value of Problem 1 can be nicely decomposed into two parts: (1) L2orthogonal projection loss and (2) independence projection loss. Also, the result now allows the data spaces X,Y,Zto be [ k]d,Nd,[0, l]d, orRdfor arbitrary dimension d <∞. To address the challenge of computing the Wasserstein barycenter in high-dimensional data spaces [3], we propose a method that applies affine transport maps to find the optimal affine estimation of the post-processing optimal fair L2-objective supervised learning outcome with an arbitrarily finite-dimensional dependent variable, which responds to the first challenge listed above. In particular, by restricting admissible 7Xu and Strohmer transport maps to be affine and making a corresponding relaxation to the fairness constraint, we derive a relaxed version of Problem 1, stated as Problem 4. Applying the optimal affine transport maps [2], Definition 3.1 introduces the post-processing pseudo-barycenter , Lemma 3.2 shows the proposed pseudo-barycenter coincides with the true barycenter when the sensitive conditionals are Gaussian, and finally, Theorem 3.1 proves that the pseudo-barycenter is the optimal affine estimation of the true barycenter in the general conditional distribution case and provides the estimation error. Optimal affine transport and pseudo-barycenter have the advantage of computational efficiency, compared to the current methods, due to the explicit matrix form of the transport map and the nearly closed-form solution to the pseudo-barycenter. The importance of optimal affine maps encompasses much more than a solution to the first challenge. The optimal affine maps together with McCann interpolation [32] help us in obtaining an explicit form of the geodesic path characterization of the Pareto frontier in Section 4. More importantly, Section 5 shows that optimal affine maps and the pseudo-barycenter are necessary tools to overcome the post-processing nature of the Wasserstein barycenter characterization by exploiting the linearity of conditional expectation and thereby generating optimal fair data representations. 2. In Section 4, we prove an exact characterization of the solution to Problem 2 (the optimal utility-parity trade-off or Pareto frontier) in response to the theoretical part of the second challenge. In particular, Theorem 4.1 shows that, when utility loss and disparity are quantified respectively by the L2distance (between the true outcome Yand the prediction ˆY=f(X, Z)) and the average pairwise W2distance among the sensitive conditionals of ˆY, the optimal trade-off happens if and only if the conditionals of ˆY travel along the Wasserstein geodesic path from the conditionals of E(Y|X, Z) to their barycenter. Therefore, we say that the Pareto frontier is on the Wasserstein space. Corollary 4.1 then derives an explicit form of the Pareto optimal solution to Problem 2. The result is a natural extension to the post-processing Wasserstein barycenter characterization of the optimal fair learning outcome: the barycenter characterization coincides with the point at zero disparity on the Pareto frontier. Interestingly, our result shows that the Pareto frontier is linear. To solve the computational challenge of the geodesic path, Remark 4.1 applies McCann interpolation together with the optimal affine maps and the pseudo-barycenter to derive a computationally efficient (nearly) closed-form formula to estimate the Pareto frontier, which results in Algorithm 1. 3. In response to the third challenge, the present work proposes in Section 1.3 Problem 3 (optimal fair data representation problem), which makes the objective function and the fairness (statistical parity) constraint model-independent and therefore suitable for fair data representation design. More specifically, by applying the Minkowski inequality, we use an objective function to maximize the potential utility remaining in the data. On the other hand, a fair data representation should provide a fairness guarantee to arbitrary L2-objective supervised learning models. Therefore, the present work proposes a pre-processing fairness constraint to guarantee fairness in the learning outcome of arbitrary L2-objective models trained via the fair data representation. 8Fair Data Representation for Machine Learning at the Pareto Frontier In Section 5, Lemma 5.3 first provides a characterization of the exact solution to Problem 3 under a mild assumption. Next, Definition 5.2 and Definition 5.1 define the dependent and independent pseudo-barycenter, respectively. Then, similar to solving a relaxation of the post-processing characterization to obtain the optimal affine estimation, Theorem 5.1 proves that the dependent and independent pseudobarycenter pair coincides with the true solution to the optimal fair data representation when the conditional data distributions are Gaussian, and Theorem 5.2 proves that the pseudo-barycenter pair forms the optimal affine estimation of the optimal fair data representation. To derive (an estimation of) fair data representation at the Pareto frontier, Corollary 5.1 in Section 5.4 first provides a characterization of the Pareto frontier for conditional expectation on a fixed sigma-algebra. Finally, combining optimal affine map, pseudo-barycenter, together with a diagonal argument in Remark 5.4, we derive an estimation of the fair representation at the Pareto frontier, which results in Algorithm 1 and Algorithm 2. Furthermore, in Section 7, experiments show that the proposed fair data representations preserve as large an amount of information (w.r.t. the L2objective) as the fairness constraint allows. Therefore, it provides a better and more flexible solution to fair learning compared to encoding-based data representations [13, 44], which encode the information of the original data into some binary feature variables designed to guarantee statistical parity for classification. Surprisingly, experiments also show that applying the pseudo-barycenter results in nearly zero utility loss compared to the post-processing barycenter characterization solution. 4. In addition to the provable utility guarantee resulting from the Pareto frontier, the proposed method also has a meaningful interpretation from a datapoint-wise perspectivein how it achieves the statistical parity requirement: A data point of the optimal fair learning outcome is the Euclidean average of the optimally matched data points from each of the sensitive groups. Here, matching means partitioning the original data set into subsets consisting of one point from each sensitive group. Each subset is called a match. The points within a match are called matched points. Optimality in matching is equivalent to minimization of the expected variance within a randomly chosen match. Such expected (hence total) variance minimization enforces points with similar relative positions in their sensitive marginal to form a match. For example, assume that there are two sensitive conditionals A={1 (low in A) ,4 (high in A) } andB={2 (low in B) ,3 (high in B) }, then the optimal matching is {{1 (low in A) ,2 (low in B) },{3 (high in B) ,4 (high in A) }} to minimize the expected or total variance within the matches. The optimal matching in high-dimensional L2spaces shares the same geometric intuition with the simple example. That is, from a point-wise perspective, the optimal fair learning achieves statistical parity by first matching the points with similar relative positions in their sensitive groups and then representing the matched ones with their Euclidean average. 9Xu and Strohmer Figure 1: The left panel depicts three distributions, sampled from an isotropic Gaussian distribution with different first two moments. The right panel shows the pseudo-barycenter of the three sample distributions. 1.3 Fair Data Representations: From Theory to Practice In this subsection, we derive a fairness objective function that is both theoretically tractable and practically appealing. This task is more involved than one initially might expect, and it sheds light on some subtleties of both the post-processing and the pre-processing approaches. Before proceeding, we need some preparation. Let X,Y, and Zrepresent respectively the independent, dependent, and sensitive random variable, with the same underlying probability space (Ω ,Σ,P). We use the term ‘random variables’ to denote random vectors with an arbitrary but finite dimension. That is, S: Ω→ S where S ∈ { [kS]dS,NdS,[0, lS]dS,RdS} with kS∈N, lS∈RanddS<∞forS∈ {X, Y, Z }. It follows from [18, 24] that the optimal fair regression outcome can be characterized by the Wasserstein barycenter. In Lemma 3.1 we will generalize their result from regression to all functions in L2(X × Z ,Y), which shows that the optimal fair L2-objective supervised learning outcome can be characterized by solutions to Problem 1: inf f∈L2(X×Z ,Y){||Y−f(X, Z)||2 2:f(X, Z)⊥Z} (4) The utility loss is quantified by L2-norm: ||Y−f(X, Z)||2 2=R Ω||Y−f(X, Z)||2dP, where || · || is the Euclidean norm. The constraint f(X, Z)⊥Zguarantees that the final result satisfies statistical parity and, therefore, is fair. Since it follows from L2orthogonal decomposition that ||Y−f(X, Z)||2 2=||Y−E(Y|X, Z)||2 2+||E(Y|X, Z)−f(X, Z)||2 2 (5) and only the second term on the right hand side depends on the choice of f∈L2(X ×Z ,Y), we conclude that (1) is equivalent to inf f∈L2(X×Z ,Y){||E(Y|X, Z)−f(X, Z)||2 2:f(X, Z)⊥Z}. (6) It turns out—see Lemma 3.1—that the solution to (6) is exactly the Wasserstein barycenter. Therefore, we say that the optimal fair L2-objective supervised learning outcome is characterized by the Wasserstein barycenter. But notice that the Wasserstein barycenter characterization (6) assumes knowledge of the learning outcome E(Y|X, Z). That is, if practitioners apply the characterization to estimate the optimal learning outcome, it is necessary to obtain an estimator of E(Y|X, Z) via supervised learning before solving the 10Fair Data Representation for Machine Learning at the Pareto Frontier post-processing rescue step (6). Therefore, we say that the characterization has a postprocessing nature and hence call it a post-processing characterization. Now, notice that the estimator of E(Y|X, Z) is obtained via the training process inf f∈F{||Y−f(X, Z)||2 2}, (7) where the admissible function set Fdepends on the choice of supervised learning models. Denote the estimator by f′(X, Z). Then in practice (6) becomes inf f∈L2(X×Z ,Y){||f′(X, Z)−f(X, Z)||2 2:f(X, Z)⊥Z}. (8) That is, the application of the post-processing characterization is model-dependent. The fundamental reason for model dependence is that (1) is optimizing over all L2functions while in practice it is necessary to reduce the admissible set from L2to some Fwhich depends on the choice of the model. As a result, the optimizer is necessarily dependent on the choice of the model. Therefore, the constrained optimization (1) and its characterization are not suitable for our ultimate goal of deriving a model-independent pre-processing approach to the optimal fair learning outcome. The present work proposes a different constrained optimization problem that characterizes the optimal fair data representation for all L2objective supervised learning models. To make a constraint optimization problem suitable for fair data representation design, we require both the objective function and the fairness constraint to be model-independent. Furthermore, the data representation design objective and the training objective given the data representation have to be consistent in the following sense: the better training and testing result on the fair data representation leads to less L2-fitting error with respect to the true data. We now derive an objective function that is suitable for fair data representation design purpose. To start, notice that our goal is to generate a synthetic data representation ( ˜X,˜Y), a deformation of ( X, Y), via which any L2-objective model that is trained by inf f∈F||˜Y−f(˜X)||2 2 (9) would result in (an estimation of) the optimal fair learning outcome. In the rest of this paper we denote the solution to (9) by f˜Y. Also, because conditional expectation is an orthogonal projection operator on L2-space, we obtain the following orthogonal decomposition of the objective in (9): ||˜Y−f(˜X)||2 2=||˜Y−E(˜Y|˜X)||2 2+||E(˜Y|˜X)−f(˜X)||2 2. (10) Only the second term on the right hand side depends on the choice of f∈ F, hence the training step objective (9) is equivalent to the following: inf f∈F||E(˜Y|˜X)−f(˜X)||2 2. (11) Thus, the solution to (11) is also f˜Y, which depends on the choice of F. 11Xu and Strohmer The key observation is that, given a data representation ( ˜X,˜Y), (11) is the objective that practitioners try to achieve via model selection, modification, and parameter turning. Furthermore, it follows from the triangle or Minkowski inequality that ||Y−f˜Y(˜X)||2|{z } total utility loss≤ || Y−E(˜Y|˜X)||2| {z } data representation utility loss+||E(˜Y|˜X)−f˜Y(˜X)||2| {z } learning utility loss. (12) The second term on the right-hand side is the target of a supervised learning task which should be left to practitioners. Thus, the natural choice of the model-independent objective of the optimal fair synthetic data design is to minimize the first term: inf (˜X,˜Y)∈D||Y−E(˜Y|˜X)||2, (13) where Dis some admissible set of deformed versions of the original data ( X, Y) that we define later. Intuitively, the loss function can be interpreted as the potential utility sacrifice resulting from deforming ( X, Y) to ( ˜X,˜Y) forL2-objective supervised learning, while leaving the task of minimizing the second term on the right-hand side to practitioners via model selection, modification, or parameter tuning. Next, we derive a fairness constraint for synthetic data design purposes. That is, the goal is to design ( ˜X,˜Y) such that f˜Y(˜X)⊥Zfor any admissible function set F ⊂L2(X,Y). The flexibility of model choice becomes important due to the increasing complexity of models in practice nowadays, such as neural networks. The key observation here is that, due to the potential dependence of f˜YonZ, one needs to look at both models that use merely measurable functions from XtoYand more complicated models consisting of Z-dependent measurable functions: 1 For measurable functions from XtoY, if we require ˜X⊥Z, then it follows that for anyf:X → Y , it is guaranteed that f(˜X)⊥Z. Hence, we require ˜X⊥Zto prevent models from exploiting sensitive information from the independent variables. 2 For advanced or adversarial models that use Z-dependent functions from X ×Z toY, the trained model fYcould still depend on Zbecause YandZare not independent. For example, consider the extreme case where Y=kZ, k∈Rand a perfect model results in E(kZ|˜X, Z) =kZwhich fully depends on Zeven if we require ˜X⊥Z. Therefore, we also require f˜Y(˜X, Z)⊥Zto prevent such a model from exploiting sensitive information from the dependent variables. But notice that the second requirement leads us back to the post-processing nature of fairness constraints as in (8). For fair data representation design purposes, it is necessary to keep the constraint model-independent. Therefore, instead of enforcing f˜Y(˜X, Z)⊥Z, the present work requires E(˜Y|˜X, Z)⊥Zfor the following two reasons: (1) Under the modified constraint E(˜Y|˜X, Z)⊥Z, the better f˜Y(˜X, Z) estimates E(˜Y|˜X, Z), the more independent ofZbecomes f˜Y(˜X, Z). Such alignment between training objective and fairness makes the modification a natural choice under the assumption that the goal of L2-objective (adversarial) supervised learning tasks is to minimize ||E(˜Y|˜X, Z)−f˜Y(˜X, Z)||2 2, which is equivalent to minimizing ||˜Y−f˜Y(˜X, Z)||2 2. (2) Since a supervised learning model with poor prediction 12Fair Data Representation for Machine Learning at the Pareto Frontier accuracy already results in severe unfairness, the dependence on sensitive information is of less concern when designing a fair data representation. Based on the fairness requirement for both measurable functions on merely XandZdependent functions, a natural choice of (pre-processing) statistical parity constraint for data representation has the following form: ˜X,E(˜Y|˜X, Z)⊥Z. (14) It guarantees: (1) statistical parity for any model that uses only a deterministic function and any model that results in a perfect estimation of E(˜Y|˜X); (2) the better f˜Y(˜X, Z) estimates E(˜Y|˜X, Z), the more independent f˜Y(˜X, Z) becomes of Z. While the fairness constraint (14) is not the only choice, it does balance utility and fairness. The following remark discusses two alternative fairness constraint choices, which are more polarized in optimizing utility or fairness. Remark 1.3 (Alternative fair data representation constraints) There are two alternative choices of fairness constraints that are valuable in practice: 1˜X⊥Z: the weaker constraint guarantees any model using merely a deterministic function, even if sub-optimal, to result in statistical parity. But it does not protect Z from advanced models, which exploit the dependence of YonZand apply Z-dependent functions. Therefore, ˜X⊥Zprovides more utility but less sensitive information protection, compared to our choice. 2(˜X,˜Y)⊥Z: the stronger constraint guarantees statistical parity in the learning outcome of any supervised learning model, even for those that adopt Z-dependent functions and are suboptimal. But it sacrifices more utility. This stronger constraint is particularly useful in practice when one does not know which variables are dependent and which ones are independent. Our choice is a compromise of the two alternatives in terms of balancing utility sacrifice and protecting sensitive information. Furthermore, simple modifications of our analysis and algorithm would solve the two alternatives because they are essentially simplified versions of our choice. Hence, the present work targets (14). Finally, combining the objective and constraint for synthetic data design, we aim to solve Problem 3: inf (˜X,˜Y)∈D{||Y−E(˜Y|˜X)||2 2:˜X,E(˜Y|˜X, Z)⊥Z}. (15) The solution provides a fair data representation via which the trained L2-objective supervised learning models become estimations of the optimal fair conditional expectation. Compared to the original constrained optimization problem (1) which results in the post-processing nature of its barycenter characterization (6), the proposed constrained optimization problem (3) has the following advantages by design: 1 It provides a fairness guarantee for arbitrary L2-objective models. 13Xu and Strohmer 2 The model-independence together with the alignment between training objective and fairness enables practitioners to enjoy flexibility in model selection, modification, and parameter tuning on the fair data representation. 3 The fair data representation approach has more applicable models than the postprocessing approach. See Remark 1.4 below for a detailed explanation of two different interpretations of L2-objective models. In the following remark, we explain the different interpretations of L2-objective models in the post-processing and pre-processing approaches. Remark 1.4 (Interpretation of L2-objective models) For the post-processing approach, it follows from (6)and(8)that the barycenter characterization works only if the supervised learning model comes with an objective function in explicit L2-form. For the proposed preprocessing approach, the applicable L2-objective models include all the models that aim to estimate the conditional expectation. In particular, it follows from (12) and(13) that the proposed fair data representation works for any supervised learning model that aims to estimate conditional expectation or conditional probability, even though some of them do not come with an explicit objective function in L2-form. For example, all classification models share the goal of estimating the conditional probability of {Y= 1}given an observation of {X=x}, which is E( 1Y=1|X=x). Therefore, the resulting synthetic data can be used for any classification model, even models such as logistic regression and random forest that do not have L2-based objective functions. 1.4 Setting and Notation In the rest of the work, L(X) =P◦X−1:BX→[0,1] denotes the distribution or law of X, which is a function that assigns each event in the Borel sigma-algebra, BX, a probability. Let λ:=L(Z) denote the law of the sensitive random variable to simplify notation. To remove sensitive information Z, the method we propose is to find a set of maps Tx:={Tx(·, z)}zsuch that Tx(·, z) :X → X pushes the conditional (on {Z=z}) distribution (see the definition of conditional distribution L(Xz) below) forward to a common probability measure L(˜X) forλ-a.e.z∈ Z. Also, when restricting Tto be a linear map or a matrix, we use T≻0 to denote Tis positive definite, and ||T||Fto denote its Frobenius norm. Given a measurable map T:X → X and a probability measure µ∈ P(X),T♯µdenotes the push-forward probability measure that is defined as the following: for any event, A, in the Borel sigma-algebra, BX,T♯µ(A) :=µ(T−1(A)). In the rest of the paper, we often say Tpushes µforward to T♯µ. The conditional distributions {L(Xz)}zare defined uniquely λ-a.e. by the disintegration theorem [36, Box 2.2]. Hence, z→ L(Xz) is Borel measurable and, for all Borel measurable setsE∈ BX,P(E) =R XP(X−1 z(E))dλ(z). The application of the disintegration theorem aims to allow Zto be uncountably infinite, such as the real line or the real vector space. In the practical case of a finite data set, when the data set ( X, Z) is{(xi, zi)}i∈[N], for each z∈ Z, the empirical conditional random variable (with uniform distribution) is defined as follows: Xz:={xi: (xi, zi)∈(X, Z), zi=z}. 14Fair Data Representation for Machine Learning at the Pareto Frontier Therefore, on the product data space X ×Z with a joint distribution, the law of the random variable or vector Xzis the conditional distribution on {Z=z}. The present work often assumes the conditionals {L(Xz)}z∈Z⊂ P 2,ac(X). Here, P2,ac(X) denotes the set of probability measures on Xthat have finite second moments and are absolutely continuous with respect to the Lebesgue measure. The finite second moment assumption guarantees the Wasserstein distance to be well-defined without being infinite. The absolute continuity assumption guarantees the existence of their Wasserstein barycenter (See Definition 2.3) and the respective (almost surely invertible) optimal transport maps that map them to the barycenter. The present work denotes the barycenter by L(Xz) or L(X) interchangeably, and denotes the optimal transport map that pushes L(Xz) toL(X) byTzorT(·, z). To simplify notation and proof, we define ¯Xto be the random variable that satisfies the following: for λ-a.e. z∈ Z, ¯Xz=Tz(Xz). (16) In other words, the couple ( Xz,¯Xz) is a coupling of ( L(Xz),L(X)) and satisfies: ||Xz−¯Xz||2 2=W2 2(L(Xz),L(X)) (17) forλ-a.e. z∈ Z. We refer interested readers to [40, 41] for more details on the assumption ofP2,ac(X) and the coupling of measures. In the rest of the paper, we call ¯Xthe Wasserstein barycenter of {Xz}z. In solving the post-processing characterization, with the assumption of E(Y|X, Z), one first finds the Wasserstein barycenter of {L(E(Y|X, Z)z)}z, denoted by L(E(Y|X, Z)z)). Here, E(Y|X, Z)zdenotes the conditional of E(Y|X, Z) on{Z=z}forλ-a.e. z∈ Z. Then one applies the optimal transport map T(·, z) :Y → Y which pushes E(Y|X, Z)zforward toE(Y|X, Z)zforλ-a.e.z∈ Z. In solving the pre-processing characterization, one has two different optimal transport maps to deform XandY. For the dependent variable, we define Ty={Ty(·, z)}z,L(Yz), andL(˜Y) analogously, but require merely the agreement of L(E(˜Y|˜X, Z)z) forλ-a.e.z∈ Z. Theλ-a.e. agreement of L(E(˜Y|˜X, Z)z) means that the laws of the random variables or vectors E(˜Y|˜X, Z)zare equal, except for some zon a λ-null set on Z. In other words, on the Borel measurable space ( Y,BY), for any set Bin the Borel sigma-algebra BY, we have P◦[E(˜Y|˜X, Z)z1]−1(B) =P◦[E(˜Y|˜X, Z)z2]−1(B) for all z1, z2∈ Z, except on a set N⊂ Z such that λ(N) = 0. Therefore, by generating and applying ( Tx, Ty) to the data, we achieve E(˜Y|˜X, Z)⊥Z, i.e.statistical parity , due to the enforced λ-a.e. agreement of L(E(˜Y|˜X, Z)z)). Combining the application of deformation maps and (3), we obtain the fair data representation optimization problem inf (˜X,˜Y)∈D{||Y−E(˜Y|˜X)||2 2:˜X,E(˜Y|˜X, Z)⊥Z} (18) with the admissible set Dis defined as D:={(˜X,˜Y) :˜X=Tx(X, Z),˜Y=Ty(Y, Z)}, (19) Here, Tx(·, z) :X → X andTy(·, z) :Y → Y are Borel measurable maps. We denote the set of admissible ˜Xand˜YbyD|XandD|Y, respectively. The reason underlying the definition 15Xu and Strohmer ofDis that the fair data should still has its foundation from the real data, albeit suitably “deformed”. 1.5 Paper Organization The rest of the paper is organized as follows: Section 2 reviews the tools in optimal transport that are needed to derive results in the present work: Wasserstein space, Wasserstein barycenter, and optimal affine transport within a location-scale family. Section 3 first generalizes the current barycenter characterization of optimal regression to optimal L2-objective supervised learning, then defines pseudo-barycenter, and proves pseudo-barycenter is the optimal affine estimation of the true barycenter. Section 4 is concerned with both the theoretical characterization and an explicit formula of the Pareto frontier on the Wasserstein space. Section 5 studies the exact solution to the optimal data representation and the optimal affine estimation of the exact solution. Section 6 proposes an algorithm based on the theoretical results in the previous sections. Section 7 provides an extensive numerical study regarding the application of the pseudo-barycenter and the optimal affine maps to (1) the estimation of optimal fair learning outcome compared to the known fair machine learning techniques on different learning models; and (2) Pareto frontier estimation for different disparity definitions. 2. Preliminaries on Optimal Transport In this section, we review the theoretical results on optimal transport and the Wasserstein barycenter that are important for the development of the main theoretical results on efficient algorithm design, Wasserstein geodesic characterization of the Pareto frontier, and the preprocessing approach resulting in the optimal fair data representation. For our purposes, we focus on Rd. We refer readers who are interested in more generalized versions, e.g. on compact Riemannian manifolds, to for example [30]. 2.1 General Distribution Case Given µ, ν∈ P(Rd), which is the set of all probability measures on Rd, Monge asked for an optimal transportation map Tµν:Rd→Rdthat solves inf T♯µ=νnZ Rd||x−T(x)||2dµo (20) Here, || · || denotes the Euclidean norm on Rd. The problem remained open until Brenier showed that Monge’s problem coincides with Kantorovich’s relaxed version: inf γ∈Q(µ,ν)nZ Rd×Rd||x1−x2||2dγ(x1, x2)o (21) and admits a unique solution provided µ∈ P2,ac(Rd). Here, P2,ac(Rd) denotes the space of probability measures on Rdthat have finite first two moments and are absolutely continuous w.r.t. (with respect to) the Lebesgue measure. That is, the optimal solution to (21) has the form: γ= (Id, T µν)♯µ, where Tµνsolves (20). Here,Q(µ, ν) denotes all the probability measures on ( R2d,B(Rd)⊗B(Rd)) such that the marginals are µandν. The relaxed problem 16Fair Data Representation for Machine Learning at the Pareto Frontier is easy to solve due to the weak* compactness ofQ(µ, ν). We refer interested readers to [40, 41] for more detailed existence and uniqueness results. Remark 2.1 The uniqueness is in the weak sense for γandµ-a.e. for Tµν. Kantorovich’s problem provides a certain kind of “distance” on P(Rd) except for the possibility of being infinite. Definition 2.1 (Wasserstein distance2)Given µ, ν∈ P(Rd), W2(µ, ν) := inf γ∈Q(µ,ν)nZ Rd×Rd||x1−x2||2dγ(x1, x2)o .1 2 (22) It is not hard to verify that the Wasserstein distance defined above satisfies the axioms of a metric except for finiteness of W2(µ, ν) for arbitrary µ, ν∈ P(Rd). In order to guarantee finiteness, one needs to put more restrictions on the set of all probability measures: Definition 2.2 (Wasserstein space) Define W2as above and P2(Rd) :=n µ∈ P(Rd) :Z Rd||x||2dµ <∞o . (23) The couple (P2(Rd),W2)is called Wasserstein space. The Wasserstein space has gained increasing popularity in image processing, economics [22, 15], and machine learning in recent years due to its useful properties such as polishness (of the space) and robustness (w.r.t. perturbation on the marginal probability measures and hence on sampling). Since the Wasserstein space is a metric space, the Fr´ echet mean on the space is welldefined and it is called the Wasserstein barycenter in the optimal transport literature. Definition 2.3 (Wassserstein barycenter [2]) Given {µz}z∈Z⊂(P2(Rd),W2)for some index set Z, the barycenter of {µz}zis the Fr´ echet mean of the set on (P2(Rd),W2). That is,¯µis the solution to inf µ∈P2(Rd)nZ ZW2 2(µz, µ)dλ(z)o , (24) where ¯µdenotes the Fr´ echet mean or barycenter. Here, for our purpose, we focus on the case where the index set Z ∈ { [k],N,[0,1],Rn}. Next, we look at optimal transport and the barycenter problem from the perspective of optimal coupling. The goal is to show that the multi-marginal coupling problem is equivalent to the Wasserstein barycenter problem. The equivalence is an essential tool in proving our result in optimal affine transport, the optimality of the pseudo-barycenter, and the geodesic characterization of the Pareto frontier. 2. Throughout this paper we work with the Wasserstein-2 distance, and thus simply call it the Wasserstein distance. 17Xu and Strohmer First, notice that Kantorovich’s problem is in fact a 2-marginal coupling problem: Let X1, X2be the random variable satisfy L(X1) =µ,L(X2) =ν, the problem looks for a γwith marginals being µ, νthat minimizes Eγ||X1−X2||2. It follows naturally by the existence and uniqueness result of the optimal transport map (also known as Brenier’s map) [11], that the Wasserstein distance admits the form in the classic probability language: W2(µ, ν) = (Eµ||X1−T(X1)||2)1 2, (25) where Tis the optimal transport map that pushes µ=L(X1) forward to ν=L(X2). More recent work in mathematics [30, 34] and economics [15, 22] has generalized the Kantorovich problem to the multi-marginal coupling problem: inf γ∈Q({µz}z∈Z) Eγ(Z Z2||Xz1−Xz2||2dλ(z1)dλ(z2))	 , (26) whereQ({µz}z∈Z) denotes all the Borel probability measures on ( Rd)|Z|with marginals being µz=L(Xz)∈ P(Rd)λ-a.e.. Hence, one can consider λ∈ P(P(Rd)). It can be shown that the above is equivalent to the following: sup γ∈Q({µz}z∈Z) Eγ(||Z ZXzdλ(z)||2)	 (27) Remark 2.2 (Justification for the name of marginals) Since{Xz}zare the marginals for the admissible couplings in (26), with the equivalence between the multi-marginal coupling and Wasserstein barycenter (see Remark 2.3 below) in mind, we often call {Xz}z and{L(Xz)}zthe sensitive marginals, even though they are also the conditional random variables and distributions constructed by disintegration. Intuitively, (27) tends to find a family of random variables parametrized by zwith fixed marginals µzsuch that the variance of the matched (by γ) group average is maximized. For readers who are more familiar with stochastic processes, consider z=tas a time variable, then Xtis a stochastic process with fixed time marginals, and (27) tends to find a way ( γ) to group the fixed marginals into trajectories so that the variance of the trajectory-wise (sample path) average is maximized. (Hence, the expected variance within a randomly chosen sample path is minimized.) As shown in [2, 34], the above multi-marginal problem is equivalent to the barycenter problem: Remark 2.3 (Equivalence between multi-marginal coupling and barycenter) Assume{µz}zare absolutely continuous w.r.t. the Lebesgue measure and let γ∗and¯µbe the solution to (27) and (24), respectively. It follows that ¯µ=γ∗◦T−1where T({xz}z) :=R Zxzdλ(z). The importance of this equivalence is twofold: 1 It is the key to proving the non-degenerate Gaussianity of the Wasserstein barycenter of non-degenerate Gaussian marginal distributions; 18Fair Data Representation for Machine Learning at the Pareto Frontier 2 It provides technical support for the interpretation (Section 1.3 point 4) of how the Wasserstein barycenter solves data-related fairness issues on a point-wise scale. Therefore, we generalize the equivalence to the case where Zis a Polish space, which is a metric space that is separable and complete. In particular, [ k]d,[0, l]d,Nd,Rdmentioned above are all examples of Polish spaces. This generalization is important for our purpose as it provides a theoretical foundation for removing Zin the form of random vectors. Now, the following result provides the existence and uniqueness result of the barycenter problem that is suitable for our purpose. Theorem 2.1 (Existence and uniqueness of barycenter [31](Theorem 2 and Proposition 6) ) Assume that Zis a Polish space and that λ:=P◦Z−1satisfiesR ZW2 2(µz, ν)dλ(z)<∞ for some ν∈ P2(X)(hence, for all ν∈ P2(X)). Then the following properties hold: 1 There exists a barycenter of {µz}z∈Zw.r.t. λ. 2 If, in addition, λ({z:µz∈ Pac(X)})>0, then the barycenter is unique. Remark 2.4 (Applicability of assumptions in Theorem 2.1) The assumption thatR ZW2 2(µz, ν)dλ(z)<∞in the above result is satisfied in our application to the optimal fair learning outcome or data representation: When generating the optimal transport maps {Tz}z, the training set has a finite number of data and hence finite different values of zin the discrete case or after discretization in the continuous case. Therefore, since {µz}z⊂ P 2(X), pick a value z0that is in the training set, we have that W2 2(µz, µz0)are essentially (w.r.t. λ) uniformly bounded. That impliesR ZW2 2(µz, µz0)dλ(z)<∞. Now, we have the theoretical results that are needed to prove the main results, except for the McCann interpolation, which will be introduced in Section 4. The next step is to develop a computationally efficient method to compute (an estimation of) the Wasserstein barycenter, (the McCann interpolation of) optimal transport maps, and thereby the optimal fair model and Pareto frontier. More specifically, we focus on positive definite affine optimal transport maps. 2.2 Rigid Translation Before deriving our main result on optimal positive definite affine maps, we first study the case where admissible maps are restricted to the set of rigid translations. The following property of rigid translations makes our results on the optimal affine maps simpler: we can assume, without loss of generality, that the first moments of the marginal measures are zero: mXz:=E(Xz) = 0 and mYz:=E(Yz) = 0. Lemma 2.1 Letµ, ν∈ P 2,mµ:=R xdµ(x), and mν:=R xdν(x). Also, let µ′, ν′be the centered versions of µ, ν, respectively. It follows that W2 2(µ, ν) =W2 2(µ′, ν′) +||mµ−mν||2. (28) 19Xu and Strohmer Proof See Appendix A. Notice that the above result allows us to assume measures to have vanishing first moments when deriving the optimal transport maps. Indeed, if Tµ′ν′is the Brenier’s map between µ′andν′, then Tµν:=T+mν◦Tµ′ν′◦T−mµis the optimal transport map between µandν. Here, T+mν(x) :=x+mνandT−mµare defined analogously. In the rest of Section 2, we assume without loss of generality that the first moments of the measures are all equal to zero. 2.3 Location-Scale Case and Optimal Affine Transport A sufficient condition for Brenier’s maps to be positive definite affine is to require a certain “similarity” between the marginal data distributions. One natural choice is to assume {Yz}z and{Xz}zto be non-degenerate Gaussian vector λ-a.e.. As shown in [4], the assumptions of Gaussian vector can easily be generalized to a location-scale family . In the definition below, Sd ++denotes the set of all d×dpositive definite matrices. The generalization from Gaussian to location-scale families is important for the main result in the next section, where we consider computationally efficient solutions to a relaxation of the Wasserstein barycenter problem in the case of general marginal distributions. Definition 2.4 (Location-Scale Family) For any L(X0)∈ P(Rd), define F(L(X0)) := L(AX0+m) :A∈ Sd ++, m∈Rd	 . (29) The set F(L(X0))is called a location-scale family characterized by L(X0). In other words, under the assumption of vanishing first moments, the random variables that share laws in the same location-scale family can be transformed into each other by a positive definite linear transformation. In [4] it is shown that Brenier’s map between two probability measures, each having a vanishing first moment, within the same location-scale family is linear and has a closed form. Lemma 2.2 (Optimal affine map) Ifµ, ν∈ F(L(X0))for some X0such that mµ= mν= 0, then the Brenier’s map that pushes µforward to νis given by: Tµν= Σ−1 2µ(Σ1 2µΣνΣ1 2µ)1 2Σ−1 2µ (30) where Σµ:=R xxTdµandΣν:=R xxTdν. Proof See, for example, Theorem 2.3 in [4]. Remark 2.5 The optimal affine map is also the midpoint of the geodesic path joining Σ−1 µ andΣνon the manifold of positive definite matrices. We refer interested readers to, for example, Chapter 6.1 in [9] for more details. 20Fair Data Representation for Machine Learning at the Pareto Frontier Now, back to the barycenter problem. It follows from Lemma 2.2 that, if one assumes that all the marginals belong to the same location-scale family, then the barycenter also belongs to the family and a nearly closed-form solution to the barycenter is available. Lemma 2.3 (Barycenter in the location-scale case) Assume {µz}zbelong to the same location-scale family F(P0)and satisfy mµz= 0,Σµz≻0, λ−a.e., then there exists a unique solution, denoted by ¯µ, to(24). Moreover, ¯µalso belongs to F(P0)and is characterized by m¯µ= 0andΣ¯µ= Σ where Σis the unique solution to the following equation: Z Z(Σ1 2ΣµzΣ1 2)1 2dλ(z) = Σ , (31) where Σµzis the second moment of µz,∀z∈ Z. Proof See Appendix A. In the case where mµz̸= 0, it follows from Lemma 2 .1 that Z ZW2 2(µz, µ)dλ(z) =Z ZW2 2(µ′ z, µ′)dλ(z) +Z Z||mµz−mµ||2dλ(z) where µ′denotes the centered version of µ. By Lemma 2 .3, we know the first term on the right is minimized at ¯ µ′∼ N(0,Σ¯µ). Also, the second term on the right is minimized at the Fr´ echet mean with Euclidean metric, which is equal to the expectation. That is, m¯µ=R Zmµzdλ(z). As a result, the optimal transport map is Tµz¯µ=T+m¯µ◦Tµ′z¯µ′◦T−mµz(32) Remark 2.6 (Solution to (31))The non-linear matrix equation (31) has a unique solution that can be approached via the following iterative process: Z Z(Σ1 2 iΣµzΣ1 2 i)1 2dλ(z)→Σi+1. (33) We refer interested readers to [4] for more details on the fixed point approach to the Wasserstein barycenter. The present work only applies this fact in the algorithm design in Section 6. 3. Wasserstein Barycenter Characterization of the Optimal Fair Learning Outcome Optimal transport has been considered an adversarial or constrained optimization problem in its application to machine learning. In particular, some of the most popular unsupervised learning methods, such as K-means and PCA, are specific examples of the Wasserstein barycenter problems when putting restrictions on the admissible transport maps and relaxation on the weak equivalence requirement of the push-forwards w.r.t. test functions. See, for example, [39] for more details. But we apply optimal transport in an opposite direction so that the independence or imperceptibility of the sensitive variable Zbecomes theoretically provable. 21Xu and Strohmer In this section, the primary goal is to develop the optimal affine map and pseudobarycenter as tools to solve the challenge of the high computational cost of Wasserstein barycenter and optimal transport maps in high-dimensional data space. More specifically, we restrict the admissible transport maps to be merely affine maps while relaxing the fairness constraint to a sufficient and necessary level. The importance of efficiency in computing the barycenter and optimal transport maps will soon be clear in Section 4 when we compute the Pareto frontier along the Wasserstein geodesic path. Furthermore, the importance of affinity of transport maps will also be soon clear in Section 5 when solving the optimal fair data representation problem (3). The organization of the current section is as follows: we first generalize the Wasserstein barycenter characterization of the optimal regression to all L2-objective supervised learning models, then apply the optimal affine maps to estimate high-dimension optimal learning outcome. Now, we show that the (unique) solution to Problem 1 can be characterized as the Wasserstein barycenter of the conditional expectation sensitive marginals. The barycenter characterization of the optimal fair regression is first proved in [18, 24]. 3.1 Wasserstein Barycenter Characterization We start with a characterization of the optimal learning outcome of the L2-objective supervised learning task. Let E(Y|X, Z)zbe the sensitive marginals of ( E(Y|X, Z), Z) (or, equivalently, the sensitive conditionals of E(Y|X, Z) on{Z=z}by Remark 2.2) for λ-a.e. z∈ Z,L(E(Y|X, Z)z) :=µz, and ¯ µdenote the Wasserstein barycenter of {µz}z∈Z. Also, letT(·, z) denote the optimal transport map from µzto ¯µ. Lemma 3.1 (Optimal fair L2-objective supervised learning characterization) Assume that the conditional expectation marginals {µz}z∈Z⊂ P 2,ac(Y), then E(Y|X, Z) =T(E(Y|X, Z), Z) :={T(E(Y|X, Z)z, z)}z∈Z (34) is the unique solution to Problem 1. Furthermore, we have ||Y−T(E(Y|X, Z), Z)||2 2= inf f∈L2(X×Z ,Y){||Y−f(X, Z)||2 2:f(X, Z)⊥Z} =||Y−E(Y|X, Z)||2 2+Z ZW2 2(µz,¯µ)dλ Proof First, notice that the fairness constraint f(X, Z)⊥Zis equivalent to L(f(X, Z)z) = µ λ-a.e. for some µ∈ P(Y). Now, we prove the lower bound: let f∈L2(X ×Z ,Y) satisfies f(X, Z)⊥Z, we have ||Y−f(X, Z)||2 2=||Y−E(Y|X, Z)||2 2+||E(Y|X, Z)−f(X, Z)||2 2 =||Y−E(Y|X, Z)||2 2+Z Z||E(Y|X, Z)z−f(X, Z)z||2 2dλ ≥||Y−E(Y|X, Z)||2 2+Z ZW2 2(µz,L(f(X, Z)z))dλ ≥||Y−E(Y|X, Z)||2 2+Z ZW2 2(µz,¯µ)dλ 22Fair Data Representation for Machine Learning at the Pareto Frontier Here, the first line follows from the L2projection characterization of conditional expectation, the second follows from disintegration, the third from the definition of W2, and the fourth from the definition of the Wasserstein barycenter and the fairness restriction f(X, Z)⊥Z. Next, we construct a fY∈L2(X × Z ,Y) such that the lower bound is obtained. Let Tzdenote the optimal transport map such that ( Tz)♯µz= ¯µforλ-a.e. z∈ Z. Define T(·, z) :=Tz(·) and fY(X, Z) :=T((E(Y|X, Z), Z)). (35) Here, π:= (Id, T z)♯µzdλdefines π∈ P(Z ×Y ×Y ). Hence, we have π=π(y,z)dλ(E(Y|X,Z),Z) andπ(y,z)=δT(y,z)λ(E(Y|X,Z),Z)−a.e.. Since ( y, z)→π(y,z)=δT(y,z)isY × Z /P(Y) measurable, we have ( y, z)→T(y, z) isY × Z /Ymeasurable. It follows from E(Y|·,·)⊗ Id|Z(·,·) :X ×Z → Y×Z beingX ×Z /Y×Z measurable that fY=T◦(E(Y|·,·)⊗Id|Z(·,·)) isX × Z /Ymeasurable. Also, ¯ µ∈ P 2(Y) =⇒ || fY(X, Z)||2<∞. This proves fY∈ L2(X × Z ,Y). It remains to show that the lower bound is obtained at fY(X, Z). Indeed, by construction, we have ||E(Y|X, Z)−fY(X, Z)||2 2=Z Z||E(Y|X, Z)z−fY(X, Z)z||2 2dλ =Z Z||E(Y|X, Z)z−T(E(Y|X, Z)z, z)||2 2dλ =Z ZW2 2(µz,(Tz)♯µz)dλ =Z ZW2 2(µz,¯µ)dλ. It follows from the derivation of the lower bound above that ||Y−fY(X, Z)||2 2= inf f∈L2(X×Z ,Y){||Y−f(X, Z)||2 2:f(X, Z)⊥Z} (36) Uniqueness follows from the uniqueness of ¯ µand the uniqueness of T(·, z). We are done. The above result shows that the minimum L2-loss for statistical parity can be nicely decomposed into two parts: (1) an L2(X × Z ,Y) orthogonal projection loss due to the inference capability of ( X, Z) w.r.t. Yand (2) an independence projection loss due to the statistical parity constraint. That is, inf f{||Y−f(X, Z)||2 2:f(X, Z)⊥Z} | {z } minimum loss for statistical parity=||Y−E(Y|X, Z)||2 2| {z } orthogonal projection loss+Z ZW2 2(µz,¯µ)dλ |{z } independence projection loss. Furthermore, to construct the optimal fair L2learning outcome, one first performs L2 orthogonal projection to obtain the conditional expectation E(Y|X, Z), then outputs the Wasserstein barycenter of the sensitive marginals of E(Y|X, Z) as the optimal (with respect toL2-objective) fair (for statistical parity) result. Unfortunately, in practice, the characterization suffers from a lack of efficient methods to compute the Wasserstein barycenter and obtain an explicit formula of the optimal transport 23Xu and Strohmer maps [3]. Current methods restrict the sensitive variable Zto be binary mainly because the computation of a multi-marginal barycenter is expensive. Furthermore, notice the current methods restrict the dependent variable Yto be one-dimensional, because the only wellknown exact solution to transport maps is the inverse of cumulative function that merely works for one-dimensional variables. Therefore, to provide methods using the characterization in high-dimensional dependent variable cases, we introduce the optimal affine map and the associated pseudo-barycenter. 3.2 Optimal Affine Estimation: Pseudo-barycenter To solve the challenge of deriving an explicit formula for the Wasserstein barycenter and optimal transport maps, we restrict the admissible transport maps to be affine and show that the estimation of the Wasserstein barycenter via optimal affine maps coincides with the true Wasserstein barycenter in the Gaussian case, and that the estimation error is bounded in the case of general distributions. In other words, we consider the choice of positive definite affine maps under two circumstances: 1 We assume the marginals are non-degenerate Gaussian. That is, {E(Y|X, Z)z}zare assumed to be non-degenerate Gaussian vectors λ-a.e.. 2 Instead of making assumptions on the data distribution, we relax the independence constraint to the independence between Zand merely the first two moments of f(X, Z). From a theoretical perspective, affine maps allow us to derive (nearly) closed-form solutions under either of the assumptions mentioned above. Also, affine maps allow us to develop a pre-processing approach by directly applying the obtained maps to the original data before training, even though such maps are constructed to push the post-training marginals toward their barycenter. From a practical perspective, the advantage is obvious: the computation of affine maps only uses (sample estimation of) the first two moments of the marginal distributions and hence is highly efficient compared to the computation of general Brenier’s maps, especially in the case of high-dimension data. Before developing the pseudo-barycenter, the following remarks compare in more detail the exact barycenter with its affine approximation. Remark 3.1 (Applying pseudo-barycenter vs exact barycenter) The comparison between the pseudo-barycenter method and the exact barycenter is an analog of the comparison between the linear regression model and the exact conditional expectation: When there is no worry about over-fitting, a practitioner who cares more about the strict goal of minimizing L2error (analog: the strict statistical parity guarantee) should always try to find the exact conditional expectation function (analog: the exact barycenter and the corresponding exact transport maps) by using more complicated models. But the simplicity, robustness, and interpretability of linear regression (analog: pseudo-barycenter and optimal affine maps) are often useful in practice. We define the pseudo-barycenter, using merely matrix calculations, as follows: 24Fair Data Representation for Machine Learning at the Pareto Frontier Definition 3.1 The post-processing pseudo-barycenter ˆY†is given via ˆY†:=Taffine(ˆY , Z), (37) where Taffine(·, z) := Σ−1 2 ˆYz(Σ1 2 ˆYzΣΣ1 2 ˆYz)1 2Σ−1 2 ˆYz, (38) andΣis the unique solution to Z Z(Σ1 2ΣˆYzΣ1 2)1 2dλ(z) = Σ . (39) To obtain (an approximation of) the unique solution, we apply the iterative method (33) in Remark 2.6 when designing our algorithm in Section 6. Now, Lemma 2.2 shows that under the assumption of Gaussianity of the learning outcome marginals, the optimal transport map is affine and the pseudo-barycenter is indeed the Wasserstein barycenter. Moreover, Lemma 2.3 shows that the barycenter of Gaussian marginals is still Gaussian. Therefore, the optimal maps from the marginals to the barycenter are determined entirely by the first two moments. Lemma 3.2 (Post-processing pseudo-barycenter in the Gaussian case) Assume ˆYz∼ N(0,Σz)forλ-a.e. z∈ Z, then ˆY†is the Wasserstein barycenter of {ˆYz}z. It follows from Theorem 3.2 that, if ˆY=E(Y|X, Z), then Y†is the solution to the Wasserstein barycenter characterization of the optimal fair learning outcome. Finally, we show that the pseudo-barycenter is the optimal affine estimation of the Wasserstein barycenter in the case of general marginal distributions. To do so, we need to first put restrictions on the admissible transport maps. However, such a restriction on admissible maps leads to a necessary relaxation of the fairness constraint. To see the necessity, Lemma 2.2 shows positive definite affine maps transform distributions within the same location-scale family. Therefore, given marginals Y1andY2from different locationscale families, affine maps are not able to transform them to each other. That implies the non-existence of the barycenter under the original independence restriction. Indeed, if a barycenter of {Yz}z∈{1,2}exists under the restriction of positive definite affine maps, then Y1 andY2belong to the same location-scale family as their barycenter, which contradicts the assumption of general distributions. That is, the Wasserstein barycenter characterization does not have a solution when we admit merely affine transport maps in the general marginal distribution case. On the other hand, notice that the best affine maps can achieve is to map Y1to aY′ 2, which shares the same first two moments with Y2within the Y1location-scale family. We call such Y′ 2aY1location-scale family analog of Y2. Therefore, we propose the following relaxation of the fairness constraint that suffices to guarantee the existence of a solution to the relaxed version of (1) with merely positive definite affine transport maps: mf(X,Z),Σf(X,Z)⊥Z (40) where mf(X,Z),and Σ f(X,Z),denotes respectively the first and second moment of f(X, Z). 25Xu and Strohmer Remark 3.2 (Fairness guarantee of the relaxation) The adversarial task of testing and exploiting probabilistic independence between f(X, Z)andZis equivalently difficult to enforcing the independence. One common strategy is to explore its equivalence to the independence between all moments of f(X, Z)andZ, provided the boundedness of the two random variables. But the verification or enforcement of independence among higher moments is extremely vulnerable to data noise in practice. Thus, instead of enforcing f(X, Z)⊥Z, one could relax the constraint to the independence between Zand some of the moments of f(X, Z). In this section, we focus on the first two moments. That is, mf(X,Z),Σf(X,Z)where mf(X,Z):=E(f(X, Z))andΣf(X,Z):=E((f(X, Z)−E(f(X, Z)))(f(X, Z)−E(f(X, Z)))T). It is not hard to notice that the relaxation is already strong enough to result in imperceptibility to any unsupervised learning algorithm that uses merely the mean and covariance of data to extract information, such as K-means and PCA. Therefore, the optimal affine estimation of the Wasserstein barycenter characterization is given by: Problem 4 (Optimal affine estimation of barycenter problem) inf f∈L2(X×Z ,Y){||Y−f(X, Z)||2 2:mf(X,Z),Σf(X,Z)⊥Z}. (41) Now, we show that the pseudo-barycenter defined above is indeed the solution to Problem 4 and hence the optimal affine estimate of the optimal fair learning outcome. To prove the main result, we need the following result: given any fixed covariance matrix, the optimal positive definite affine maps result in the lowest Wasserstein distance such that the push-forwards all share the same fixed covariance matrix. To simplify notation, let µz:=L(E(Y|X, Z)z). Also, let mY|Xzand Σ Y|Xzdenote the mean and covariance matrix ofE(Y|X, Z)zrespectively. Lemma 3.3 (Projection Lemma) Assume {µz}z⊂ P 2,ac(Y). IfmY|Xz= 0,ΣY|Xz≻0 λ-a.e., for any Σ≻0, inf ˆY:ΣˆYz=ΣZ ZW2 2(µz,L(ˆYz))dλ(z) (42) admits a unique solution, denoted by ˆYΣ, that satisfies ˆYΣ,z:=TΣ(ˆYz, z) (43) where TΣ(·, z) := Σ−1 2 Y|Xz(Σ1 2 Y|XzΣΣ1 2 Y|Xz)1 2Σ−1 2 Y|Xz. Proof Z ZW2 2(µz,L(ˆYz)dλ(z) =Z Z||E(Y|X, Z)z−TΣ(ˆYz, z)||2 2dλ(z) =Z Zinf ν:Σν=ΣW2 2(µz, ν)dλ(z) = inf ν:Σνz=ΣZ ZW2 2(µz, νz)dλ(z), 26Fair Data Representation for Machine Learning at the Pareto Frontier where the second equality follows from the characterization of Gelbrich’s bound, see for example Proposition 2.4 in [20]. Now, let ˆY′̸=ˆYΣbut also satisfy Σ ˆY′= Σλ-a.e., then we have Z Z||E(Y|X, Z)z−ˆYΣ,z||2 2dλ(z)<Z ZW2 2(µz,L(ˆY′ z))dλ(z) ≤Z Z||E(Y|X, Z)z−ˆY′ z||2 2dλ(z), where the first inequality is strict due to the uniqueness of Brenier’s maps TΣ(·, z) and hence ofTΣ(ˆYz, z)λ-a.e.. The proof is complete. Remark 3.3 (Intuition of the Projection Lemma) Intuitively, for an arbitrary positive definite matrix Σ, one can consider TΣ(·, z)as the projection map (w.r.t. W2distance) onto {ν∈ P2(Y) : Σ ν= Σ} (44) which is the set of centered probability measures with fixed covariance matrix Σin(P2(Y),W2). In other words, given a probability measure, the maps {TΣ(·, z)}zfinds the closest (w.r.t. the Wasserstein distance) point in the set for each of the marginals. Finally, we are ready to prove the justification of the pseudo-barycenter in the case of general distributions. Theorem 3.1 (Optimal affine estimation of W2barycenter: Pseudo-barycenter) E(Y|X, Z)†:={Taffine(E(Y|X, Z)z, z)}zis the unique solution to Problem 4: inf f∈L2(X×Z ,Y){||Y−f(X, Z)||2 2:mf(X,Z),Σf(X,Z)⊥Z}, (45) provided {µz}z⊂ P 2,ac(Y). Proof First, we fix Σ ≻0 arbitrary and denote ˆYΣ,z:=TΣ(E(Y|X, Z)z, z) forλ-a.e. z∈ Z, we have ||Y−TΣ(E(Y|X, Z), Z)||2 2− ||Y−E(Y|X, Z)||2 2=Z Z||E(Y|X, Z)z−ˆYΣ,z||2 2dλ(z) (46) and it follows from Lemma 3 .3 that Z Z||E(Y|X, Z)z−ˆYΣ,z||2 2dλ(z) =Z ZW2 2(µz,L(TΣ(E(Y|X, Z)z, z))dλ(z) = min ν:Σνz=ΣZ ZW2 2(µz, νz)dλ(z). Therefore, (41) boils down to the following: inf Σ≻0nZ Z||E(Y|X, Z)z−TΣ(E(Y|X, Z)z, z)||2 2dλ(z)o . (47) 27Xu and Strohmer Finally, notice that Z Z||E(Y|X, Z)z−TΣ(E(Y|X, Z)z, z)||2 2dλ(z) =Z Z||E(Y|X, Z)z||2 2+||TΣ(E(Y|X, Z)z, z)||2 2−2⟨E(Y|X, Z)z, TΣ(E(Y|X, Z)z, z)⟩2dλ(z) =Z ZTrace(Σ Y|Xz) + Trace(Σ) −2E(E(Y|X, Z)T zTΣ(E(Y|X, Z)z, z)dλ(z) =Z ZTrace(Σ Y|Xz) + Trace(Σ) −2⟨TΣ,ΣY|Xz⟩Fdλ(z) =Z Z||E(Y|X, Z)′ z−TΣ(E(Y|X, Z)′ z, z)||2 2dλ(z), where ⟨·,·⟩Fdenotes the Frobenius inner product and X′∼ N(mX,ΣX) denotes the Gaussian analog of X. It follows from definition of Taffine(E(Y|X, Z)z, z) with Taffine(·, z) := Σ−1 2 Y|Xz(Σ1 2 Y|XzΣΣ1 2 Y|Xz)1 2Σ−1 2 Y|Xzand Lemma 2 .3 thatR Z||E(Y|X, Z)z−E(Y|X, Z)† z||2 2dλ(z) is the unique lower bound of the objective function in (47). It then follows from the uniqueness of Brenier’s map that E(Y|X, Z)†is the unique solution to (41). In this section, we focus on applying the optimal affine transport map and the pseudobarycenter to find a computationally efficient estimation of the optimal fair learning outcome in high-dimensional space. As we mentioned above, it will soon become clear in the next two sections and numerical experiments that a combination of McCann interpolation and optimal affine maps in matrix form results in not only a mathematically neat solution to estimate the Pareto frontier, which significantly reduces computational expense in practice, but also a necessary tool to help us circumvent the post-processing nature and solve the optimal fair data representation problem (3). Now, we are ready to address the lack of a precise theoretical characterization of the Pareto frontier between utility and fairness, which turns out to be a natural generalization of the Wasserstein barycenter characterization of the optimal fair L2-objective learning outcome. 4. Wasserstein Geodesics Characterization of Pareto Frontier In reality, rather than looking for the optimal fair learning outcome, practitioners may have to choose a middle ground: sacrificing some prediction accuracy while tolerating a certain level of disparity. Therefore, it is tempting to generalize the barycenter characterization of the optimal fair learning outcome to the entire Pareto frontier between prediction error and statistical disparity. In this section, we show that the constant-speed geodesics from the conditional expectation sensitive marginals to their Wasserstein barycenter characterize the Pareto frontier on the Wasserstein space, in which utility loss and statistical disparity are quantified respectively by the L2norm and the average pair-wise Wasserstein distance among the sensitive marginals. As a result, given the optimal transport maps, one can derive a closed-form solution to the geodesics and thereby the Pareto frontier using McCann interpolation. 28Fair Data Representation for Machine Learning at the Pareto Frontier Here, we first provide a post-processing characterization of the Pareto frontier, Theorem 4.1, which is of theoretical interest and great generality. Then, we derive a closed-form solution to Problem 2 based on this characterization. The results form a direct generalization of the barycenter characterization, which is Lemma 3.1, and practitioners can apply the result together with the pseudo-barycenter and McCann interpolation to obtain the optimal affine estimation to the post-processing Pareto frontier. Later in Section 5, we further apply the result to provide a characterization of the exact solution and an optimal affine estimation of the solution to the optimal fair data representation problem (3). Now, we start to characterize the Pareto frontier. In the rest of the section, we denote L(E(Y|X, Z)) =: µ,L(E(Y|X, Z)z) =: µz. For utility, given any measurable function f: X × Z → Y , we define the increased prediction error by the L2-norm of the difference between f(X, Z) and the orthogonal projection E(Y|X, Z): L(f(X, Z)) :=||E(Y|X, Z)−f(X, Z)||2= (Z Z||E(Y|X, Z)z−f(X, Z)z)||2 2dλ(z))1 2.(48) To simplify notation, we also denote L(T′) :=L(T′(E(Y|X, Z), Z)) = (Z Z||E(Y|X, Z)z−T′ z(E(Y|X, Z)z)||2 2dλ(z))1 2.(49) for any measurable T′:Y × Z → Y . To relax the hard independence constraint for the Pareto frontier, we quantify the statistical disparity of a given learning outcome or prediction ˆYby the average pairwise Wasserstein distance among its sensitive marginals: Definition 4.1 (Wasserstein disparity) D(ˆY , Z) := Z Z2W2 2(L(ˆYz1),L(ˆYz2))dλ(z1)dλ(z2) 1 2. (50) In our setting, ˆY=f(X, Z) for some f:X ×Z → Y . Also, to simplify notation, we denote the Wasserstein disparity that remains in the already deformed (by applying T′) conditional expectation by D(T′) :=D(T′(E(Y|X, Z), Z), Z) = (Z Z2W2 2((T′ z1)♯µz1,(T′ z2)♯µz2)dλ(z1)dλ(z2))1 2(51) for any measurable T′:Y × Z → Y . Here, Tz=T(·, z) :Y → Y forλ-a.e. z∈ Z. We adopt the Wasserstein disparity as a statistical disparity quantification due to the following desirable properties: •Wasserstein disparity characterizes statistical parity: D(f(X, Z), Z) = 0 ⇐⇒ f(X, Z)⊥Z •Physics interpretation: Due to the definition based on the Wasserstein distance, Wasserstein disparity can be understood as the expected minimum amount of work that is required to move one randomly chosen marginal to another random chosen one. Therefore, the larger D(f(X, Z), Z) is, the more necessary work is expected to remove the distributional discrepancy among the sensitive groups on f(X, Z). 29Xu and Strohmer Now, let T:Y × Z → Y satisfy T(·, z) being the optimal transport maps from {µz}zto their barycenter ¯ µforλ-a.e. z∈ Z(See construction of Tin the proof of Lemma 3.1), we define V:=L(T) = (Z Z||E(Y|X, Z)z−T(E(Y|X, Z)z, z)||2 2dλ(z))1 2 (52) = (Z Z||E(Y|X, Z)z−E(Y|X, Z)z||2 2dλ(z))1 2. (53) As shown in Lemma 3.1, Vis the minimum increase of L2error (or, in physics, the minimum work/energy required) to deform E(Y|X, Z) to satisfy statistical parity. Before showing the main result, we need to define the geodesic on metric space to show the explicit form of constant speed geodesic on the Wasserstein space, which plays a key role in the proof. Definition 4.2 (Constant-speed geodesic between two points on metric space) Given a metric space (X, d)andx, x′∈X, the constant-speed geodesic between xandx′ is a continuously parametrized path {xt}t∈[0,1]such that x0=x,x1=x′, and d(xs, xt) = |t−s|d(x, x′),∀s, t∈[0,1]. The following lemma, which is well known as the McCann (displacement) interpolation [41, Chapter 7] in the optimal transport literature, shows that a linear interpolation using the optimal transport plan results in the constant-speed geodesic on the Wasserstein space. Lemma 4.1 (Constant-speed geodesic on Wasserstein space, [32, 41]) Given µ0, µ1∈ (P2(Rd),W2)andγthe optimal transport plan in between, let πt(x, y) := (1 −t)x+ty, then µt:= (πt)♯γ, t∈[0,1] (54) is the constant-speed geodesic between µ0andµ1. Proof See Appendix B Remark 4.1 (Linear interpolation formula for W2deodesics) If there exists an optimal transport map Tsuch that T♯(µ0) =µ1, then the McCann interpolation has the simple form µt= ((1−t)Id+tT)♯µ0, t∈[0,1]. (55) We apply this simple formula to obtain a closed-form estimation of the Pareto frontier in algorithm design, see Section 6. Now, we are ready to establish the main result, which shows that Vis a lower bound ofL(f(X, Z)) +1√ 2D(f(X, Z), Z) for any measurable function f:X × Z → Y and is achieved along the constant-speed geodesics from the sensitive marginals of the conditional expectation to their barycenter on the Wasserstein space. 30Fair Data Representation for Machine Learning at the Pareto Frontier Theorem 4.1 ( W2geodesics characterization of a linear Pareto frontier) Define L, D, V as above and assume µz∈ P2,ac(Y), λ−a.e.. It follows that V≤L(f(X, Z)) +1√ 2D(f(X, Z), Z) (56) for any measurable function f:X ×Z → Y . Furthermore, define T(t)such that T(t)(·, z) := (1−t)Id+t(T(·, z)), t∈[0,1]is the linear interpolation between the identity map and the optimal transport map for λ-a.e. z∈ Z, then equality holds in (56) if and only if f(X, Z) =T(t)(E(Y|X, Z), Z), t∈[0,1]as L(T(t)) =tL(T(1)) = tV (57) 1√ 2D(T(t)) =1√ 2(1−t)D(T(0)) = (1 −t)V. (58) Proof See Appendix B. Remark 4.2 (Intuition of Theorem 4.1: a Euclidean analog) Here, we provide a Euclidean analog of Theorem 4.1. In fact, our proof is based on the observation of the analog and equivalent to it when one considers x→δxas an embedding from XtoP2(X). LetX:={xi}N i=1be a fixed data set on the Euclidean space X(N= 3in Figure 2), ˜X:={˜xi}N i=1be a data set consisting of Narbitrarily chosen data points on X, and define the following: •[Euclidean analog of V] std(X) := (1 NPN i=1||xi−mx||2)1 2withmx:=1 NPN i=1xi, •[Euclidean analog of L] solid (˜X) := (1 NPN i=1||xi−˜xi||2)1 2, •[Euclidean analog of D] dotted (˜X) := (1 N2PN i,j=1||˜xi−˜xj||2)1 2. 31Xu and Strohmer Figure 2: In this figure, we have three data points on an Euclidean space traveling along straight lines (Euclidean geodesics) to their average (Euclidean barycenter). Define (1) std := the standard deviation of the three points, (2) solid line (loss) := the average moving (Euclidean) distance away from their original location, and (3) dotted line (disparity) := the average pairwise (Euclidean) distance among them. One can show that std ≤solid +1√ 2dotted where equality holds if and only if the three points travel at constant-speed along straight lines to their average. It is straight-forward to verify that (1) std(X)≤solid(˜X)|{z} utility loss+1√ 2dotted (˜X)|{z} disparity, and (2) equality holds if and only if ˜X=X(t) :={(1−t)xi+tmx}N i=1fort∈[0,1]as loss(X(t)) =tstd(X)and1√ 2disparity (X(t)) = (1 −t)std(X). Since V(the minimum work or energy required for statistical parity) is fixed for the data ( X, Y, Z ) when one applies ( X, Z) to predict Y, the above theorem implies that the Pareto frontier between the increased prediction error L(T) and the remaining disparity D(T) is a line that results from the constant-speed geodesics from the marginal conditional expectations to their barycenter on the Wasserstein space. In particular, let T(t)(E(Y|X, Z), Z) :={T(t)(E(Y|Xz), z)}z,λ-a.e., t∈[0,1], we arrive at a closed-form solution to Problem 2: Corollary 4.1 (Pareto optimal fair L2-objective learning) Given (X, Y, Z )satisfying µz∈ Pac,λ-a.e., then fd(X, Z) :=( T(1−d√ 2V)(E(Y|X, Z), Z),ifd∈[0,√ 2V] E(Y|X, Z), ifd∈(√ 2V,∞)(59) are the unique solutions to Problem 2 for d∈[0,∞). Proof Ifd∈(√ 2V,∞), then it follows from Theorem 4.1 that D(E(Y|X, Z)) =D(T(0)) =√ 2V < d . Hence, Problem 2 reduces to the unconstrained L2projection problem and the 32Fair Data Representation for Machine Learning at the Pareto Frontier optimal solution is E(Y|X, Z). Now, for a fixed d∈[0,√ 2V], assume for contradiction that ∃f∈L2(X × Z ,Y) such that ||Y−f(X, Z)||2 2<||Y−T(t)(E(Y|X, Z), Z)||2 2 fort= 1−d√ 2V. Then, let f(X, Z) denote the Wasserstein barycenter of {f(X, Z)z}z, we have ||Y−f(X, Z)||2 2≤ ||Y−f(X, Z)||2 2+||f(X, Z)−f(X, Z)||2 2 <||Y−T(t)(E(Y|X, Z), Z)||2 2+||f(X, Z)−f(X, Z)||2 2 =||Y−E(Y|X, Z)||2 2+L(T(t)) +1√ 2D(f(X, Z)) =||Y−E(Y|X, Z)||2 2+ (V−1√ 2d) +1√ 2d =||Y−E(Y|X, Z)||2 2+V where the second line follows from the assumption, the third from L2orthogonal decomposition and Theorem 4.1, and the forth from the assumption and Theorem 4.1. The strict inequality above contradicts the optimality of E(Y|X, Z) shown in Lemma 3.1. That proves the optimality of T(1−d√ 2V)(E(Y|X, Z), Z) for the fixed d. Uniqueness result follows from the uniqueness of E(Y|X, Z) shown in Lemma 3.1. Since the choice of d∈[0,√ 2V] is arbitrary, we are done. We note that Corollary 4.1 together with Lemma 4.1 and Remark 4.1 provide a postprocessing approach to (estimate) the Pareto frontier: applying McCann interpolation to the Brenier’s maps between the learning outcome sensitive marginals {E(Y|X, Z)z}zand their (pseudo-) barycenter. One can apply Algorithm 1 directly with the learning outcome marginals as inputs. From a theoretical perspective, various metrics of disparity that differ from D, the Wasserstein disparity (Definition 4.1), can be used and the theoretical results derived in this section provide a lower bound estimation for the Pareto frontier that uses other disparity metrics. The quality of the lower bound can be studied using the relationship between the Wasserstein distance and the defined disparity metric. Also, the present work provides a numerical study on the lower bound estimation in Section 6 to which we refer the interested readers for more details. In practice, various metrics of disparity are adopted, such as the prediction success ratio (difference from 1) in classification [13] and the Kolmogorov-Smirnov distance for 1dimensional regression [18]. The proposed estimation of the Pareto frontier leaves the choice ofαto practitioners who would face specific fairness requirements and disparity metrics. 5. Optimal Fair Data Representation for Supervised Learning In this section, we study the optimal fair data representation problem, Problem 3, that is motivated by the current challenges in the pre-processing or synthetic data design approach to fair machine learning. To solve the problem, we first characterize the exact solution 33Xu and Strohmer using a dependent and independent Wasserstein barycenter pair, see Lemma 5.3. Then, we define a dependent and independent pseudo-barycenter pair via optimal affine maps, and prove that the pair is the exact optimal fair data representation with Gaussian marginals, cf. Lemma 5.5 and the optimal affine estimate of the representation with general marginals in Theorem 5.2. 5.1 Wasserstein Barycenter Pair Characterization We will prove a characterization of the solutions to Problem 3. To start, notice that since (˜X, Z) =T⊗Id|Z(X, Z) for some measurable map T⊗Id|Z:X × Z → X × Z , we have σ((˜X, Z))⊂σ((X, Z)). Also, from ˜X⊥Z, we have σ(˜X)⊂σ(˜X)⊗σ(Z) =σ((˜X, Z)). Therefore, σ(˜X)⊂σ((X, Z)) and it follows from L2orthogonal decomposition that ||Y−E(˜Y|˜X)||2 2=||Y−E(Y|X, Z)||2 2+||E(Y|X, Z)−E(˜Y|˜X)||2 2. (60) The first term on the right hand side can be interpreted as the minimum loss of information by using ( X, Z) to predict Y. Furthermore, one can decompose the second term on the right hand side of (60): ||E(Y|X, Z)−E(˜Y|˜X)||2 2 =||E(Y|X, Z)−E(Y|˜X, Z)||2 2+||E(Y|˜X, Z)−E(˜Y|˜X)||2 2 =||E(Y|X, Z)−E(Y|˜X, Z)||2 2+Z Z||E(Yz|˜X)−E(˜Y|˜X)z||2 2dλ(z). Here, the first equality follows from L2orthogonal decomposition. The second equality follows from disintegration, the fairness constraint ˜X,E(˜Y|˜X)⊥Z, and the fact that ˜X⊥Z implies E(Yz|˜X) =E(Y|˜X, Z)z. See Appendix C for the proof. Now, the key observation is that, given a fixed ˜X⊥Z, the choice of ˜Ydepends only on the second term on the right, which forms a Wasserstein barycenter problem with marginals being {E(Yz|˜X)}z. Hence, the optimal choice of ˜Yis the one which satisfies E(˜Y|˜X) = E(Y|˜X, Z), where E(Y|˜X, Z)) is the Wasserstein barycenter of {E(Yz|˜X)}z. Therefore, we denote the optimal choice of ˜Yto be ¯Ywhich satisfies E(¯Y|˜X) =E(Y|˜X, Z). It remains to find the optimal choice of ˜X. The following result shows that the optimal choice is the one admissible ˜Xwhich generates the finest sigma-algebra. Lemma 5.1 (Finer sigma-algebra, more accurate optimal fair learning) Let˜X,˜X′∈ {˜X∈ D|X:˜X⊥Z}. Ifσ(˜X′)⊂σ(˜X), then ||E(Y|X, Z)−E(¯Y|˜X)||2 2≤ ||E(Y|X, Z)−E(¯Y′|˜X′)||2 2 (61) where ¯Yand¯Y′satisfy E(¯Y|˜X) =E(Y|˜X, Z)andE(¯Y′|˜X′) =E(Y′|˜X′, Z). Proof See Appendix C. 34Fair Data Representation for Machine Learning at the Pareto Frontier Therefore, it is clear that our optimal choice of ˜Xis the one that generates the finest sigma-algebra while satisfying ˜X⊥Z. The following technical lemma shows that the barycenter of {Xz}z∈Zis one of the optimal choices. Lemma 5.2 ( ¯Xgenerates the finest sigma-algebra among admissible) If{L(Xz)}z ⊂ P 2,ac(X)λ-a.e., then σ((¯X, Z)) = σ((X, Z)). In addition, σ(˜X)⊂σ(¯X)for all ˜X∈ {˜X∈ D|X:˜X⊥Z}. Proof See Appendix C. Therefore, Lemma 5.1, Lemma 5.2, and the choice of ¯Yabove together provide a characterization of the solution to Problem 3. Lemma 5.3 (Characterization of optimal fair data representation) Let ¯Xand E(Y|¯X, Z)denote the respective Wasserstein barycenter of {Xz}zand{E(Yz|¯X)}z. If {L(Xz)}z⊂ P 2,ac(X)and{L(E(Y|¯X, Z)z)}z⊂ P 2,ac(Y), then the following are equivalent: •(˜X,˜Y)∈arg min(˜X,˜Y)∈D{||Y−E(˜Y|˜X)||2 2:˜X,E(˜Y|˜X, Z)⊥Z}. •(˜X,˜Y)∈ {(˜X,˜Y)∈ D:σ(˜X) =σ(¯X),E(˜Y|¯X) =E(Y|¯X, Z)}. In Lemma 5.3, the choice of ¯Xis not unique. In fact, any random variable ˜Xthat satisfies σ(˜X) =σ(¯X) can be our choice according to Lemma 5.1 and Lemma 5.2. This is because any ˜Xthat satisfies the above conditions gives E(Y|˜X) =E(Y|¯X). For both theoretical and computational convenience, we fix our choice to be ¯Xfrom now on. Remark 5.1 (Application of the optimal fair representation characterization to algorithm design) In theory, we should always take ¯Xbecause we prove that ¯Xgenerates the finest sigma-algebra among all the admissible ˜Xthat is independent of Z. Especially when working with data sets with clear high-dimensional structure such as image data, one should apply more complicated models to estimate the optimal transport map instead of using affine maps. But when working with data with less high-dimensional structure such as tabular data, we hope to take advantage of the simplicity, robustness, and interpretability of linear maps in practice and hence restrict the admissible transport maps to be affine, as mentioned in Remark 3.1. Therefore, we showed that the pseudo-barycenter X†, which is equal to ¯Xin the Gaussian case and solves a relaxed version of the barycenter problem in the general distribution case, can be achieved using optimal affine maps. As a result, we apply X†in the algorithm design and experiments. Still, if there is no concern about over-fitting or computational cost, it is recommended for strict statistical parity guarantee purposes to compute ¯Xto improve the result. Now, it remains to find ¯Yto obtain the optimal fair data representation characterized by Lemma 5.3. In general, it is difficult to find E(Y|¯X, Z), not to mention find a ˜Y satisfying E(˜Y|¯X) =E(Y|¯X, Z). The key observation here is that if the Brenier’s maps 35Xu and Strohmer {Ty|¯X(·, z)}zthat push {E(Yz|¯X)}zforward to E(Y|¯X, Z) are affine, then a straight-forward choice in ¯Yis{Ty|¯X(Yz, z)}z∈Z=Ty|¯X(Y, Z). This step is the key to circumvent the postprocessing nature. Therefore, following the same derivation of (41) from (1) in Section 3 to guarantee feasibility of affine maps, we relax the fairness constraint to the first two moments in Problem 3, and show a pseudo-barycenter pair provides us an exact solution to Problem 3 in the Gaussian marginal case and the optimal affine estimation in the general marginal case. 5.2 Fairness with Gaussian Marginals Assume {(Xz, Yz)}zto be non-degenerate Gaussian vectors λ-a.e. and define the following: Definition 5.1 (Independent pseudo-barycenter: X†) X†:=Tx(X, Z), (62) where Tx(·, z) := Σ−1 2 Xz(Σ1 2 XzΣΣ1 2 Xz)1 2Σ−1 2 Xz(63) andΣis the unique solution to Z Z(Σ1 2ΣXzΣ1 2)1 2dλ(z) = Σ . (64) Definition 5.2 (Dependent pseudo-barycenter: Y†) Y†:=Ty|X†(Y, Z) (65) where Ty|X†(·, z) := Σ−1 2 Yz|X†(Σ1 2 Yz|X†ΣΣ1 2 Yz|X†)1 2Σ−1 2 Yz|X† (66) with ΣYz|X†:= ΣYzX†Σ−1 X†ΣT YzX†, and Σis the unique solution to Z Z(Σ1 2ΣYz|X†Σ1 2)1 2dλ(z) = Σ (67) Here, to obtain (an estimation of) the solution to equations (67) and (64), we apply the iterative method (33) in Remark 2.6 when designing our algorithm in Section 6. Since it is a direct result of Lemma 2.3 that X†=¯X, the goal is now to show that E(Y†|¯X) =E(Y|¯X, Z), (68) and therefore by Lemma 5 .3 to conclude E(Y†|X†) =E(Y†|¯X) indeed minimizes the estimation error while staying independent of Z. To prove the above equation and justify the definition of the pseudo-barycenter, we need the following results: (1) existence and uniqueness of both ¯XandE(Y|¯X, Z); (2) affinity of the corresponding Brenier’s maps Tx(·, z) and Ty|X†(·, z). By assumption, we have{L(Xz)}z⊂ P 2,ac(X), and {L(E(Yz|¯X))}z⊂ P 2,ac(Y). The existence and uniqueness 36Fair Data Representation for Machine Learning at the Pareto Frontier then follow directly from Lemma 2 .1. It remains to show that the corresponding Brenier’s maps are affine. But by Lemma 2 .3, if{Xz}zand{E(Yz|¯X)}zboth are from some locationscale family, then the barycenters are also from the corresponding location-scale family and the Brenier’s maps are affine. The following result shows that if {Yz}zcome from the same location-scale family, then {E(Yz|¯X)}zalso belongs to the same location-scale family. Lemma 5.4 (Conditional expectation preserves location-scale family) Assume that {Yz}z⊂ F(P0)for some P0, then {E(Yz|¯X)}z⊂ F(L(E(Yz|¯X)))for any z. Proof This follows immediately from the existence of positive definite affine transformations among {Yz}z, Lemma 2 .2, and the linearity of conditional expectation. Therefore, given {(Xz, Yz)}zbeing Gaussian vectors, we have {(¯X, Y z)}being Gaussian vectors, which further implies that {E(Yz|¯X)}zare Gaussian vectors by Lemma 5.4. (We note that it is not necessary to apply Lemma 5.4 to show {E(Yz|¯X)}zare Gaussian because it is a well-known result in probability theory, but the lemma becomes necessary later in the case of general marginal distributions.) Lemma 5.5 (Solution to the optimal fair data representation in the Gaussian case) Let{(Xz, Yz)}zbe Gaussian vectors satisfying Σz≻0λ-a.e., then there exists a unique barycenter pair (¯X,E(Y|¯X, Z))which are Gaussian vectors characterized by the covariance matrix being the unique solution to Z Z(Σ1 2SΣ1 2)1 2dλ(z) = Σ (69) forS∈ {ΣXz,ΣYz|X†}respectively, where ΣYz|X†= ΣYzX†Σ−1 X†ΣT YzX†. Moreover, {Tx(·, z)}z and{Ty|X†(·, z)}zwhich push XzandE(Yz|¯X)respectively to ¯XandE(Y|¯X, Z)are affine with closed-form (63) and(66). As a result, for λ−a.e. z ∈ Z, we have E(Y|¯X, Z)z=Ty|X†(E(Yz|Tx(Xz, z)), z) =E(Ty|X†(Yz, z)|Tx(Xz, z)) (70) Proof The existence, uniqueness, and Gaussianity of the barycenter follow from Lemma 2.3, whereas the affinity of corresponding Brenier’s maps results from Lemmas 5 .4 and 2 .2. The above result provides us a theoretical foundation to apply the affine maps {Tx(·, z)}z and{Ty|X†(·, z)}zto{Xz}zand{Yz}zrespectively as a pre-processing step before the training step. Furthermore, notice that although Ty|X†(E(Yz|¯X), z) =E(Yz|¯X, Z)zλ-a.e. by construction,{Ty|X†(Yz, z)}zdoes not agree in general: for z1̸=z2, Ty|X†(Yz1, z1)̸=Ty|X†(Yz2, z2). (71) The pseudo-barycenter solves the disagreement by merging them directly. Despite of the differences among {Ty|X†(Yz, z)}z, the L2projections of them on σ(¯X) agree. Therefore, a direct merging of {Ty|X†(Yz, z)}zis simply: Ty|X†(Y, Z) =Y†. It follows: 37Xu and Strohmer E(Y†|X†) =E(Y†|¯X) =E(Ty|X†(Y, Z)|¯X) =Z ZE(Ty|X†(Yz, z)|¯X)dλ(z) =Z ZTy|X†(E(Yz|¯X), z)dλ(z) =Z ZTy|X†(E(Y|¯X, Z)z, z)dλ(z) =Z ZE(Y|¯X, Z)zdλ(z) =E(Y|¯X, Z), where the second equality follows from disintegration, the third from linearity of Ty|¯X, and the forth from E(Yz|¯X) =E(Y|¯X, Z)z. Therefore, we have proved a result that justifies the definition of the pseudo-barycenter: Theorem 5.1 (Justification of Y†in Gaussian case) (X†, Y†)is a solution to Problem 3 inf (˜X,˜Y)∈D{||Y−E(˜Y|˜X)||2 2:˜X,E(˜Y|˜X, Z)⊥Z}, (72) if{(Xz, Yz)}zare non-degenerate Gaussian vectors. 5.3 The Case of General Distribution In practice, one should not always expect the sensitive marginal data distributions to be Gaussian, and the results we derived under the assumption of Gaussianity may not apply to the general marginal distribution case. Instead, we solve the following relaxed optimal fair data representation problem: inf (˜X,˜Y)∈D{||Y−E(˜Y|˜X)||2 2:m˜X, m˜Y|˜X,Σ˜X,Σ˜Y|˜X⊥Z}, (73) where m˜Y|˜X:=E(E(˜Y|˜X, Z)) and similarly for Σ ˜Y|˜X, to find the optimal affine estimation of the true solution to the original Problem 3. The fairness guarantee of the affine estimation is the same as mentioned in Remark 3.2. Now, we justify the pseudo-barycenter pair ( X†, Y†) in the case of general distributions by proving it is a solution to the relaxed optimal fair L2-objective supervised learning problem (73). To start, notice that ( X†, Y†)∈ Dand satisfies mX†, mY†|X†,ΣX†,ΣY†|X†⊥ Zby construction and therefore is admissible. Remark 5.2 (Finest sigma-algebra vs. most variance) Due to the relaxation, the admissible ˜X∈ D| Xare no longer required to be independent of Z. Furthermore, without the assumption of Gaussianity, X†is no longer equal to ¯X. As a result, although one can still prove σ((X, Z)) = σ((X†, Z))by following the same argument in the proof of Lemma 5.2 as in the Gaussian case, but this fact now cannot imply σ(˜X)⊂σ(X†)due to the lack of independence condition. Instead, the present work shows that Var( ˜X)≤Var(X†)for all admissible ˜X∈ D| X, which in general implies σ(˜X)⊂σ(X†). For example, whenever set 38Fair Data Representation for Machine Learning at the Pareto Frontier inclusion forms an order between σ(˜X)andσ(X†), then it is true that Var( ˜X)≤Var(X†) implies σ(˜X)⊂σ(X†). As a result, we still fix X†as our optimal choice among all the admissible ˜X∈ D|X. In addition, for any Σ ≻0, define TΣ,x:= Σ−1 2 Xz(Σ1 2 XzΣΣ1 2 Xz)1 2Σ−1 2 Xz(74) TΣ:= Σ−1 2 Yz|X† z(Σ1 2 Yz|X† zΣΣ1 2 Yz|X† z)1 2Σ−1 2 Yz|X† z(75) where ΣYz|X† z:=E((E(Yz|X† z)−mYz)(E(Yz|X† z)−mYz)T) and E(Yz|X† z) :=E(Y|X†, Z)z. Now, the goal is to show ( X†, Y†) is indeed a solution to the relaxed problem (73), under the following two assumptions: 1 Set inclusion forms an order between X†and all ˜X∈ {˜X∈ D|X:m˜X,Σ˜X⊥Z}. 2 ΣYz|X† z= ΣYzX† zΣ−1 X† zΣT YzX† z. Remark 5.3 (Applicability of the assumptions) For the first assumption, Lemma 5.6 below guarantees that X†generates the finest sigma-algebra among all the admissible sigmaalgebras. In other words, for any admissible ˜X, either it generates a coarser sigma-algebra than σ(X†)or the two sigma-algebras do not contain each other. In other words, there is no admissible ˜Xsuch that σ(X†)⊂σ(˜X). The second assumption allows us to directly compute the covariance matrix of E(Yz|X† z) from ΣYzX† zandΣX† z. The second assumption is necessary to keep our pre-processing approach. In general, E(Yz|X† z)is not a linear function of X† zas in the Gaussian case. When the second assumption is not true, our pre-processing approach uses ΣYzX† zΣ−1 X† zΣT YzX† zas our best affine estimate of ΣYz|X† z. To that end, we need the following result on the relationship among the variance of the original distribution, the variance of the barycenter, and the Wasserstein distance. Lemma 5.6 (Variance reduction of Wasserstein barycenter [39]) Given Xsatisfies {L(Xz)}z⊂ P 2,ac(X)and ¯Xsatisfies L(¯X)being the Wasserstein barycenter of {L(Xz)}, it follows that ||X−E(X)||2 2− ||¯X−E(¯X)||2 2=Z ZW2 2(L(Xz),L(¯X))dλ(z) (76) As a result, we obtain the following: Lemma 5.7 ( X†Contains the largest variance among admissible) X†is the unique solution to sup ˜X∈D|X{Var( ˜X) :m˜X,Σ˜X⊥Z}. (77) 39Xu and Strohmer Proof To simplify notation, by the invariance of variance under translation and Lemma 2.1, we can assume without loss of generality that mXz= 0λ−a.e.in the rest of the proof, which only deal with variance and Wasserstein distance. Now, for λ−a.e. z ∈ Z, we have ||Xz−TΣ,x(Xz, z)||2 2=||Xz||2 2+||TΣ,x(Xz, z)||2 2−2⟨Xz, TΣ,x(Xz, z)⟩2 = Trace(Σ Xz) + Trace(Σ) −2E(XT zTΣ,x(Xz, z)) = Trace(Σ Xz) + Trace(Σ) −2⟨TΣ,x,ΣXz⟩F = Trace(Σ X′z) + Trace(Σ) −2⟨TΣ,x,ΣX′z⟩F =||X′ z−TΣ,x(X′ z, z)||2 2 =W2 2(L(X′ z),L(TΣ,x(X′ z))) where X′∼ N (mX,ΣX) is the Gaussian analog of Xand⟨·,·⟩Fis the Frobenius inner product. Similarly, by the disintegration theorem, we also have for S∈ {X, X†} Var(S) =||S||2 2=Z Z||Sz||2 2dλ=Z ZTrace(Σ Sz)dλ. (78) Therefore, it follows from Lemma 5.6 that Var(X)−Var(X†) = Var( X′)−Var(( X′)†) = Var( X′)−Var( ¯X′) =Z ZW2 2(L(X′ z),L(¯X′))dλ(z). Finally, assume there exists a ˜X∈ D| Xsuch that Var( X†)≤Var( ˜X). It follows Var(X′)−Var( ˜X′)≤Var(X′)−Var(( X′)†) = Var( X′)−Var( ¯X′). But since m˜X′,Σ˜X′⊥Z, we have ˜X′⊥Zas˜X′is Gaussian by construction. In other words, there exists a ˜X′⊥Z such that Z ZW2 2(L(X′ z),L(˜X′))dλ(z)≤Z ZW2 2(L(X′ z),L(¯X′))dλ(z) (79) which contradicts the uniqueness of ¯X′. The above lemma shows that Var( ˜X)≤Var(X†) for all admissible ˜X∈ D| Xsatisfies m˜X,Σ˜X⊥Z, which together with the first assumption imply σ(˜X)⊂σ(¯X) in practice. Therefore, from now on, we fix the choice of ˜Xto be X†and prove the general characterization result based on the two assumptions listed above. It remains to justify the choice of Y†. To do so, we need the following lemma, which provides a multi-marginal characterization of the optimal affine map. Lemma 5.8 (Projection Lemma for conditional expectations) Given mYz|X† z= 0 andΣYz|X† z≻0λ-a.e., for any Σ≻0, 40Fair Data Representation for Machine Learning at the Pareto Frontier inf E(˜Y|X†):Σ˜Yz|X† z=ΣZ ZW2 2(L(E(Yz|X† z)),L(E(˜Yz|X† z)))dλ(z) (80) admits a unique solution, denoted by Y† Σ, that has the form Y† Σ:=TΣ(Y, Z) (81) where TΣ(·, z) := Σ−1 2 ˜Yz|X† z(Σ1 2 ˜Yz|X† zΣΣ1 2 ˜Yz|X† z)1 2Σ−1 2 ˜Yz|X† z Proof This is a direct corollary from Lemma 3.3. Finally, we are ready to prove the justification of the pseudo-barycenter in the case of general distributions. Theorem 5.2 (Justification of (X†, Y†)in general distribution case) E(Y†|X†)is a solution to inf (˜X,˜Y)∈D{||Y−E(˜Y|˜X)||2 2:m˜X, m˜Y|˜X,Σ˜X,Σ˜Y|˜X⊥Z} (82) under the assumptions: (1) set inclusion forms an order between X†and all ˜X∈ {˜X∈ D|X:m˜X,Σ˜X⊥Z}; and (2) ΣYz|X† z= ΣYzX† zΣ−1 X† zΣT YzX† z. Proof The choice of X†follows from the first assumption and Lemma 5.7. It remains to show that Y†is a solution to inf ˜Y∈D|Y{||Y−E(˜Y|X†)||2 2:m˜Y|X†,Σ˜Y|X†⊥Z} (83) Fix Σ ≻0 arbitrary, we have ||Y−E(Y† Σ|X†)||2 2− ||Y−E(Y|X†)||2 2=Z Z||E(Yz−Y† Σ,z|X† z)||2 2dλ(z) (84) and it follows from Lemma 5 .8 that Z Z||E(Yz−Y† Σ,z|X† z)||2 2dλ(z) =Z ZW2 2(L(E(Yz|X† z)),L(TΣ(E(Yz|X† z), z))dλ(z) = min ν:Σνz=ΣZ ZW2 2(L(E(Yz|X† z)), νz)dλ(z) Therefore, (73) boils down to the following: inf Σ≻0{Z Z||E(Yz−Y† Σ,z|X† z)||2 2dλ(z)}. (85) 41Xu and Strohmer Finally, notice that Z Z||E(Yz−Y† Σ,z|X† z)||2 2dλ(z) =Z Z||E(Yz|X† z)−TΣ(E(Yz|X† z), z)||2 2dλ(z) =Z Z||E(Yz|X† z)||2 2+||TΣ(E(Yz|X† z), z)||2 2−2⟨E(Yz|X† z), TΣ(E(Yz|X† z), z)⟩2dλ(z) =Z ZTrace(ΣYz|X† z) + Trace(Σ) −2E(E(Yz|X† z)TTΣ(E(Yz|X† z), z))dλ(z) =Z ZTrace(ΣYz|X† z) + Trace(Σ) −2⟨TΣ,ΣYz|X† z⟩Fdλ(z) =Z Z||E(Yz|X† z)′−TΣ(E(Yz|X† z)′, z)||2 2dλ(z) where ⟨·,·⟩Fdenotes the Frobenius inner product and X′∼ N(mX,ΣX) denotes the Gaussian analog of X. It follows from the definition of Y†and Lemma 2 .3 thatR Z||E(Yz− Y† z|X†)||2 2dλ(z) is the lower bound of (85). The proof is complete. To conclude, given an arbitrary L2-objective supervised learning model that aims to estimate conditional expectation, the training via ( X†, Y†) results in an estimate of E(Y|¯X, Z). In other words, any supervised learning model trained via ( X†, Y†) is guaranteed to be independent of Zin the location-scale family marginal case (or, to have first two moments independent of Zin the general marginal case), while resulting in the minimum prediction error among all the admissible functions of some specific model due to the training step. Here, the assumption is that the test sample distribution is the same as the training sample distribution, which is a ubiquitous assumption for machine learning. 5.4 Optimal Fair Data Representation at the Pareto Frontier Finally, we extend the pseudo-barycenter pair, which is the solution to the optimal fair data representation, to the fair data representation at the Pareto frontier using McCann interpolation via a similar approach as we derived the post-processing Pareto frontier in Section 4. But notice a direct application of Theorem 4.1 does not work here because there is no direct interpolation between E(Y|X, Z) and E(Y|¯X, Z) due to the change of the underlying sigma-algebra. Therefore, we apply a diagonal argument, Remark 5.4, to estimate the interpolation between E(Y|X, Z) and E(Y|¯X, Z) and thus the fair data representation at the Pareto frontier. To start, we derive the following post-processing optimal trade-off result directly from Theorem 4.1 for a fixed choice of ˜X∈ {˜X∈ D|X:˜X⊥Z}. For any f:X × Z → Y , define Ly|˜X,Dy|˜X, and Vy|˜Xas follows: Ly|˜X(f(˜X, Z)) := (Z Z||E(Yz|˜X)−f(˜X, Z)z||2 2dλ(z))1 2 (86) Dy|˜X(f(˜X, Z)) := (Z Z2W2 2(f(˜X, Z)z1, f(˜X, Z)z2)dλ(z1)dλ(z2))1 2. (87) 42Fair Data Representation for Machine Learning at the Pareto Frontier To simplify notation, for any T′:Y × Z → Y , we also define the following: Ly|˜X(T′) := (Z Z||E(Yz|˜X)−T′ z(E(Yz|˜X))||2 2dλ(z))1 2 (88) Dy|˜X(T′) := (Z Z2W2 2((T′ z1)♯L(E(Yz1|˜X)),(T′ z2)♯L(E(Yz2|˜X)dλ(z1)dλ(z2))1 2.(89) Also, let Tdenote the optimal transport map from {E(Yz|˜X)}zto the barycenter E(Y|˜X, Z), letT(t), t∈[0,1] be the McCann interpolation, and define Vy|˜X:=Ly|˜X(T) = (Z Z||E(Yz|˜X)−Tz(E(Yz|˜X))||2 2dλ(z))1 2 (90) = (Z Z||E(Yz|˜X)−E(Y|˜X, Z)||2 2dλ(z))1 2. (91) Then the result below follows directly similar to the proof of Theorem 4.1. Corollary 5.1 (Pareto frontier for conditional expectation on fixed sigma-algebra) Given Ly|˜X,Dy|˜X, and Vy|˜Xdefined above, we have Vy|˜X≤Ly|˜X(f(˜X, Z)) +1√ 2Dy|˜X(f(˜X, Z)) (92) where equality holds if and only if f(˜X, z) =T(t)(E(Yz|˜X), z)λ-a.e. for t∈[0,1]as Ly|˜X(T(t)) =tLy|˜X(T(0)) = tVy|˜X, (93) 1√ 2Dy|˜X(T(t)) =1√ 2(1−t)Dy|˜X(T(0)) = (1 −t)Vy|˜X. (94) The above result shows that by fixing ˜X∈ {˜X∈ D| X:˜X⊥Z}, the McCann interpolation between IdandTy|˜Xyields the Pareto frontier from E(Y|˜X, Z) toE(Y|˜X, Z), which is a weak version of the true frontier from E(Y|X, Z) toE(Y|˜X, Z). The only difficulty remaining is to coarsen the underlying sigma-algebra from σ(X, Z) toσ(¯X). But by Remark 5.2, we know that one can coarsen the sigma-algebra by reducing the variance. Therefore, we apply a diagonal argument to estimate the McCann interpolation between ( X, Y) and (¯X,¯Y). Remark 5.4 (Diagonal estimate of the post-processing Pareto frontier) The key observation is that the optimal affine transport map that pushes (X, Y)forward to (X†, Y†) is the pair (Tx, Ty|¯X). Therefore, McCann interpolation between Id and Txcan optimally reduce variance and thereby coarsen σ((X, Z))toσ(X†), whereas the interpolation betwen Id and Ty|¯Xforms an estimation of the geodesic path between YandY†. Therefore, the present work matches the two interpolations diagonally (Tx(t), Ty|¯X(t)) := ((1 −t)Idx+tTx,(1−t)Idy+tTy|¯X), to estimate the true optimal fair data representation at the Pareto frontier. 43Xu and Strohmer Finally, since X†andE(Y†|X†) are the estimation of ¯XandE(Y|¯X, Z), respectively, as shown in the last section, it follows from Corollary 5.1 and Remark 5.4 that E(Ty|¯X(t)(Y, Z)|Tx(t)(X, Z)), t∈[0,1] (95) provides a pre-processing estimate of the Pareto frontier from E(Y|X, Z) toE(Y|¯X, Z) that is characterized by Theorem 4.1. 6. Algorithm Design In this section, we propose two algorithms based on the theoretical results above. Algorithm 2 is designed for the fair learning outcome in the post-processing approach and for the dependent variable in fair data representation, whereas Algorithm 1 is designed for the independent variable in fair data representation. 1. For practitioners who want to generate fair learning outcomes along the Pareto frontier, Algorithm 2 takes the learning outcomes marginals {f(X, Z)z}zas input and outputs the learning outcomes at (the optimal affine estimation of) the post-processing estimation of the Pareto frontier: {f(X, Z)(t)}t∈[0,1], which is the Wasserstein geodesic paths from the original learning outcome, f(X, Z)(0), to the estimate of the optimal fair learning outcome, f(X, Z)(1). Here, f(X, Z)(1) is the best estimate of the optima fair learning outcome based on the provided learning outcome {f(X, Z)z}z. 2. For practitioners who want to generate a fair data representation, Algorithm 1 and Algorithm 2 take in respectively the marginal independent and dependent data: {Xz}z and{Yz}z, then outputs respectively the independent and dependent data representations along the Wasserstein geodesics from the marginals to their pseudo-barycenter: {(X†(t), Y†(t))}t∈[0,1]. So that any conditional expectation estimation supervised learning model trained via {(X†(t), Y†(t))}t∈[0,1]results in (an diagonal affine estimation of) the learning outcome at the Pareto frontier. The choice of the Frobenius norm in Step 1 is due to computational efficiency. Any matrix norm would work. Remark 6.1 (Solution to alternative fair data representation constraint) In Section 1.3, the present work shows two alternative fair data representation constraints: (1) (˜X,˜Y)⊥Zand (2) ˜X⊥Z, which offer different trade-offs between fairness protection and utility. If a practitioner applies the alternative constraint, the proposed algorithms can be applied to generate (the optimal affine estimation of) corresponding fair data representation as the following: 1 For (˜X,˜Y)⊥Z, one applies Algorithm 1 to both {(Xz, Yz)}z. This alternative is especially useful when practitioners or data publishers do not know which features would be chosen as independent or dependent. 2 For ˜X⊥Z, one applies Algorithm 1 to {Xz}zand leaves {Yz}untouched. 44Fair Data Representation for Machine Learning at the Pareto Frontier Algorithm 1: Pseudo-Barycenter Geodesics for Independent Variable Input: marginal data sets {Xz}z, stop criterion ϵ; Step 1: Find the optimal barycenter covariance; Initialization: δ=∞, Σ = rand orId while δ > ϵ do Σnew=1 |X|P z|Xz|(Σ1 2ΣXzΣ1 2)1 2; //(33) δ=||Σ−Σnew||F; Σ = Σ new; end Step 2: Find the optimal affine transport maps; Tz= Σ−1 2 Xz(Σ1 2 XzΣΣ1 2 Xz)1 2Σ−1 2 Xz; //(63) Step 3: Find the geodesic path to independent pseudo-barycenter; X† z(t) =Tz(t)(Xz−mXz) +mX; //(62) where Tz(t) := (1 −t)Id+tTz,t∈[0,1]; //(55) Step 4 (optional): For binary rows Xi∈I, reshape ( X†(t))ito binary by randomized rounding for all i∈I; For all Xibinary: p(t) =(X† z(t))i max(( X† z(t))i)−min(( X† z(t))i), (X† z(t))i∼Bernoulli( p(t)); Step 5 (optional): If sensitive information needs to be attached, merge the marginals back with mitigating Z; X† z(t) = (Xz(t), z(t)) where z(t) = (1 −t)(z−mZ) +mZ,t∈[0,1] Output: {{X† z(t)}z∈Z}t∈[0,1] 45Xu and Strohmer Algorithm 2: Dependent (or Post-processing) Pseudo-Barycenter Geodesics Input: marginal data sets {Yz}z(post-processing: {f(X, Z)z}z), stop criterion ϵ; Step 1: Find the optimal barycenter covariance; Initialization: δ=∞, Σ = rand orId while δ > ϵ do Σnew=1 |Y|P z|Yz|(Σ1 2ΣYz|X† zΣ1 2)1 2 //(33) (post-processing: Σ new=1 |Y|P z|f(X, Z)z|(Σ1 2Σf(X,Z)zΣ1 2)1 2); δ=||Σ−Σnew||F; Σ = Σ new; end Step 2: Find the optimal affine transport maps; Tz= Σ−1 2 Yz|X† z(Σ1 2 Yz|X† zΣΣ1 2 Yz|X† z)1 2Σ−1 2 Yz|X† z//(66) (post-processing: Tz= Σ−1 2 f(X,Z)z(Σ1 2 f(X,Z)zΣΣ1 2 f(X,Z)z)1 2Σ−1 2 f(X,Z)z); //(38) Step 3: Find the geodesic path to dependent pseudo-barycenter; Y† z(t) =Tz(t)(Yz−mYz) +mY //(65) where Tz(t) := (1 −t)Id+tTz, t∈[0,1] //(55) (post-processing: f(X, Z)z(t) =Tz(t)(f(X, Z)z−mf(X,Z)z) +mf(X,Z)); //(37) Step 4 (optional): For binary rows Yi∈I(post-processing: ( f(X, Z))i∈I), reshape (Y†(t))i(post-processing: ( f(X, Z)(t))i∈I) to binary by randomized rounding for alli∈I; For all Yibinary: p(t) =(Y† z(t))i max(( Y† z(t))i)−min(( Y† z(t))i), (Y† z(t))i∼Bernoulli( p(t)); Output: {{Y† z(t)}z∈Z}t∈[0,1](post-processing: {{f(X, Z)z(t)}z∈Z}t∈[0,1]) 46Fair Data Representation for Machine Learning at the Pareto Frontier 7. Empirical Study: Fair Supervised Learning In this section, we present numerical experiments with the proposed Algorithms 1 and 2 from Section 6. The proposed fair data representation method is bench-marked against two baselines: 1. the prediction model trained via the original data (denoted by “supervised learning name” in the experiment result figure below): supervised learning models trained via data including the sensitive variable provide an estimation of statistical disparity resulting from both disparate treatment and impact. 2. the prediction model trained via data excluding the sensitive variable (denoted by “supervised learning name + Excluding Z”): supervised learning models trained via data excluding the sensitive variable provide an estimation of statistical disparity resulting from only disparate impact. 7.1 Benchmark Data and Comparison Methods For comparison, we implement the following known methods for different types of supervised learning tests: 1. For classification test, the present work compares the current state-of-the-art preprocessing methods [13, 44] (“supervised learning name + Calmon or Zemel”, the later is also known as “Learning Fair Representation”) with the proposed fair data representation methods (“supervised learning name + pre-proc. Pareto frontier Est. or Pseudo-barycenter”). 2. For uni-variate regression test, we compare the post-processing Wasserstein barycenter based fair regression [18] (“supervised learning name + Chzhen”) with the proposed post-processing pseudo-barycenter methods (“supervised learning name + post-proc. Pareto frontier Est. or Pseudo-barycenter”) and the fair data representation methods. 3. For multi-variate supervised learning test, we compare the post-processing pseudobarycenter methods with the fair data representation methods. The reasons for this choice are as follows: (1) the known attempts via the pre-processing approach are only available for fair classification; (2) the post-processing Wasserstein barycenter based methods on fair classification are analogous to the one on fair regression, which is shown to outperform other in-processing or post-processing methods in reducing discrimination while preserving accuracy; (3) there exists no practical attempt along the Wasserstein characterization approach to multi-dimensional supervised learning due to the computational complexity of finding the barycenter and the optimal transport maps. We adopt the following metrics of accuracy and discrimination that are frequently used in fair machine learning experiments on various data sets: (1) For fair classification, the prediction accuracy, and statistical disparity are quantified respectively by AUC (area under the Receiver Operator Characteristic curve) and Definition 7.1 (Classification discrimination) Discrimination = max z,z′∈ZP(ˆYz= 1) P(ˆYz′= 1)−1 47Xu and Strohmer as defined in [13]. (2) For univariate supervised learning, the prediction error and statistical disparity are quantified respectively by MSE (mean squared error, equivalent to the squared L2norm on sample probability space) and KS (Kolmogorov-Smirnov) distance as in [18] for indirect comparison purpose. So that readers can compare the proposed methods indirectly with other methods that are tested in [13, 18, 44] and their references. (3) For univariate and multivariate supervised learning, the prediction error and statistical disparity are quantified respectively by L2andW2(Wasserstein) distances, which are the quantification the current work adopts to prove the Pareto frontier in the above sections. In addition, we perform tests on four benchmark data sets: CRIME, LSAC, Adult, COMPAS, which are also frequently used in fair learning experiments. A brief summary is given below. For all the test results, we apply 5-fold cross-validation with 50% training and 50% testing split, except for 90% training and 10% testing split in the linear regression test on LSAC due to the high computational cost of the post-processing Wasserstein barycenter method [18]. Therefore, interested readers can also compare the pseudo-barycenter test results indirectly to other methods tested in [13, 18]. Data set Tests Data size dim( X) dim( Y) UCI Adult logit regression, random forest 162805 16 1 COMPAS logit regression, random forest 26390 7 1 LSAC linear regression, ANN 20454 9 1 CRIME linear regression, ANN 1994 97 1 CRIME linear regression, ANN 1994 87 11 •Communities and Crime Data Set (CRIME) contains the social, economic, law executive, and judicial data of communities in the United States with 1994 examples [35]. The task of univariate learning is to predict the number of crimes per 105population using the rest of the information on the data set. Here, race is the sensitive information and, for (indirect) comparison purposes, we made race a binary categorical variable of whether the percentage of the African American population (racepctblack) is greater than 30%. In multivariate supervised learning on CRIME, we keep the same sensitive variable. But the learning task is to predict the following vector that represents the local housing and rental market information: (low quartile occupied home value, median home value, high quartile home value, low quartile rent, median rent, high quartile rent, median gross rent, number of immigrants, median number of bedrooms, number of vacant households, number of crimes). •LSAC National Longitudinal Bar Passage Study data set (LSAC) contains social, economic, and personal data of law school students with 20454 examples [42]. The goal of univariate models is to predict the students’ GPA using other information on the data set. Here, race is the sensitive variable and, for (indirect) comparison purposes, we make it a binary variable on whether the student is non-white. •UCI Adult Data Set (Adult) contains the 1994 Census data with 162805 examples [7]. The goal is to predict the binary categorization (whether gross annual income is 48Fair Data Representation for Machine Learning at the Pareto Frontier greater than 50k) using age, education years, and gender, where gender is the sensitive information. •Correctional Offender Management Profiling for Alternative Sanctions (COMPAS) is a benchmark set of data from Broward County, Florida for algorithmic bias studies [5]. Following [13], the goal here is to predict whether an individual would commit any violent crime while race is the sensitive binary variable (African-American and Caucasian). 7.2 Numerical Result In this subsection, we summarize the experimental results3. The classification test result is summarized in Figure 3 below. Here, the vertical and horizontal axes are AUC and Discrimination defined in Definition 7 .1. That is, the more upper-left, the better the result. The first row of Figure 3 shows the results of logistic regression (left) and random forest (right) on Adult whereas the second shows the corresponding results on COMPAS. 3. The code for the results of our experiments is available online at: github.com/xushizhou/fair_data_ representation 49Xu and Strohmer Figure 3: As shown in the classification test above, the proposed fair data representation method (+ Pre-proc. Pareto frontier Est. or Pseudo-barycenter) outperforms the other methods (+ Zemel or + Calmon) in estimating the optimal fair learning outcome. It reduces the Discrimination metric to nearly zero while keeping the relatively high level of AUC with both logistic regression (LR) and random forest (RF) on both Adult and COMPAS. Furthermore, fair data representation method offers flexibility in choosing the desired trade-off while other methods only estimate a random point near the Pareto frontier. We note that there exists a large disparate impact in the learning outcome on COMPAS due to the relatively small difference between the “Discrimination” of learning outcome on the original data (LR and RF) and the outcome on the data excluding Z(LR and RF + Excluding Z). Therefore, a further reduction of statistical disparity is needed. In contrast, the relatively large difference in the Adult data set implies a small disparate impact. That is, a simple exclusion of the sensitive variable Zresults in a significant improvement in fairness. For further reduction of statistical disparity, it is clear from the experiment results on both COMPAS and Adult that the estimation via the Wasserstein geodesics to Pseudobarycenter (LR and RF + Pseudo-barycenter) consistently outperforms LR and RF + Calmon by obtaining lower Discrimination with higher AUC. In addition, although “LR and RF + Zemel” achieves a point near the Pareto frontier estimated by the proposed Pseudo-barycenter methods, the point estimation is rather random. Hence, “+ Zemel” is not consistent in estimating the optimal fair learning outcome (the end point of the Pareto curve). Practitioners cannot know which point on the Pareto frontier is estimated by “+ Zemel”. In comparison, the pseudo-barycenter methods are consistent in estimating the optimal fair learning outcome. In addition, they providef 50Fair Data Representation for Machine Learning at the Pareto Frontier the entire Pareto frontier, and hence offer practitioners the flexibility to choose the desired trade-off. Moreover, the proposed method works for any model that aims to estimate conditional expectation, including classification and regression, while “+ Zemel” only works for classification. The univariate regression test result on the LSAC and the one on CRIME are shown respectively in Figure 4 and 5 below. Here, the vertical and horizontal axes in the first rows are MSE and KS distance. The corresponding axes in the second row are the L2-quantified test error and the W2distance that quantifies the remaining statistical disparity among sensitive groups. Therefore, the more lower-left, the better is the result in both rows. The two supervised learning methods we use are linear regression and artificial neural networks (ANN with 4 linearly stacked layers where each of the first three layers has 32 units all with ReLu activation while the last has 1 unit with linear activation). Figure 4: As shown in the univariate regression test on LSAC above, the proposed fair data representation method (+ pre-proc. Pareto frontier Est. or Pseudo-barycenter) and the post-processing pseudo-barycenter geodesics method (+ post-proc. Pareto frontier Est. or Pseudo-barycenter) achieved similar performance as the exact barycenter method (+ Chzhen). The proposed methods outperformed “+ Chzhen” with linear regression and were exceeded with the artificial neural network, both by a narrow margin. But the performance of the proposed methods is achieved at 0.0128% of the time costs “+ Chzhen” (see Figure 7 below). In addition, the proposed methods offer the flexibility of choosing the desired (optimal) trade-off between utility loss (MSE or L2-loss) and statistical disparity (KS or W2 distance), whereas “+ Chzhen” only estimate the end point of the Pareto curve. In the regression tests, post-processing Pareto frontier estimation via ANN is smooth while the pre-processing estimation is not. Here, the smoothness is due to the McCann interpolation between the identity matrix and the optimal transport map in the post-processing 51Xu and Strohmer approach. The non-smoothness is due to the randomness in training the neural network. When testing fair data representations via ANN, one has to train the neural network for the data representation at every time t∈[50]. Hence, the randomness in ANN training results in the non-smoothness in the Pareto frontier estimation via fair data representations. On the LSAC data set, the proposed methods (+ pre-proc. Pseudo-barycenter and + post-proc. Pseudo-barycenter) obtains a similar performance as the post-processing exact Wasserstein barycenter method (+ Chzhen): the proposed methods outperformed the exact method in the linear regression test and were outperformed by the exact method in the nonlinear artificial neural network tests, which is consistent with our theoretical results. But the performance of the proposed methods is achieved at 0 .81 seconds on average, whereas the average time cost of “+ Chzhen” is 6365 .98 seconds (see Figure 7 below). In addition, we gained the flexibility in choosing the desired trade-off, computational efficiency, model selection, parameter tuning, and composition. Figure 5: As shown above, the fair data representation method ( + pre-proc. Pareto frontier Est. or Pseudobarycenter) achieved the same, if not better, performance as the exact barycenter method (+ Chzhen) in estimating the optimal learning outcome. In addition, the fair data representations method offers flexibility in choosing a desired (optimal) trade-off between utility and fairness. For CRIME data, the small difference between the KS of learning outcome on the original data (LR and ANN) and the one on the data excluding the sensitive variable (LR and ANN + Excluding Z) implies a significant disparate impact. This observation and the multi-dimensional test below agree with the following statement in [17]: “Simply removing 52Fair Data Representation for Machine Learning at the Pareto Frontier the ‘protected attribute’ is insufficient. As long as the model takes in features that are correlated with, say, gender or race, avoiding explicitly mentioning it will do little good.” In Figure 5, it is clear that the fair data representation methods (+ pre-proc. Pareto frontier Est. or Pseudo-barycenter) achieved the same, if not better, performance as the comparison method (+ Chzhen): the proposed method was outperformed by “+ Chzhen” with linear regression and outperformed “+ Chzhen” with artificial neural network, both by a narrow margin. But the performance of the fair data representation method is achieved at 4.735% of the time costs “+ Chzhen.” In addition, the fair data representation method provides (an estimation of) the entire Pareto frontier and works for multivariate supervised learning (see Figure 6 below), whereas “+ Chzhen” only estimates the end point of the Pareto frontier and only works in the univariate learning. Remark 7.1 One possible explanation for the proposed method to outperform the exact post-processing Wasserstein barycenter method (“+ Chzhen”) is the following: Although [18] is designed specifically for univariate learning and the KS distance by matching the sensitive marginal cumulative distribution functions, such matching on training data can lead to over-fitting. Therefore, the resulting optimal transport map fits the training data too well to be optimal for the test data. Next, we show the multivariate supervised learning on CRIME data to provide a highdimensional baseline, to which later proposed machine learning fairness methods on highdimensional data can compare. The vertical and horizontal axes are the L2test error and theW2distance among sensitive groups. Hence, the more lower-left, the better the result. Figure 6: As shown above, the fair data representation method (+ pre-proc. Pareto frontier Est. or Pseudo-barycenter) achieves similar performance to the post-processing pseudo-barycenter method (+ post-proc. Pareto frontier Est. or Pseudo-barycenter). Due to the relatively high dimensionality of X(87-dimensional) and Y(11-dimensional), the probabilistic dependence and correlation between the learning outcome and the sensitive variable Zbecomes more difficult to remove. It is clear that (LR or ANN + Excluding Z) now removes almost none of the statistical disparity compared to the learning outcome on the original data. 53Xu and Strohmer To show the difference in practical computational cost among the comparison methods, we include the following processing time table, where the unit of time is second, and the simulations were run on a 2019 Macbook pro with Intel i9 processor. Figure 7: As shown in the table above, the computational cost of the pseudo-barycenter method is significantly lower than the cost of the known post-processing methods: on average 7836 times faster on LSAC and 21 times faster on CRIME in a single train-test cycle for a single supervised learning model. Furthermore, in model selection or composition, the pre-processing time is a fixed one-time cost while the post-processing time is additive. (See point 4 below for a more detailed explanation) Now, we show the major advantages of the proposed method compared to the postprocessing ones, such as [18, 28, 24]: 1. Flexibility in Trade-off: The pre-processing method provides an estimation for the entire Pareto frontier and thereby allows practitioners to balance between prediction error and disparity. In contrast, the known post-processing method merely estimates the starting (left) point of the frontier. 2. Sensitive data privacy protection: The geodesics to the pseudo-barycenter allow practitioners to suppress the sensitive information remaining in the data to the desired level. That is, given the resulting suppressed data, anyone who has leaked data from the training or decision stage can merely extract the level of sensitive information up to the pre-determined remaining level. For example, if one chooses to suppress as much sensitive information as possible by setting t= 1, then it follows from the construction of dependent and independent pseudobarycenter, it is guaranteed that any unsupervised learning method that uses only the first two moments of the sample data distribution, such as the K-means and PCA, would be unable to extract any information about Zfrom X†orfY†(X†). 3. Computational efficiency in high-dimensional learning: As summarized in Figure 7, the computation of the pseudo-barycenter estimation of the optimal fair learning outcome is significantly faster than the computation of the exact barycenter via the post-processing matching cdf approach, especially on the LSAC data which has a larger sample size. 4. Flexibility in model selection, modification, and composition: in practice, one needs to repeat the training process multiple times to compare different supervised learning algorithms or parameters. The proposed fair data representation method has a fixed 54Fair Data Representation for Machine Learning at the Pareto Frontier pre-processing time while the processing time of post-processing methods is additive. For example, if a practitioner needs to compare linear regression and ANN on LSAC as shown in Figure 7 and repeat the training process Ntimes for parameter tuning or validation purpose, the total processing time for pseudo-barycenter method is 0.81 +N(0.0025 + 104 .2) while the processing time for the post-processing method is N(0.003 + 6380 .61 + 105 .738 + 6351 .36). Acknowledgement The authors want to thank the referees, whose profound and detailed feedback greatly enhanced the quality and clarity of this paper. The authors acknowledge support from NSF DMS-2027248, NSF DMS-2208356, NSF CCF-1934568, NIH R01HL16351, and DESC0023490. A. Appendix: Proof of Results in Section 2 A.1 Proof of Lemma 2.1 Proof W2 2(µ, ν) =Z ||x−y||2dγ∗(x, y) =Z ||((x−mµ)−(y−mν)) + ( mµ−mν)||2dγ∗(x, y) =Z ||(x−mµ)−(y−mν)||2dγ∗(x, y) +||mµ−mν||2 ≥ W2 2(µ′, ν′) +||mµ−mν||2 =Z ||x−y||2d(γ′)∗(x, y) +||mµ−mν||2 =Z ||(x+mµ)−(y+mν)||2d(γ′)∗(x, y) ≥ W2 2(µ, ν) where γ∗and ( γ′)∗denote the optimal transport plan for ( µ, ν) and ( µ′, ν′), respectively. The first inequality results from the fact that γ′(x, y) :=γ∗(x−mµ, y−mν)∈Q(µ′, ν′), the second inequality from γ(x, y) := ( γ′)∗(x+mµ, y+mν)∈Q(µ, ν), and the equalities from direct expansion. A.2 Proof of Lemma 2.3 Proof Existence and uniqueness follow directly from Theorem 2.1. For the equivalent multi-marginal coupling problem, there exists an optimal solution γ∗=L({Xz}z). It follows from Remark 2.3 that ¯X=T({Xz}z) where L(¯X) is the Wasserstein barycenter. Therefore, the Gaussianity of barycenter results from linearity of Tin the finite |Z|case, and the fact that the set of Gaussian distribution is closed in ( P2,ac,W2) when |Z|is infinite. The 55Xu and Strohmer characterization equation is proved in the case of finite |Z|in [2]. For infinite |Z|, the equation still holds due to the continuity of the covariance function on ( P2,ac,W2). The sufficiency and necessity of the equation follows from the following characterization of the barycenter via Brenier’s maps {T¯XXz}zderived in [2]: Z ZT¯XXzdλ(z) =Id. (96) It follows from the explicit form of {T¯XXz}zin Lemma 2 .2 that Z ZT¯XXzdλ(z) =Z ZΣ−1 2¯X(Σ1 2¯XΣXzΣ1 2¯X)1 2Σ−1 2¯Xdλ(z) =Id ⇐⇒Σ1 2¯XΣ−1 2¯XZ Z(Σ1 2¯XΣXzΣ1 2¯X)1 2dλ(z)Σ−1 2¯XΣ1 2¯X= Σ1 2¯XIdΣ1 2¯X ⇐⇒Z Z(Σ1 2¯XΣXzΣ1 2¯X)1 2dλ(z) = Σ ¯X. B. Appendix: Proof of Results in Section 4 B.1 Proof of Lemma 4.1 Proof First, it follows from the triangle inequality that W2(µ0, µ1)≤ W 2(µ0, µs) +W2(µs, µt) +W2(µt, µ1) for any s, t∈[0,1]. On the other hand, it follows from the definition of µtthat for s, t∈[0,1] W2 2(µs, µt)≤Z (Rd)2||x−y||2d(πs)♯γ(x)⊗d(πt)♯γ(y) =Z (Rd)2||πs(x, y)−πt(x, y)||2dγ(x, y) =Z (Rd)2||(1−s)x+sy−(1−t)x−ty||2dγ(x, y) =Z (Rd)2||(t−s)x−(t−s)y||2dγ(x, y) =|t−s|2Z (Rd)2||x−y||2dγ(x, y) =|t−s|2W2 2(µ0, µ1), where the first equation results from definition of W2. Given the above two facts, we complete the proof by contradiction. Assume ∃s, t∈[0,1] such that W2(µs, µt)<|t− s|W2(µ0, µ1), then W2(µ0, µ1)≤ W 2(µ0, µs) +W2(µs, µt) +W2(µt, µ1) <|s|W2(µ0, µ1) +|t−s|W2(µ0, µ1) +|1−t|W2(µt, µ1) =W2(µ0, µ1). 56Fair Data Representation for Machine Learning at the Pareto Frontier B.2 Proof of Theorem 4.1 Proof First, we derive the inequality from the triangle inequality and the optimality of {T(·, z)}z: Let f:X × Z → Y be an arbitrary measurable function. It follows that V≤(Z Z||E(Y|X, Z)z−f(X, Z)z||2 2dλ(z))1 2 ≤L(f(X, Z)) + (Z Z||f(X, Z)z−f(X, Z)z||2 2dλ(z))1 2 ≤L(f(X, Z)) + (Z ZW2 2(L(f(X, Z)z),L(f(X, Z)z))dλ(z))1 2 =L(f(X, Z)) + (1 2Z Z2W2 2(L(f(X, Z)z1),L(f(X, Z)z2))dλ(z1)dλ(z2))1 2 =L(f(X, Z)) +1√ 2D(f(X, Z)). Here, the penultimate equation results from the fact that, for any {νz}z⊂ P 2,ac(Rd), Z Z2W2 2(νz1, νz2)dλ(z1)dλ(z2) = 2Z ZW2 2(νz,¯ν)dλ(z), (97) where ¯ νis the Wasserstein barycenter of {νz}z. Now, we show that the lower bound is achieved if and only if f(X, Z) =T(t)(E(Y|X, Z), Z), t∈[0,1]. Let t∈[0,1],Tz:=T(·, z), andµz:=L(E(Y|X, Z)z). It follows from Lemma 4.1 and Remark 4.1 that: V= (Z ZW2 2(µz,¯µ)dλ(z))1 2 ≤(Z ZW2 2(µz, Tz(t)♯µz)dλ(z))1 2+ (Z ZW2 2(Tz(t)♯µz,¯µ)dλ(z))1 2 = (t2Z ZW2 2(µz,¯µ)dλ(z))1 2+ ((1−t)2Z ZW2 2(µz,¯µ)dλ(z))1 2 =tV+ (1−t)V=V. Therefore, the second inequality is an equality where the first term is L(T(t)): L(T(t)) = (Z Z||E(Y|X, Z)z−Tz(t)(E(Y|X, Z)z)||2 2dλ(z))1 2 = (Z ZW2 2(µz, Tz(t)♯µz)dλ(z))1 2 =t(Z ZW2 2(µz,¯µ)dλ(z))1 2=tV. 57Xu and Strohmer For the second term, we claim that it equals1√ 2D(T(t)). To see this, we need to first show Tz(t)♯µz= ¯µ. Indeed, if not, thenR ZW2 2(Tz(t)♯µz,Tz(t)♯µz)dλ(z) is strictly less thanR ZW2 2(Tz(t)♯µz,¯µ)dλ(z) by the definition and uniqueness of Tz(t)♯µz. It follows that (Z ZW2 2(µz,Tz(t)♯µz)dλ(z))1 2 ≤(Z ZW2 2(µz, Tz(t)♯µz)dλ(z))1 2+ (Z ZW2 2(Tz(t)♯µz,Tz(t)♯µz)dλ(z))1 2 <L(T(t)) + (Z ZW2 2(Tz(t)♯µz,¯µ)dλ(z))1 2 =(Z ZW2 2(µz,¯µ)dλ(z))1 2, which contradicts the definition and uniqueness of ¯ µ. Therefore, D(T(t)) = (Z Z2W2 2(Tz1(t)♯µz1, Tz2(t)♯µz2)dλ(z1)dλ(z2))1 2 = (2Z ZW2 2(Tz(t)♯µz,Tz(t)♯µz)dλ(z))1 2 =√ 2(Z ZW2 2(Tz(t)♯µz,¯µ)dλ(z))1 2 =√ 2((1−t)2Z ZW2 2(µz,¯µ)dλ(z))1 2 =√ 2(1−t)V. That completes the proof. C. Appendix: Proof of Results in Section 5 C.1 Proof of ˜X⊥Zimplies E(Yz|˜X) =E(Y|˜X, Z)z Proof Let ˜X⊥Zand assume for contradiction that E(Yz|˜X)̸=E(Y|˜X, Z)z. Then, we have ||Y−E(Y|˜X, Z)||2 2=Z Z||Yz−f∗(˜X, Z)z||2 2dλ =Z Z||Yz−f∗(˜X, z)||2 2dλ >Z Z||Yz−E(Yz|˜X)||2 2dλ =Z Z||Yz−˜fz(˜X)||2 2dλ 58Fair Data Representation for Machine Learning at the Pareto Frontier where the first line follows from disintegration and the fact that there exists a measurable function f∗:X × Z → Y such that f∗(˜X, Z) =E(Y|˜X, Z), the second from ˜X⊥Z, the third line follows from orthogonal projection property of conditional expectation and the assumption, and the forth from the fact that there exists a measurable function ˜fz:X → Y such that ˜fz(˜X) =E(Yz|˜X). Now, define ˜f:X × Z → Y by˜f(·, z) := ˜fzforλ-a.e. z∈ Z. It follows that ||Y−E(Y|˜X, Z)||2 2>Z Z||Yz−˜fz(˜X)||2 2dλ =Z Z||Yz−˜f(˜X, z)||2 2dλ =||Y−˜f(˜X, Z)||2 2 =||Y−E(Y|˜X, Z)||2 2+||E(Y|˜X, Z)−˜f(˜X, Z)||2 2. That implies ||E(Y|˜X, Z)−˜f(˜X, Z)||2 2<0, a contradiction. This completes the proof. C.2 Proof of Lemma 5.1 Proof Let˜X,˜X′∈ {˜X∈ DX:˜X⊥Z}satisfy σ(˜X′)⊂σ(˜X). We have ||E(Y|X, Z)−E(¯Y|˜X, Z)||2 2− ||E(Y|X, Z)−E(¯Y′|˜X′, Z)||2 2 =||E(Y|X, Z)−E(Y|˜X, Z)||2 2− ||E(Y|X, Z)−E(Y|˜X′, Z)||2 2 Notice that ||E(Y|X, Z)−E(Y|˜X, Z)||2 2=||E(Y|X, Z)−E(Y|˜X, Z)||2 2+Z ZW2 2(µz,¯µ)dλ where µz:=L(E(Y|˜X, Z)z) and ¯ µ:=L(E(Y|˜X, Z)). Also, we define µ′ zand ¯µ′analogously to have ||E(Y|X, Z)−E(Y|˜X′, Z)||2 2 =||E(Y|X, Z)−E(Y|˜X′, Z)||2 2+Z ZW2 2(µ′ z,¯µ′)dλ =||E(Y|X, Z)−E(Y|˜X, Z)||2 2+||E(Y|˜X, Z)−E(Y|˜X′, Z)||2 2+Z ZW2 2(µ′ z,¯µ′)dλ. Combining the above, we have ||E(Y|X, Z)−E(¯Y|˜X, Z)||2 2− ||E(Y|X, Z)−E(¯Y′|˜X′, Z)||2 2 =Z ZW2 2(µz,¯µ)dλ−Z ZW2 2(µ′ z,¯µ′)dλ− ||E(Y|˜X, Z)−E(Y|˜X′, Z)||2 2. It remains to show thatR ZW2 2(µz,¯µ)dλ <R ZW2 2(µ′ z,¯µ′)dλ+||E(Y|˜X, Z)−E(Y|˜X′, Z)||2 2. Indeed, assume for contradiction thatR ZW2 2(µ′ z,¯µ′)dλ+||E(Y|˜X, Z)−E(Y|˜X′, Z)||2 2≤ 59Xu and Strohmer R ZW2 2(µz,¯µ)dλ, then we have Z ZW2 2(µz,¯µ′)dλ≤ ||E(Y|˜X, Z)−E(Y|˜X′, Z)||2 2+Z ZW2 2(µ′ z,¯µ′)dλ ≤Z ZW2 2(µz,¯µ)dλ. This contradicts the optimality and uniqueness of ¯ µby Lemma 3.1. Therefore, we prove by contradiction thatR ZW2 2(µz,¯µ)dλ <R ZW2 2(µ′ z,¯µ′)dλ+||E(Y|˜X, Z)−E(Y|˜X′, Z)||2 2and, hence, ||E(Y|X, Z)−E(¯Y|˜X, Z)||2 2− ||E(Y|X, Z)−E(¯Y′|˜X′, Z)||2 2<0. That completes the proof. C.3 Proof of Lemma 5.2 Proof We first prove σ((¯X, Z)) = σ((X, Z)). Since L(Xz)⊂ P 2,ac, it follows from Lemma 3.1 that there exists a measurable map T:X × Z → X such that T(Xz, z) =¯Xz λ-a.e., where ¯Xdenotes the Wasserstein barycenter of {Xz}z. Define T⊗Id|Z:X × Z → X ×Z , we have T⊗Id|ZisX ×Z /X ×Z -measurable and satisfies T⊗Id|Z((X, Z)) = ( ¯X, Z). That implies σ((¯X, Z))⊂σ((X, Z)). Furthermore, since L(¯X)∈ P 2,ac, it follows from Brenier’s theorem [11] that there exists T−1(·, z) such that T−1(¯Xz, z) = Xz. Therefore, we have ( T⊗Id|Z)−1=T−1⊗Id|ZisX × Z /X × Z -measurable and satisfies (T⊗Id|Z)−1((¯X, Z)) = ( X, Z). That implies σ((X, Z))⊂σ((¯X, Z)). That completes the proof of σ((¯X, Z)) =σ((X, Z)). Now, we show σ(˜X)⊂σ(¯X). From the construction of ˜X, we have σ((˜X, Z))⊂σ((¯X, Z)) =σ((X, Z)). But ˜X⊥Zimplies that, for any BX∈ BX, we can construct BX× Z ∈ B X⊗ BZ. In addition, due to σ((˜X, Z))⊂σ((¯X, Z)), there exists B′ XZ∈ BX⊗BZsuch that ( ¯X, Z)−1(B′ XZ) = (X, Z)−1(BX×Z). Lastly, ¯X⊥Zalso implies that there exists B′ X∈ BXsatisfying B′ XZ=B′ X× Z. It follows that ˜X−1(BX) = ( ˜X, Z)−1(BX× Z) = (X, Z)−1(B′ X× Z) =X−1(B′ X) (98) Since our choice of BX∈ BXis arbitrary, it follows that σ(˜X)⊂σ(¯X). Finally, since our choice of ˜X∈ {˜X∈ D|X:˜X⊥Z}is arbitrary, we are done. References [1] B. L. Adamson. Ricci v. DeStefano: Procedural Activism (?). National Black Law Journal (University of California, Los Angeles) , 24:11–01, 2011. [2] M. Agueh and G. Carlier. Barycenters in the Wasserstein space. SIAM Journal on Mathematical Analysis , 43(2):904–924, 2011. [3] J. M. Altschuler and E. Boix-Adsera. Wasserstein barycenters are NP-hard to compute. SIAM Journal on Mathematics of Data Science , 4(1):179–203, 2022. 60Fair Data Representation for Machine Learning at the Pareto Frontier [4] P. C. ´Alvarez-Esteban, E. Del Barrio, J. Cuesta-Albertos, and C. Matr´ an. A fixedpoint approach to barycenters in Wasserstein space. Journal of Mathematical Analysis and Applications , 441(2):744–762, 2016. [5] J. Angwin, J. Larson, S. Mattu, and L. Kirchner. Machine Bias. In Ethics of data and analytics , pages 254–264. Auerbach Publications, 2022. [6] J. B. Aristotle et al. The complete works of Aristotle , volume 2. Princeton University Press Princeton, 1984. [7] A. Asuncion and D. Newman. UCI machine learning repository, 2007. [8] R. Berk, H. Heidari, S. Jabbari, M. Joseph, M. Kearns, J. Morgenstern, S. Neel, and A. Roth. A convex framework for fair regression. arXiv preprint arXiv:1706.02409 , 2017. [9] R. Bhatia. Positive Definite Matrices . Princeton University Press, 2009. [10] A. W. Blumrosen. Strangers in paradise: Griggs v. Duke Power Co. and the concept of employment discrimination. Mich. L. Rev. , 71:59, 1972. [11] Y. Brenier. Polar factorization and monotone rearrangement of vector-valued functions. Communications on pure and applied mathematics , 44(4):375–417, 1991. [12] T. Calders and I. ˇZliobait˙ e. Why unbiased computational processes can lead to discriminative decision procedures. In Discrimination and Privacy in the Information Society: Data mining and profiling in large databases , pages 43–57. Springer, 2013. [13] F. Calmon, D. Wei, B. Vinzamuri, K. Natesan Ramamurthy, and K. R. Varshney. Optimized pre-processing for discrimination prevention. Advances in neural information processing systems , 30, 2017. [14] Y. Cao and J. Yang. Towards making systems forget with machine unlearning. In 2015 IEEE symposium on security and privacy , pages 463–480. IEEE, 2015. [15] G. Carlier and I. Ekeland. Matching for teams. Economic theory , 42:397–418, 2010. [16] A. Chouldechova and A. Roth. The frontiers of fairness in machine learning. arXiv preprint arXiv:1810.08810 , 2018. [17] B. Christian. The alignment problem: Machine learning and human values . WW Norton & Company, 2020. [18] E. Chzhen, C. Denis, M. Hebiri, L. Oneto, and M. Pontil. Fair regression with Wasserstein barycenters. Advances in Neural Information Processing Systems , 33:7321–7331, 2020. [19] J. M. Cooper, D. S. Hutchinson, et al. Plato: complete works . Hackett Publishing, 1997. 61Xu and Strohmer [20] J. A. Cuesta-Albertos, C. Matr´ an-Bea, and A. Tuero-Diaz. On lower bounds for the l2Wasserstein metric in a Hilbert space. Journal of Theoretical Probability , 9(2):263–283, 1996. [21] C. Dwork, M. Hardt, T. Pitassi, O. Reingold, and R. Zemel. Fairness through awareness. In Proceedings of the 3rd innovations in theoretical computer science conference , pages 214–226, 2012. [22] I. Ekeland. Existence, uniqueness and efficiency of equilibrium in hedonic markets with multidimensional types. Economic Theory , 42:275–315, 2010. [23] M. Feldman, S. A. Friedler, J. Moeller, C. Scheidegger, and S. Venkatasubramanian. Certifying and removing disparate impact. In proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining , pages 259–268, 2015. [24] T. L. Gouic, J.-M. Loubes, and P. Rigollet. Projection to fairness in statistical learning. arXiv preprint arXiv:2005.11720 , 2020. [25] S. Hajian and J. Domingo-Ferrer. A methodology for direct and indirect discrimination prevention in data mining. IEEE transactions on knowledge and data engineering , 25 (7):1445–1459, 2012. [26] M. Hardt, E. Price, and N. Srebro. Equality of opportunity in supervised learning. Advances in neural information processing systems , 29, 2016. [27] L. Hu and Y. Chen. A short-term intervention for long-term fairness in the labor market. In Proceedings of the 2018 World Wide Web Conference , pages 1389–1398, 2018. [28] R. Jiang, A. Pacchiano, T. Stepleton, H. Jiang, and S. Chiappa. Wasserstein fair classification. In Uncertainty in artificial intelligence , pages 862–872. PMLR, 2020. [29] F. Kamiran and T. Calders. Data preprocessing techniques for classification without discrimination. Knowledge and information systems , 33(1):1–33, 2012. [30] Y.-H. Kim and B. Pass. Wasserstein barycenters over Riemannian manifolds. Advances in Mathematics , 307:640–683, 2017. [31] T. Le Gouic and J.-M. Loubes. Existence and consistency of Wasserstein barycenters. Probability Theory and Related Fields , 168:901–917, 2017. [32] R. J. McCann. A convexity principle for interacting gases. Advances in mathematics , 128(1):153–179, 1997. [33] E. O. of the President. Big data: Seizing opportunities, preserving values. President PACT report , 2014. [34] B. Pass. Optimal transportation with infinitely many marginals. Journal of Functional Analysis , 264(4):947–963, 2013. 62Fair Data Representation for Machine Learning at the Pareto Frontier [35] M. Redmond and A. Baveja. A data-driven software tool for enabling cooperative information sharing among police departments. European Journal of Operational Research , 141(3):660–678, 2002. [36] F. Santambrogio. Optimal transport for applied mathematicians. Birk¨ auser, NY , 55 (58-63):94, 2015. [37] C. Silvia, J. Ray, S. Tom, P. Aldo, J. Heinrich, and A. John. A general approach to fairness with optimal transport. In Proceedings of the AAAI Conference on Artificial Intelligence , volume 34(04), pages 3633–3640, 2020. [38] L. Sweeney. Discrimination in online ad delivery: Google ads, black names and white names, racial discrimination, and click advertising. Queue , 11(3):10–29, 2013. [39] E. G. Tabak and G. Trigila. Explanation of variability and removal of confounding factors from data through optimal transport. Communications on Pure and Applied Mathematics , 71(1):163–199, 2018. [40] C. Villani. Topics in optimal transportation , volume 58. American Mathematical Soc., 2021. [41] C. Villani et al. Optimal transport: old and new , volume 338. Springer, 2009. [42] L. F. Wightman. LSAC National Longitudinal Bar Passage Study. LSAC Research Report Series. 1998. [43] M. B. Zafar, I. Valera, M. Gomez Rodriguez, and K. P. Gummadi. Fairness beyond disparate treatment & disparate impact: Learning classification without disparate mistreatment. In Proceedings of the 26th international conference on world wide web , pages 1171–1180, 2017. [44] R. Zemel, Y. Wu, K. Swersky, T. Pitassi, and C. Dwork. Learning fair representations. InInternational conference on machine learning , pages 325–333. PMLR, 2013. [45] N. Zhou, Z. Zhang, V. N. Nair, H. Singhal, J. Chen, and A. Sudjianto. Bias, Fairness, and Accountability with AI and ML Algorithms. arXiv preprint arXiv:2105.06558 , 2021. 63