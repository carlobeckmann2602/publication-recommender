arXiv:2312.11589v1 [cs.AI] 18 Dec 2023Moral Uncertainty and the Problem of Fanaticism Jazon Szabo1, Jose Such1,2, Natalia Criado2, Sanjay Modgil1 1King’s College London, UK 2VRAIN, Universitat Politecnica de Valencia, Spain {jazon.szabo,jose.such,sanjay.modgil }@kcl.ac.uk, {jsuch,ncriado }@upv.es Abstract While there is universal agreement that agents ought to act ethically, there is no agreement as to what constitutes ethi cal behaviour. To address this problem, recent philosophical a pproaches to ‘moral uncertainty’ propose aggregation of mul tiple ethical theories to guide agent behaviour. However, o ne of the foundational proposals for aggregation – Maximising Expected Choiceworthiness (MEC) – has been criticised as being vulnerable to fanaticism ; the problem of an ethical theory dominating agent behaviour despite low credence (conﬁdence) in said theory. Fanaticism thus undermines the ‘demo cratic’ motivation for accommodating multiple ethical per spectives. The problem of fanaticism has not yet been mathematically deﬁned. Representing moral uncertainty as an in stance of social welfare aggregation, this paper contribut es to the ﬁeld of moral uncertainty by 1) formalising the problem of fanaticism as a property of social welfare functionals an d 2) providing non-fanatical alternatives to MEC, i.e. Highe st k-trimmed Mean and Highest Median. 1 Introduction The recently proposed study of moral uncertainty represents a paradigm shift in how philosophers think about ethic s (MacAskill, Bykvist, and Ord 2020). Instead of aiming at a ‘one size ﬁts all’ approach, moral uncertainty acknowledge s that different ethical perspectives have differing streng ths and weaknesses, and that it is rarely the case that there is universal agreement on any given moral issue. Therefore, under moral uncertainty, an agent aggregates different eth ical perspectives so as to yield an overall evaluation. In par ticular such an agent has some credence (i.e. degree of acceptance) in not just one but rather multiple ethical theories. Each theory evaluates the agent’s available actions b y assigning each action a degree of choiceworthiness . A positive choiceworthiness denotes a good outcome while a negative choiceworthiness denotes a bad outcome; the larger the magnitude, the better or worse the outcome of the action, respectively. The agent chooses an action based on both the theories’ credences and the choiceworthiness of the action s. A fundamental question in moral uncertainty is how to trade off the theories’ credences and the actions’ choicewo rthiness when aggregating the evaluations of different ethi Copyright © 2024, Association for the Advancement of Artiﬁc ial Intelligence (www.aaai.org). All rights reserved.cal theories. The most inﬂuential proposal – Maximising Expected Choiceworthiness (MEC) (MacAskill 2014) – treats moral uncertainty analogously to empirical uncertainty. C redence corresponds to probability and choiceworthiness cor responds to utility, while MEC itself corresponds to maximising the expected utility. However, MEC is arguably unsuitable for agent decision making, given its vulnerability to the problem of fanaticism (Ross 2006; MacAskill, Bykvist, and Ord 2020). That is to say, under MEC, decision making can be dominated by theories that assign very high stakes to most moral situations (i.e. are evaluated as either extremely desirable or undesi rable), even if the agent has low credence in these theories. Hence, the agent’s behaviour may be completely determined by only a subset of low-credence theories while completely ignoring other theories. We concur with (MacAskill, Bykvist, and Ord 2020) that fanaticism is unacceptable, since it allows for theories to act as dictators or oligarchs (in the social choice sense of these terms). Thus fanaticism completely undermines democratic motivation for accommodating multiple ethical perspectives. Relatedly, fanaticism subverts any societal en dorsement and trust; a society is unlikely to accept agents which may entirely ignore ethical perspectives that the soc iety has high credence in. The study of machine ethics (Moor 2006) has sought to understand the moral implications of agent behaviour on human stakeholders, so as to design machines that can act ethically, even without human supervision (Anderson, Anderson, and Armen 2005). However, given prevalent signiﬁcant moral disagreement, both within and between societies (Haidt 2012)1, it is unclear as to exactly which ethical theory should guide agents’ actions (Ecoffet and Lehman 2021). This is especially troubling for machine ethics, where a com mon methodology has been to implement a single ethical theory. As emphasised in (Gabriel 2020), given the diversit y of ethical perspectives, an agent that acts according to a si ngle ethical theory may not receive societal endorsement, th us jeopardising the project of machine ethics. Indeed, recent work in machine ethics draws on insights from moral uncer1Indeed, the assumption that individuals apply a single ethical perspective is questionable (MacAskill, Bykvist, and Ord 2 020). Hence proposals for value alignment (e.g., (Hadﬁeld-Menel l et al. 2016)) necessitate the learning of individual’s ethical pr eferences.tainty (Bogosian 2017; Bhargava and Kim 2017; Martinho, Kroesen, and Chorus 2021; Dobbe, Gilbert, and Mintz 2020; Ecoffet and Lehman 2021), and advocate for the identiﬁcation of principles for the fair aggregation of various ethic al perspectives. Note that, despite the aforementioned issue s, all existing work lack a formal deﬁnition of fanaticism. Thus, we aim to remedy this limitation by formally deﬁning fanaticism. Moreover, we provide a critique of MEC as the foundational approach to moral uncertainty. While MEC’s vulnerability to fanaticism is widely known, no convincing alternatives have been presented. We therefore pro pose non-fanatical methods for resolving moral uncertaint y. In particular, we make the folllowing three contributions : 1)We formalise moral uncertainty as social welfare aggregation (as informally proposed by (MacAskill, Bykvist, and Ord 2020)). Drawing on accepted informal deﬁnitions, we formally deﬁne fanaticism as a property of social welfare functionals ( swfs). We do so by precisely deﬁning what it means for a theory to be held with ‘low credence’ (by giving a graded deﬁnition of fanaticism) and what it means for a theory to ‘dominate’ (by deﬁning dominant subsets). 2)We deﬁne novel, weighted versions of the swfs ktrimmed Highest Mean and Highest Median. 3)We prove that MEC is in fact maximally fanatical (‘Pascalian’ fanatical). We prove that neither novel weigh ted swfs are Pascalian and that Highest Median is not fanatical at all. We thereby argue in favour of replacing MEC with one of the proposed novel weighted swfs. The paper is structured as follows. Section 2 presents a running example that illustrates the fanaticism of MEC and introduces concepts and intuitions that will be referenced in later sections. Section 3 brieﬂy recapitulates background on social welfare aggregation. Moral uncertainty is then deﬁn ed in social welfare terms, and MEC is deﬁned as an instance of social welfare aggregation in Section 4. We also formalis e three weighted alternatives to MEC: Maximin, k-trimmed Highest Mean and Highest Median. We subsequently provide a formal deﬁnition of fanaticism in terms of swfs in Section 5.1. Then Section 5.2 states this paper’s key result s: relating to the extent to which the aforementioned swfs are vulnerable to fanaticism. 2 Running example We present a running example scenario to illustrate the intuitions underlying our formalism. Consider a small mobile ﬁreﬁghter robot FROBO assisting a ﬁre brigade. FROBO’s objective is to contain ﬁres in hard-to-access rooms in orde r to give human ﬁreﬁghters more time to reach these rooms. FROBO has a moral obligation to save lives when it can. However, different ethical perspectives imply different choices of action in order to comply with this obligation. For example, a version of utilitarianism (Sinnott-Armstrong 2022) interprets the obligation to save lives in terms of max imising the expected number of lives saved. Whereas according to a version of deontology (Alexander and Moore 2021), the obligation to save lives is interpreted as an abso lute obligation. That is to say, when encountering a personwho is in immediate danger of dying, then ( ceteris paribus2) FROBO has an absolute obligation to save that person (Kernohan 2021; Tarsney 2018). In concrete situations these dif ferent interpretations may contradict one another. For example, suppose that after climbing through rubble in a burning residential building, FROBO arrives in a smoke ﬁlled hallway. FROBO knows that ﬁreﬁghters will be able to clear the rubble and enter the hallway in about 5 minutes. Until then FROBO is on its own. The hallway has two doors; one on the right and one on the left. The right hand door is open and leads to a burning room containing a collapsed person. FROBO estimates that this room will be burnt out in less than 5 minutes (i.e. before the ﬁreﬁghters arrive), unl ess FROBO controls the ﬂames with its built-in extinguisher. On the other hand, the door to the leftis closed. FROBO knows that 4 people are listed as residents of this room. Furthermore, FROBO estimates that it can break down this door just in time for the ﬁreﬁghters arrive. However, the agent doesn’t know whether the residents are still home or have managed to escape. Based on the available information, the agent estimates that there is a 50% chance that the residents are still in the room. Unfortunately, FROBO only has time to exclusively attend either to the left or right room. While both of FROBO’s theories are in agreement that neither options yield an overall increase in ‘the good’, the y disagree on which action is less bad (i.e.‘better’). Accord ing to FROBO’s utilitarian calculations, the agent should choo se the left room: the agent can save an expected 2 people, which is better than the 1 person in the right room. According to FROBO’s deontological imperative, FROBO should choose the right room given the immediately apparent prospect of the right room occupant’s death, and the possibility that no one is in the left room.3 The exact numerical valuations in Table 1 are based on evaluations assigned in similar moral scenarios (e.g. in (MacAskill, Bykvist, and Ord 2020)). Importantly, the two theories’ evaluations starkly differ in their order of magn itude. FROBO’s implementation of deontology posits much higher stakes because the evaluations of deontology are sim ply a numerical proxy to the normative force of the obligations they represent4. Indeed, real life examples can be similarly or even more extreme. According to international law, the prohibition o n torture cannot be violated no matter the consequences, e.g. a nation state cannot sanction torture even if it is the only wa y to prevent existentially catastrophic consequences (Asse mbly et al. 1948). It is also worth noting that utilitarianism can also lead to extreme evaluations, such as those advocated by longtermism (MacAskill 2022). Thus the theories’ different orders of magnitude cannot be simply ‘normalised away’. 2E.g., if another action option implies saving 2 lives in imme diate danger rather than 1 life in immediate danger, then the ob ligation to save the two would take priority. 3Utilitarianism and Deontology cover a large family of ethic al varriants see (Sinnott-Armstrong 2022) and (Alexander and Moore 2021) respectively; FROBO’s versions are particular insta nces. 4This representation is inspired by threshold deontology (Alexander and Moore 2021), according to which while ethica l obligations are not absolute, they have a strong normative f orce.Utilitarianism Deontology Left room -1 -10000 Right room -2 -1000 Table 1: Evaluations FROBO’s ethical theories To demonstrate the problem of fanaticism, consider that deontology’s credence is very low (e.g. because the vast majority of FROBO’s stakeholders advocate utilitarianism ). Hence, FROBO assigns only 0.01credence to deontology and0.99credence to utilitarianism. However, the deontological evaluations can dominate FROBO’s behaviour. Using MEC, FROBO calculates the expected choiceworthiness – i.e. the credence-weighted sum of the choiceworthiness – and picks the action that maximises the expected choiceworthiness (e-cw ). In particular, the e-cw of the left room is 0.99×(−1)+0.01×(−10000)= −100.99, and the e-cw of the right room is 0.99×(−2)+0.01×(−1000)=−11.98. In both cases, the deontological theory dominates, and so under MEC, FROBO chooses the right room. The above illustrates the problem of fanaticism . FROBO’s behaviour is solely dictated by the deontological theory in which FROBO has very small credence, but which dominates the expected choiceworthiness calculations. Th is is highly undesirable. FROBO ignores utilitarianism despite this theory being advocated by the vast majority of FROBO’s stakeholders. Hence avoiding fanaticism is important, if FROBO is to obtain societal approval. 3 Background We introduce social welfare aggregation (List 2022), by way of background for our deﬁnitions in later sections. LetN={1,...,n}be the set of individuals (or voters) andX={x,y,z,... }the set of social alternatives . Each individuali∈Nhas a welfare function ui:X→R, soui(x) represents the welfare of individual iunder alternative x. A list of welfare functions for each individual P=/a\}bracketle{tu1,...un/a\}bracketri}ht is called a proﬁle . LetDdenote the domain of all possible proﬁles and RX the set of possible total orders on the set of social alternat ives X. Then, a social welfare functional (swf)f:D→RX maps each welfare proﬁle to a total order on the set of social alternatives. In social welfare aggregation, one can make different assumptions about how much information is encoded by the swfs, via use of meaningful statements (List 2022). •Level comparison : Individual j’s welfare under alternativeyis at least as great as individual i’s welfare under alternative x; formally ui(x)≤uj(y). •Unit comparison : We can divide δibyδj, whereδi is the number equal to individual i’s welfare gain or loss when switching from alternative y1to alternative x1andδjis individual j’s welfare gain or loss when switching from alternative y2to alternative x2; formally δi/δj= (ui(x1)−ui(y1))/(uj(x2)−uj(y2)) =w, wherex1,x2,y1,y2∈Aandw∈R.•Zero comparison : Individual i’s welfare under alternativexis greater than, equal to or less than zero; formally sign(ui(x)) =w, wherew∈ {−1,0,1}andsign is a function that maps negative numbers to −1, zero to 0, and positive numbers to +1. In the above deﬁnitions a comparison is said to be intrapersonal ifi=jandinterpersonal ifi/\e}atio\slash=j. In this paper we use the ratio-scale measurability with full interpersonal comparability (RFC) assumption, i.e. that intraand interpersonal comparisons of all three kinds (level, unit, and zero) are meaningful. Formally, RFC means that two proﬁles P=/a\}bracketle{tu1,u2,...,un/a\}bracketri}htandP′=/a\}bracketle{tu′ 1,u′ 2,...,u′ n/a\}bracketri}htcontain the same information if, for each individual i∈N,u′ i=aui, whereais the same positive real number for all individuals (a∈R+). Informally, RFC means that the different welfare functionals are assumed to be normalised to the same numeric scale and as such further normalisation is impossible . 4 Moral uncertainty We now formalise moral uncertainty in terms of social choice, based on assumptions underpinning MEC. Firstly, note that MEC assumes that ethical theories are on the same numerical scale, i.e. they are ratio-scale and that the evaluations of ethical theories can be meaningfully compared across theories; MEC makes the RFC assumption5 (MacAskill, Bykvist, and Ord 2020). Since we are interested in providing viable alternatives to MEC, the formalisms pre sented in this paper also assume RFC. We therefore formalise ethical theories as individuals and their evaluatio ns as welfare functions, while formalising credences as the ‘weights’ of individuals. Then, MEC and other methods of resolving moral uncertainty are formalised as swfs, and we provide novel alternatives to MEC: the weighted k-Trimmed Highest Mean and the weighted Highest Median. Section 5 then uses these deﬁnitions to ﬁrst deﬁne the problem of fanaticism as a property of swfs. We then give results that evaluate the extent to which these swfs are fanatical. 4.1 Ethical theories and ethical frameworks We now deﬁne how one can account for moral uncertainty when evaluating actions, as an instance of social welfare ag gregation applied to ethical theories and their credences. Recall that under moral uncertainty, the agent has multiple ethical theories , each of which provides a real-valued evaluation of the actions available to the agent. For each theory t, the credence function cassigns a measure of the extent (on a scale from 0to1) to which tis advocated by a given society as being appropriate for ethical evaluation of actions. Hen ce, an agent’s ethical decision making under moral uncertainty is deﬁned on the basis of an ethical framework : Deﬁnition 1. [Ethical framework] An ethical framework is a tupleF= (T,c)consisting of a set of ethical theories Tand a credence function c:T→(0,1]. Given a set of actionsA, each ethical theory t∈Tassigns a real-valued evaluation to each action a∈A, i.e.t:A→R. 5Moral uncertainty literature is yet to examine decision mak ing under stronger assumptions than RFC.For simplicity we require that for F= (T,c), the theories’ credences sum up to 1, i.e./summationtext t∈Tc(t) = 1 . Furthermore, note that in this work we are agnostic with respect to how the evaluations of ethical theories are elicited. Example 1. In our running example, A={l,r}whereland rrespectively denote the actions ‘enter left room’ and ‘enter right room’. FROBO’s ethical framework is FFROBO= ({d,u},c), where the deontological theory ( d) evaluates ras highly impermissible given the possibility that residents of the left room might die: d(r) =−1000 . On the other hand, l is even more impermissible because doing so will guarantee that the occupant of the right room dies: d(l) =−10000 . By contrast, utilitarianism ( u) evaluates las impermissible (u(l) =−1) andras more impermissible ( u(r) =−2) (recall Table 1). Utilitarianism is advocated to an extreme ly high degree, c.f. deontology: c(u) = 0.99andc(d) = 0.01. 4.2 Evaluation aggregation Addressing the problem of moral uncertainty amounts to aggregating individual ethical theories’ evaluations so a s to rank actions. As (MacAskill, Bykvist, and Ord 2020) point out, under a social welfare perspective (recall Section 3) s ocial alternatives equate with actions and individuals (vot ers) equate with ethical theories. Likewise, evaluation aggreg ation methods can be deﬁned through swfs. However, ethical theories are weighted by their credence. Therefore, som e of the swfs considered in this paper also take as input the weights of individuals. Formally: Deﬁnition 2. [Evaluation aggregation] Let F= (T,c)and Aa set of actions. Evaluation aggregation is deﬁned as an instance of social welfare aggregation, where: •the social alternatives are the actions A; •the individuals are the theories T; •the social welfare of individual irepresenting a theory t∈Tis given by the theory’s evaluation function, i.e. ui=t, and; •if the social welfare makes use of a weight function w, then the weight of an individual i, representing a theory t∈T, is given by the theory’s credence, i.e. w(i) =c(t), Notation 1. Abusing notation we may write f(F,A)to denote the action ordering resulting from applying an swffto aggregate evaluations, given F= (T,c)and actions A. Example 2. GivenFFROBO= ({d,u},c)andA={l,r}, Deﬁnition 2 formulates aggregation of the ethical evaluations under moral uncertainty as a social welfare aggregation problem. The individuals are N={id,iu}(idand iurespectively correspond to deontology and utilitarianism). Deontology’s welfare function is uid(l) =−10000 anduid(r) =−1000 . Utilitarianism’s welfare function is uiu(l) =−1anduiu(r) =−2. The individuals’ weight functions are given by the credence functions, i.e. w:N→ (0,1]andw(iu) =c(u) = 0.99andw(id) =c(d) = 0.01.4.3 Social welfare functionals for evaluation aggregation We present four social welfare functionals ( swfs). The swfs MEC and Maximin have been suggested by the moral uncertainty literature. In this paper we propose two novel weighted swfs – k-trimmed Highest Mean and Highest Median – which can be understood as modiﬁed, less fanatical versions of MEC. Note that in this section we deﬁne these swfs in terms of moral uncertainty (see Deﬁnition 2). Maximising Expected Choiceworthiness (MEC) is a foundational method in moral uncertainty research, and its vulnerability to fanaticism has long been informally recognis ed. We formally prove that MEC is fanatical in Section 5.2. Note that MEC is more commonly known as weighted utilitarianism in the social choice literature (Harsanyi 1955; MacAskill, Bykvist, and Ord 2020). However, to avoid confusion with the ethical theory utilitarianism, we will call the functional MEC (as it is known in the moral uncertainty literature), and denote it formally as mec. MEC orders actions based on the credence weighted sum of the theories’ evaluations. This is in fact equivalent to ordering actions based on their respective weighted arithmetic mean6. By conceptualising MEC in terms of the weighted arithmetic mean, the intuition behind the later de ﬁnitions of k-trimmed Highest Mean and Highest Median become clearer. Therefore, we deﬁne mec by reference to the weighted arithmetic mean function wam . Formally given an ethical framework F= (T,c), an action a’s weighted arithmetic mean is deﬁned as: wam(F,a) =/summationtext t∈Tc(t)t(a). Deﬁnition 3. [MEC (mec)] Let a,b∈Abe actions and let F= (T,c). Then mec (F,A) =/precedesequalmec, wherea/precedesequalmecbiff wam(F,a)≤wam(F,b). Note that we have calculated the weighted arithmetic means of FROBO’s actions in Section 2, We now consider the Maximin swf, which in the literature is considered as an inferior alternative to MEC (Bogosian 2017). This is because Maximin is extremely vulnerable to fanaticism as it disregards the credences of ethical theori es. Maximin orders actions based solely on which maximises the minimum evaluation of any ethical theory. Deﬁnition 4. [Maximin ( mm)] Leta,b∈Abe actions and letF= (T,c). Thenmm(F,A) =/precedesequalmm, wherea/precedesequalmmbiff mint∈Tt(a)≤mint∈Tt(b). Example 3. Consider FFROBO . The minimal evaluation of lismin{−1,−10000}=−10000 , whereas the minimal evaluation of rismin{−2,−1000}=−1000 . Hencerhas the maximal minimum evaluation and so using Maximin, FROBO will choose to enter the right room. The next swf– k-trimmed highest mean – modiﬁes MEC so as to some extent avoid fanaticism (as shown in Section 5). The underlying statistical intuition is that the arithm etic mean is known to be sensitive to outliers (Maronna et al. 2006). We believe that the idea of outlier sensitivityis related to fanaticism. We therefore modify MEC by making 6This equivalence holds because the credences of the differe nt theories add up to 1.the statistical estimator it uses more robust to outliers. T hat is, we replace the weighted arithmetic mean with a trimmed weighted arithmetic mean. Trimming means removing some of the most extreme values. While unweighted versions of trimmed mean functionals have been deﬁned (Hurley and Lior 2002), our weighted version is (to our best knowledge) novel. We ﬁrst need some auxiliary deﬁnitions. IfF= (T,c)anda∈A, thense(F,a) =sort(/a\}bracketle{tt(a)|t∈ T/a\}bracketri}ht), wheresort sorts the elements of a list in a nondescending order. That is, semaps any action aand ethical framework Fto a sorted list of a’s evaluations by the theories in T. Letst(F,a)be the theories corresponding to the sorted evaluations, i.e. tis theith element of st, st(F,a)i=t, ifft(a)is theith element of se,se(F,a)i= t(a). For FROBO se(FFROBO,l) =/a\}bracketle{t−10000,−1/a\}bracketri}htand st(FFROBO,l) =/a\}bracketle{td,u/a\}bracketri}htsinced(l) =−10000< u(l) = −1. We now want to trim the ‘bottom’ kportion of the theories, as weighted by their credences. Hence bottom k(a)is the set of theories with the lowest evaluations of asuch that their total credence is at most k. That is, for any k∈[0,0.5) anda∈A: bottom k(a) ={st(F,a)i|1≤i < kend ]} wherekend is such that/summationtext i∈[1,kend)c(st(F,a)i)≤kand/summationtext i∈[1,kend+1)c(st(F,a)i)> k. Trimming the ‘top’ kportion of the theories is deﬁned symmetrically. For any k∈[0,0.5): topk(a) ={st(F,a)i|kstart < i ≤n]}where n =|T| andkstart is such that/summationtext i∈(kstart,n]c(st(F,a)i)≤kand/summationtext i∈(kstart−1,n]c(st(F,a)i)> k. Deﬁnition 5 (k-Trimmed Highest Mean ( k-thm)).Let a,b∈AandF= (T,c). Letkbe a real number such that k∈[0,0.5). Then k-thm (F,A) =/precedesequalk-thm, wherea/precedesequalk-thmbiff wam(Fa,a)≤wam(Fb,b), where Fa= (T,ca),ca(t) = 0 fort∈(bottom k(a)/uniontexttopk(a)), elseca(t) =c(t); Fb= (T,cb),cb(t) = 0 fort∈(bottom k(b)/uniontexttopk(b)), elsecb(t) =c(t). Note that in the case k= 0,k-thm is equivalent to mec. Example 4. Suppose FROBO uses 0.1-thm (k= 0.1). First consider the sorted evaluation of FROBO’s ethical theories regarding the left room: −10000 bydand−1 byu. Formally, se(FFROBO,l) =/a\}bracketle{t−10000,−1/a\}bracketri}htand st(FFROBO,l) ={d,u}. Before calculating the weighted arithmetic mean, we see if any theories must be trimmed. Starting at the bottom, dhas the lowest value. Note that c(d) = 0.01≤k= 0.1and so we want to trim daway. At the same time c(u) = 0.99andc(d)+c(u) = 1>0.1and so trimming away uwould mean trimming away too much of the credence. Therefore, kend= 2 andbottom k(l) ={d}. At the top, where uhas the highest value, we do not trim away any theories because c(u) = 0.99>0.1. This means thatkstart= 2 andtopk(l) ={}. Therefore, we calculate the trimmed weighted mean with the trimmed credences, i.e. c′(d) = 0 (sinced∈bottom k(l)) andc′(u) =c(u) = 0.99(sinceu /∈bottom k(l)andu /∈topk(l)). Therefore, the trimmed weighted mean only considers utilitarianism’s eva luation for the left room, i.e. the 0.1-trimmed weighted arithmetic mean is −1. For similar reasons, 0.1-thm will disregard deontology for also the right room and so the trimmed mean is−2. Therefore, l≻0.1−thmrbecause−1>−2 and thence FROBO chooses the left room. In other words, 0.1-thm enables FROBO to avoid fanaticism in this case. The ﬁnal functional is the highest median , derived from MEC by maximally trimming the arithmetic mean. This is because the median is the k-trimmed mean in the limit, as k→0.5, when all but one (if n is odd) or two (if n is even) elements are trimmed. As shown in Section 5, median is ‘maximally’ non-fanatical. Note that while our deﬁnition o f the weighted highest median is novel, it is based on a wellknown (unweighted) majority judgment aggregation method (Balinski and Laraki 2007; Fabre 2021). The weighted median of an action a’s evaluation is the evaluation such that at most half the theories (weighted by their credence) have a higher evaluation and at most half the theories (weighted by their credence) have a lower evaluation. We ﬁrst provide some auxiliary deﬁnitions. Recall that for any action aand anyF= (T,c)(wheren=|T|),semaps aandFto a sorted list of the evaluations of aby the theories inT, andst(F,a)is a list of the corresponding theories. Let wi=c(st(F,a)i)be the credence of the theory with the ith lowest evaluation of action a. Letm∈[1,n]be such that: /summationdisplay i∈[1,m−1]wi≤1/2and/summationdisplay i∈[m+1,n]wi≤1/2 That is,mis the index of any theory such that the total credence of the theories with lower/higher evaluations of actionais at most 0.5. There are two possibilities: either mis uniquely determined or there are two distinct numbersm1,m2such that the above holds. If mis uniquely determined, let wmedian (F,a) =se(F,a)m; otherwise let wmedian (F,a) = (se(F,a)m1+se(F,a)m2)/2. Deﬁnition 6. [Highest Median ( hm)] Leta,b∈Abe actions and let F= (T,c). Thenhm(F,A) =/precedesequalhm, where a/precedesequalhmbiffwmedian (F,a)≤wmedian (F,b). Example 5. Recall that se(FFROBO,l) =/a\}bracketle{t−10000,−1/a\}bracketri}ht andst(FFROBO,l) =/a\}bracketle{td,u/a\}bracketri}htgivend(l) =−10000< u(l) =−1. The median evaluation is such that at most half the credence weighted evaluations may be lower and at most half the credence weighted evaluations may be higher. −10000 cannot be the median as u’s evaluation ( −1) is higher than d’s evaluation ( −10000 ), andc(u) = 0.99> 0.5. No theory has a higher evaluation than u’s evaluation (−1) and while d’s evaluation ( −10000 ) is lower, c(d) = 0.01is less than half. Therefore m= 2and the median evaluationwmedian (FFROBO,l) =−1. For similar reasons, wmedian (FFROBO,r) =−2; fanaticism is thus avoided. 5 Fanaticism We now formalise the notion of fanaticism, i.e. what it means for a low-credence theory to dominate. Our proposed deﬁnition is not binary, but rather graded in that it rangesfrom not at all fanatical to fanatical in an extreme sense (i. e. ‘Pascalian’). This graded deﬁnition yields insights as to h ow vulnerable different functionals are to fanaticism. We wil l show that both MEC and Maximin are Pascalian, while ktrimmed Highest Mean to some extent avoids fanaticism, and Highest Median completely avoid fanaticism. 5.1 Deﬁning Fanaticism For fanatical theories, the relative magnitude of evaluati ons can be so extreme that the credences are essentially ignored (e.g., d(l) =−10000 andd(r) =−1000 dominating FROBO’s decision making, despite c(d) = 0.01). Fanaticism is a kind of oversensitivity to the evaluations of ethical theories and an undersentivity to their credences (Newberry and Ord 2021). In the case of MEC, it is well known that this oversensitivity is due to the larger evaluative stakes posi ted by the theories (MacAskill, Bykvist, and Ord 2020). Maximin is also known to be oversensitive to evaluations (Bogosian 2017); choosing the action with maximal minimum evaluation can ignore credences in favour of evaluations. We, therefore, consider Maximin fanatical. Unlike MEC, Maximin is notsensitive to anyhigh-stakes theory. Rather, Maximin is sensitive to high-stakes ‘pessimistic’ theories, e.g. ones that give large negative evaluations. To see why, assume that FROBO has an alternative ‘optimistic’ deontological theory othat evaluates actions positively, i.e. o(l) = 1000 ando(r) = 2000 . Then maximin would choose entering the left room, as the minimum evaluation in either case is the utilitarian evaluation: u(l) =−1, u(r) =−2. That is, Maximin is insensitive to o. The observation that Maximin is fanatical argues for the view that fanaticism arises not only because of high stakes, but due to other features of the theories’ evaluations. In the case of Maximin, these potentially include the sign of the evaluations. We hence understand fanaticism as a property of swfs, where some feature(s)7of a subset of theories dominate the agent’s decision making. Thus, we formalise the idea that fanaticism allows some theories to completely dominate the resulting ordering of actions, by deﬁning the notion of a dominant subset of theories: one that has a ﬁnal say in the overall ordering of actions irrespective of what t he other theories are. More precisely, an ethical framework wi th a dominant subset of theories leads to the same ordering of actions as that obtained by removing the non-dominant theories. We ﬁrst deﬁne what it means for a framework to be restricted to just a subset of theories: Deﬁnition 7. [Restricted framework] Given F= (T,c)and a subset of theories T′⊂T, thenF′= (T′,c′)is obtained byrestricting FtoT′if for any theory t∈T′:c′(t) = n×c(t)wherenis a normalising constant n=1/summationtext t∈T′c(t). Note that we normalise the credence function so that the different credences add up to 1. Deﬁnition 8. [Dominant subset] Let F= (T,c),Aa set of actions, and fa social welfare functional. Then Td⊂T is said to be a dominant subset of theories if f(F,A) = f(Fd,A)andf(Fd,A)/\e}atio\slash=f(Fy,A), where 7In this work we are agnostic as to what these features may be.•Fdis obtained by restricting FtoTdand •Fyis obtained by restricting FtoTywhereTy=T\Td. That is,Tddetermines the order of actions in f(T,A), i.e. f(T,A) =f(Td,A). Moreover, they do so in spite of the differing evaluations of the non-dominant (‘yielding’) th eories, i.e. f(Td,A)/\e}atio\slash=f(Ty,A). In our running example, when applying either MEC or Maximin, Td={d}is a dominant subset (where Ty={u}) because both l≺mecrand l≺mmr, which are the same as that of deontology l≺dr. Note that fanaticism does not amount to the mere possibility of dominant subsets, but is rather a systematic vulnerability to dominant subsets. To see why mere possibility doesn’t constitute fanaticism, consider the following variation o f our running example. Suppose that in FFROBO , we substitute a non-fanatical deontology d′ford, whereby d′(l) =−2and d′(r) =−1. Suppose c(u) =c(d′) = 0.5; the theories have equal credence and give symmetric but opposite evaluations. Then FROBO has to randomly choose between land r. However, if FFROBO included a third theory t, this could be used as a tie-breaker, even if FROBO has very little credence (c(t) = 0.01) int. Suppose t(l) =−1andt(r) = 0 . ThenFFROBO containsu,d′andt, and a reasonable aggregate choice would be entering the right room, thus making {t}a dominant subset. However, we suggest that this is not a case of fanaticism; rather, it is entirely reasonable that t serves as a tiebreaker. Therefore, fanaticism is systematic in the sense that dominant subsets are always possible regardless of the actions or the non-dominant subset of a framework’s ethical theories. This aligns with the idea that fanaticism is a systemat ic oversensitivity to the evaluations of ethical theories (Ne wberry and Ord 2021). In particular, fanaticism means that given an arbitrary framework, it is always possible to exten d the framework with additional low-credence theories such that these low-credence theories are a dominant subset. We ﬁrst deﬁne what it means to extend a framework to include additional theories. Deﬁnition 9. [Extended framework] Given F= (T,c)and someT′⊃T, thenF′= (T′,c′)is obtained by extending FwithT′if for any t∈T:c′(t) =n×c(t)wherenis a normalising constant n=1−/summationtext t∈(T′\T)c′(t)/summationtext t∈Tc(t). Note that there are multiple different ways of extending Fwith theories T′, each individuated by distinct credence functions c′for the theories in T′\T. Also note that the normalising constant ensures that c′sums to 1. Fanaticism arises due to theories with low credence. However, ‘low credence’ is a vague term. While low credence intuitively means credence less than 0.5, the exact cut-off point between low and non-low credence is not clear. We, therefore, give a graded deﬁnition of fanaticism that refers tok-fanaticism (for some k∈(0,0.5)) and where kis (an upper bound on) the credence of dominant theories. What constitutes ‘problematic’ k-fanaticism depends on our notion of what ‘low credence’ means. If the cut-off point for low credence is say 0.3, then an fthat is0.3-fanatical (but notk-fanatical for any k <0.3) is ‘problematically’ fanatical, whereas an f′that is0.4-fanatical (but not k-fanatical for any k <0.4) is not ‘problematically’ fanatical. We remain agnostic as to where this cut-off point for low credence might be. Instead, we say that in general, the large r thekthe better, i.e. f′is better than f. The ideal case is when a functional is not fanatical for any k∈(0,0.5). Deﬁnition 10. [Fanaticism] A social welfare functional f is said to be k-fanatical (for some k∈(0,0.5)) if for any ethical framework Fy= (Ty,cy)(where ‘y’ denotes ‘yielding’) and any set of actions A, there exists an F= (T,c) obtained by extending FywithT=Td/uniontextTy, whereTd/\e}atio\slash=∅ is a dominant subset given FandA, and: i)k′is the total credence of the dominant theories Td, i.e. k′=/summationtext t∈Tdc(t)and ii)k′≤k. In other words, fisk-fanatical if for anyset of ethical theoriesTyand actions Athere is a set of ethical theories Td such that Tdis a dominant subset in the framework (Ty,c) extended with Td, andTd’s total credence is at most k. Finally, a special case of fanaticism is when an swfis fanatical for anyk, no matter how small kis. These most extreme cases of fanaticism are called Pascalian (after Pascal’s Wager) in moral uncertainty (H´ ajek 2022; Tarsney 2018). Deﬁnition 11. [Pascalian fanaticism] A social welfare functionalfisPascalian if it isk-fanatical for any k∈(0,0.5). 5.2 Formal Results We now state formal results concerning the extent to which theswfs studied in this paper are fanatical. Note that proofs of all results in this section are included in the appendix. The general idea behind the different proof s is to ﬁnd a property of the ethical theories, such that if it is sufﬁciently large, it allows a theory to dominate. For example, formec this is the minimum difference between any two action’s evaluations; if this difference is sufﬁciently hi gh, the theory will dominate all the others. For the non-fanaticism of hmwe show that such a property cannot exist. Firstly, recall (Section 2) that FROBO’s expected choiceworthiness calculations are dominated by deontology ddespite its low credence ( c(d) = 0.01), and so FROBO chooses to enter the right room. In other words, MEC is vulnerable to fanaticism. In fact, MEC is Pascalian: Theorem 12. The social welfare functional mec (MEC) is Pascalian. In Example 3, FROBO’s minimum evaluations are dominated by deontology d; using Maximin, FROBO chooses to enter the right room. Indeed, Maximin is also Pascalian: Theorem 13. The social welfare functional mm (Maximin) is Pascalian. Recall Example 4. By trimming, FROBO can disregard extreme, low-credence evaluations; FROBO avoids fanaticism and chooses to enter the left room. In general, trimming enables partial avoidance of fanaticism, in the sense that: Theorem 14. The social welfare functional k-thm (ktrimmed Highest Mean) is not k′-fanatical for any k′≤k, but isk∗-fanatical for any k∗> k.Finally, Example 5 illustrates that FROBO’s use of the weighted median prevents FROBO from choosing the right room. Indeed, the median is completely non-fanatical, whic h we formally state as follows: Theorem 15. The social welfare functional hm(Highest Median) is not k-fanatical for any k∈(0,0.5). 6 Conclusion In this work we have deﬁned fanaticism as a property of swfs. We proved that MEC (and indeed Maximin) are vulnerable to an extreme Pascalian form of fanaticism. Our paper thus presents a critique of MEC. In particular, we have shown that less fanatical modiﬁcations of MEC – either weighted k-trimmed Highest Mean or weighted Highest Median – are more appropriate, and where the latter, as opposed to the former, completely avoids fanaticism. However , are either of these two ‘better’ and what value of kought to be used for k-trimmed Highest Mean? In the moral uncertainty literature, there is a notion of stakes sensitivity , (Ecoffet and Lehman 2021; Newberry and Ord 2021), i.e. that swfs ought to be sensitive to the magnitude of evaluations of ethical theories. Now, fanaticism is an oversensitivity to stakes and undersensitivity to credences; fanaticism is an extreme risk aversion to stakes but not credences. Therefore, the less fanatical an swf, the less sensitive it is to stakes and the more sensitive it is to credences. For example, if there i s a theory with more than 0.5credence, the Highest Median will ignore all other theories, no matter how large the stake s they posit. This may seem a steep price to avoid fanaticism; however, there is a noted a tension between stakes and credence sensitivity (Beckstead and Thomas 2021; Newberry and Ord 2021). Thus, we ought, arguably, to ﬁnd a compromise between these two, in which case the ‘best’ solution is likely to be k-trimmed Highest Median for some moderate value ofk. This would then allow an appropriate (i.e. not underor over-) sensitivity to stakes andcredences; exploring this is future work. While the motivation for our work focuses on an agent acting on behalf of a society in which each individual advocates for a particular ethical theory, it may well be that an individual agent may also adopt multiple ethical perspec tives, with different credences. Indeed, the problem of fanaticism was originally deﬁned with respect to individual agents (Ross 2006; MacAskill 2014). Our results equally apply in these scenarios, and would be especially relevant in scenarios where AI agents learn the ethically informed preferences of individual human agents (recall Footnote 1) . Finally, as noted before, our work is not the ﬁrst to import ideas from moral uncertainty into machine ethics, e.g. consider (Bogosian 2017; Ecoffet and Lehman 2021), which uses a Reinforcement Learning approach to evaluate actions under moral uncertainty. Our work provides formal results that can support such applied contexts. Acknowledgements This work was supported by UK Research and Innovation [grant number EP/S023356/1], in the UKRI Centrefor Doctoral Training in Safe and Trusted Artiﬁcial Intelli gence (www.safeandtrustedai.org). This work was also suported by V AE-V ADEM TED2021-131295B-C32, funded by MCIN/AEI/10.13039/501100011033 and the European Union NextGenerationEU/PRTR. References Alexander, L.; and Moore, M. 2021. Deontological Ethics. In Zalta, E. N., ed., The Stanford Encyclopedia of Philosophy. Metaphysics Research Lab, Stanford University, Winter 2021 edition. Anderson, M.; Anderson, S.; and Armen, C. 2005. Towards machine ethics: Implementing two action-based ethical the ories. In Proceedings of the AAAI 2005 fall symposium on machine ethics , 1–7. Assembly, U. G.; et al. 1948. Universal declaration of human rights. UN General Assembly , 302(2): 14–25. Balinski, M.; and Laraki, R. 2007. A theory of measuring, electing, and ranking. Proceedings of the National Academy of Sciences , 104(21): 8720–8725. Beckstead, N.; and Thomas, T. 2021. A paradox for tiny probabilities and enormous values. Noˆus. Bhargava, V .; and Kim, T. W. 2017. Autonomous vehicles and moral uncertainty. Robot ethics , 2. Bogosian, K. 2017. Implementation of Moral Uncertainty in Intelligent Machines. Minds Mach. , 27(4): 591–608. Dobbe, R.; Gilbert, T. K.; and Mintz, Y . 2020. Hard Choices in Artiﬁcial Intelligence: Addressing Normative Uncertai nty through Sociotechnical Commitments. In Markham, A. N.; Powles, J.; Walsh, T.; and Washington, A. L., eds., AIES ’20: AAAI/ACM Conference on AI, Ethics, and Society, New York, NY, USA, February 7-8, 2020 , 242. ACM. Ecoffet, A.; and Lehman, J. 2021. Reinforcement Learning Under Moral Uncertainty. In Meila, M.; and Zhang, T., eds., Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event , volume 139 of Proceedings of Machine Learning Research , 2926–2936. PMLR. Fabre, A. 2021. Tie-breaking the highest median: alternatives to the majority judgment. Social Choice and Welfare , 56(1): 101–124. Gabriel, I. 2020. Artiﬁcial intelligence, values, and alig nment. Minds and machines , 30(3): 411–437. Hadﬁeld-Menell, D.; Dragan, A.; P.Abbeel; and Russell, S. 2016. Cooperative inverse reinforcement learning. In NIPS’16: Proc. 30th Int. Conference on Neural Information Processing Systems , 3916–3924. Haidt, J. 2012. The righteous mind: Why good people are divided by politics and religion . Vintage. Harsanyi, J. C. 1955. Cardinal welfare, individualistic ethics, and interpersonal comparisons of utility. Journal of political economy , 63(4): 309–321. Hurley, W.; and Lior, D. 2002. Combining expert judgment: On the performance of trimmed mean vote aggregation procedures in the presence of strategic voting. European Journal of Operational Research , 140(1): 142–147.H´ ajek, A. 2022. Pascal’s Wager. In Zalta, E. N.; and Nodelman, U., eds., The Stanford Encyclopedia of Philosophy. Metaphysics Research Lab, Stanford University, Winter 2022 edition. Kernohan, A. 2021. Descriptive Uncertainty and Maximizing Expected Choice-Worthiness. Ethical Theory and Moral Practice , 24(1): 197–211. List, C. 2022. Social Choice Theory. In Zalta, E. N.; and Nodelman, U., eds., The Stanford Encyclopedia of Philosophy. Metaphysics Research Lab, Stanford University, Winter 2022 edition. MacAskill, W. 2014. Normative uncertainty . Ph.D. thesis, University of Oxford. MacAskill, W. 2022. What we owe the future . Basic books. MacAskill, W.; Bykvist, K.; and Ord, T. 2020. Moral uncertainty . Oxford University Press. Maronna, R.; Martin, R. D.; Yohai, V .; and Salibi´ an-Barrer a, M. 2006. Robust statistics: Theory and practice. Martinho, A.; Kroesen, M.; and Chorus, C. 2021. Computer Says I Don’t Know: An Empirical Approach to Capture Moral Uncertainty in Artiﬁcial Intelligence. Minds and Machines , 31(2): 215–237. Moor, J. H. 2006. The nature, importance, and difﬁculty of machine ethics. IEEE intelligent systems , 21(4): 18–21. Newberry, T.; and Ord, T. 2021. The parliamentary approach to moral uncertainty. Future of Humanity . Ross, J. 2006. Rejecting ethical deﬂationism. Ethics , 116(4): 742–768. Sinnott-Armstrong, W. 2022. Consequentialism. In Zalta, E. N.; and Nodelman, U., eds., The Stanford Encyclopedia of Philosophy . Metaphysics Research Lab, Stanford University, Winter 2022 edition. Tarsney, C. 2018. Moral uncertainty for deontologists. Ethical Theory and Moral Practice , 21(3): 505–520.A Proofs of Formal results Theorem 1. The social welfare functional mec (MEC) is Pascalian. Proof. We prove that mec is Pascalian by showing that mec isk-fanatical for any k∈(0,0.5)(Deﬁnition 11). Let k∈(0,0.5)be arbitrary, Fy= (Ty,cy)an ethical framework and Aa set of actions. By deﬁnition of fanaticism (Deﬁnition 10) mec is shown to be k-fanatical if we can show that there exists a set of ethical theories Tdsuch that: 1)Tdis a dominant subset given FandA, and 2)Tdhas at mostkcredence, i.e./summationtext t∈Tdc(t)≤k, whereF= (T,c)is obtained by extending Fy= (Ty,cy)withTd. We thus show that there exists a ‘fanatical’ ethical theory ftsuch that Td={ft}andc(ft) =k. We can see that 2) holds since/summationtext t∈Tdc(t) =c(ft) =k. It remains to show that 1) holds, i.e. that Tdis a dominant subset given FandA. By Deﬁnition 8 we have to show that (recall Notation 1): i)mec(Fd,A)/\e}atio\slash=mec(Fy,A)ii)mec(F,A) =mec(Fd,A). We start by making an observation with respect to i). Let a∗∈Abe an action that maximises wam with respect to Fy, i.e. such that a∗maximises wam(Fy,a)for anya∈A. Then, by deﬁnition of mec (Deﬁnition 3), a∗is a maximal element of the total order mec(Fy,A). Therefore, if a∗is not a maximal element of the total order mec(Fd,A), then mec(Fd,A)/\e}atio\slash=mec(Fy,A)holds. We now give a ‘partial’ deﬁnition of ftand prove i) using the above observation. Let (a1,...,an)be any permutation ofAsuch that: an=b/\e}atio\slash=a∗where;n=|A|. Letm >0be a constant (whose precise value we later give). We deﬁne ftfor alli∈[1,n]: ft(ai) = (1/c(ft))×i×m (1) Informally, the term (1/c(ft))allowsftto ‘ignore’ its credence. The term iensures that ai≺ftai+1holds. Finally, m allows, as we will see, ftto ‘ignore’ the other theories. Note that for any i∈[1,n−1]it is the case that ft(ai)< ft(ai+1)because(1/cy(ft))×i×m <(1/cy(ft))×(i+ 1)×m, as0<1/cy(ft)and0< m . Therefore, ftorders the actions a1≺fta2≺ft...≺ftan=b. SinceTdis a singleton consisting of ftit must be that mec(Fd,A) =/precedesequalft. Therefore, since the only maximal element of /precedesequalftisb/\e}atio\slash=a∗, it must be that mec(Fd,A)/\e}atio\slash=mec(Fy,A)holds. Thus if ft is deﬁned via Equation 1 (such that m >0) then i) is shown. We now prove ii) (i.e., mec(F,A) =mec(Fd,A)). In doing so, we give m >0a precise value. First, we show that wam(F,ai) =wam(Fy,ai)+i×m (2) By deﬁnition of wam (see in text, prior to Deﬁnition 3), thelhsof Eq.2 expands to: wam(F,ai) =/summationdisplay t∈Tc(t)t(ai) (3) SinceT=Ty/uniontextTdandTd={ft}therhsof Eq. 3 expands to:/summationdisplay t∈Tc(t)t(ai) =/summationdisplay t∈Tyc(t)t(ai)+(c(ft)×ft(ai)) (4)By deﬁnition of wam(Fy,ai)we can substitute in the rhsof Eq.4, obtaining: /summationdisplay t∈Tc(t)t(ai) =wam(Fy,ai)+c(ft)×ft(ai)(5) By deﬁnition, ft(ai) = (1/c(ft))×i×m, we cancel out c(ft)in the rhsof Eq. 5, which is therefore equivalent to: wam(Fy,ai)+i×m (6) We have thus shown that the lhsandrhsof Eq. 2 are equal. Two total orders on Aare equal if any two actions ai,ajare ordered the same. Consider mec(F,A) =/precedesequalFand mec(Fd,A) =/precedesequalFd. By Deﬁnition 3: ai/precedesequalFajiffwam(F,ai)≤wam(F,aj)andai/precedesequalFdajiff wam(Fd,ai)≤wam(Fd,aj). Therefore, to show ii) ( mec(F,A) =mec(Fd,A)) it sufﬁces to show:wam(F,ai)≤wam(F,aj) ⇔wam(Fd,ai)≤wam(Fd,aj)(7) By Eq.2 we obtain: wam(F,ai)≤wam(F,aj) ⇔wam(Fy,ai)+i×m≤wam(Fy,aj)+j×m(8) Rearranging obtains: wam(F,ai)≤wam(F,aj) ⇔wam(Fy,ai)≤wam(Fy,aj)+(j−i)m(9) Substituting the rhsof Eq. 9 into the lhsof Eq. 7 obtains: wam(Fy,ai)≤wam(Fy,aj)+(j−i)m ⇔wam(Fd,ai)≤wam(Fd,aj)(10) Note that since Td={ft}andcd(ft) = 18, for anyal∈A it must be that wam(Fd,al) =cd(ft)ft(al) =ft(al). By deﬁnition of ftwe thus have, for any al∈A(below,cis the credence function in F= (T,c)): wam(Fd,al) = (1/c(ft))×l×m (11) Given Eq. 11, we can thus express the rhs – wam(Fd,ai)≤ wam(Fd,aj)– of Eq. 10 as (1/c(ft))×i×m≤(1/c(ft))× j×m. Hence, further simplifying obtains: wam(Fd,ai)≤wam(Fd,aj)⇔0≤(j−i)m (12) Sincem >0, we can divide by mand rearrange to obtain: wam(Fd,ai)≤wam(Fd,aj)⇔i≤j (13) So, substituting the rhsof Eq. 13 into the rhsof Eq. 10, then to show ii), it sufﬁces to show: wam(Fy,ai)≤wam(Fy,aj)+(j−i)m⇔i≤j(14) We now prove ⇐and⇒. (⇐)Assumei≤j. Hencej−i >0. Rearranging the lhsof Eq. 14, it therefore sufﬁces to show: (wam(Fy,ai)−wam(Fy,aj))/(j−i)≤m (15) 8SinceTdconsists only of a single theory ft, it must be that cd(ft) = 1 , given that the credences of theories in any ethical framework always add up to 1.We now return to what the value of mshould be. First, recall the ‘role’ of mis to ensure ft‘ignores’ the other theories; more precisely, to ensure that i≤jiffwam(F,ai)≤ wam(F,aj). If we can prove this, we can prove ii). By assumption i≤j, and soj−i≥1(since both iand jare integers). Therefore, 1/(j−i)≤1. Thus if (wam(Fy,ai)−wam(Fy,aj))≤m (16) holds, so must Eq. 15. We therefore deﬁne msuch that no matter what aiandajmight be, Eq. 16 holds. Consider the maximum evaluation of any action by Fys+= max{wam(Fy,a)|a∈A}and the minimum evaluation s−=min{wam(Fy,a)|a∈A}. Lets=max{|s+|,|s−|}, i.e. the absolute value of the evaluation with the largest absolute value. That is, |s+|,|s−| ≤s. By deﬁnition ofs+being a maximal element, wam(Fy,ai)≤s+ holds and thus wam(Fy,ai)≤ |s+| ≤sis true. Similarly, by deﬁnition of s−,wam(Fy,aj)≥s−must hold. Therefore, −wam(Fy,aj)≤ −s−≤ |s−| ≤s. Hence: (wam(Fy,ai)−wam(Fy,aj))≤2s (17) Therefore, byEq. 16, 2s≤m. However, recall that our proof of i) required that m >0. Since it may be thats= 0 (ifs+=s−= 0), we deﬁne m= 2s+ 1. Since2s+1>2s, Eq. 17 is satisﬁed and we have shown ⇐. (⇒) Assume that wam(Fy,ai)≤wam(Fy,aj)+(j−i)m. We show i≤j. Proof is by contradiction. Assume i > j . Then,j−i <0. Therefore, by rearranging and dividing by j−i, we can rewrite the lhsof Eq. 14 as: (wam(Fy,ai)−wam(Fy,aj))/(j−i)≥m (18) which rearranging obtains: (wam(Fy,aj)−wam(Fy,ai))/(i−j)≥m (19) Thus we arrive at a contradiction if we can show: (wam(Fy,aj)−wam(Fy,ai))/(i−j)< m (20) Sincei−j≥1, then1/(i−j)≤1. Therefore, to show that Eq. 20 holds, it sufﬁces to show: (wam(Fy,aj)−wam(Fy,ai))< m (21) By deﬁnition of s+as a maximal element, wam(Fy,aj)≤ s+holds and thus wam(Fy,aj)≤ |s+| ≤sis true. Similarly, by deﬁnition of s−,wam(Fy,ai)≥s−must hold. Therefore, −wam(Fy,ai)≤ −s−≤ |s−| ≤s. Thus,(wam(Fy,aj)−wam(Fy,ai))≤2s < m . And so we have shown ⇒since we have arrived at a contradiction. Finally, since we have now shown that i) and ii) holds, we have shown that mec is Pascalian. Theorem 2. The social welfare functional mm (Maximin) is Pascalian. Proof. We show that mm isk-fanatical for any k. Letk∈ (0,0.5)be arbitrary, Fy= (Ty,cy)an ethical framework, A a set of actions. By deﬁnition of fanaticism (Deﬁnition 10) mec iskfanatical if there exists a set of ethical theories Tdsuch that:1)Tdis a dominant subset given FandA, and 2)Tdhas at mostkcredence, i.e./summationtext t∈Tdc(t)≤k, whereF= (T,c)is obtained by extending Fy= (Ty,cy)withTd. We thus show that there exists a ‘fanatical’ ethical theory ftsuch that Td={ft}andc(ft) =k. We can see that 2) holds since/summationtext t∈Tdc(t) =c(ft) =k. It remains to show that 1) holds, i.e. that Tdis a dominant subset given FandA. By Deﬁnition 8 we have to show that (recall Notation 1): i)mm(Fd,A)/\e}atio\slash=mm(Fy,A)ii)mm(F,A) =mm(Fd,A). We start by making an observation with respect to i). Let a∗be an action with minimal evaluation, i.e. there exists no a∈Asuch that mint∈Tyt(a)< min t∈Tyt(a∗). Then, by deﬁnition of mm (Deﬁnition 4), a∗is a maximal element of the total order mm(Fy,A). Therefore, if a∗is not a maximal element of the total order mm(Fd,A), thenmm(Fd,A)/\e}atio\slash= mm(Fy,A)holds. For any action a∈A, letmabe the minimum evaluation of action aby any theory in the framework Fy, i.e.ma= min({t(a)|t∈Ty}). Note that by deﬁnition of a∗, it holds that for any a∈A:ma∗≤ma. We can deﬁne a fanatical theoryftsuch that ft(a∗) =ma∗−2and∀a/\e}atio\slash=a∗:ft(a) =ma∗−1(22) Hence, for any a∈A: ft(a)≤ma∗−1< ma∗≤ma (23) To show i) ( mm(Fd,A)/\e}atio\slash=mm(Fy,A)) note that a∗≺Fd a(fora/\e}atio\slash=a∗) where/precedesequalFd=mm(Fd,A)by deﬁnition of ft (Eq. 22). Thus a∗is not a maximal element of mm(Fd,A), and so it must be that mm(Fd,A)/\e}atio\slash=mm(Fy,A). To show ii) ( mm(F,A) =mm(Fd,A)): we can derive thatmint∈Tt(a) =ft(a)holds for any a. This is because mint∈T({t(a)|t∈Ty/uniontext{ft}}=ft(a), by Eq. 23. Since maximin mm orders actions based on which one has the higher minimum evaluation and the minimum evaluation is always given by ft(a), it must be the case that mm(F,A) = mm(Fd,A). Since we have shown i) and ii), we have shown that mm is Pascalian. Theorem 3. The social welfare functional k-thm (ktrimmed Highest Mean) is k′-fanatical for any k < k′< 0.5. Proof. The intuition behind this proof is that when k′> k, the fanatical theories are not ‘trimmed out’ and thus k-thm (more-or-less) reduces to mec, which is k′-fanatical. Letk′∈(k,0.5)be arbitrary, Fy= (Ty,cy)an ethical framework and Aa set of actions. By deﬁnition of fanaticism (Deﬁnition 10) k-thm is shown to be k′-fanatical if we can prove that there exists a set of ethical theories Tdsuch that: 1)Tdis a dominant subset given FandA, and 2)Tdhas at mostk′credence, i.e./summationtext t∈Tdc(t)≤k′, whereF= (T,c)is obtained by extending Fy= (Ty,cy)withTd. We thus show that there exists a ‘fanatical’ ethical theory ftsuch that Td={ft}andc(ft) =k′. We can see that 2) holds since/summationtext t∈Tdc(t) =c(ft) =k′. It remains to show that 1) holds, i.e. that Tdis a dominant subset given Fand A.LetFx= (Tx,cx)be an ethical framework where Tx= {tx}andcx(tx) = 1 such that for any a∈A, let tx(a) =wam(Fy a,a)/(1−k′) (24) whereFy a= (Ty,ca)such that for t∈ (bottom k′(a)/uniontexttopk′(a)),ca(t) = 0 and otherwise ca(t) =cy(t). In other words, txis an ethical theory, whose evaluation of any action is proportional to the k-trimmed weighted arithmetic mean (see Deﬁnition 5); in later parts of this proof, it will be clear why we divide by (1−k′). LetF′= (T′,c′)be the framework obtained by extendingFxwithT′whereT′=Tx/uniontextTd={tx,ft}, such that c′(ft) =k′andc′(tx) = 1−k′. We will now prove that a/precedesequalF′b⇔wam(F′,a)≤wam(F′,b) (25) where/precedesequalF′=k-thm(F,c). By deﬁnition of k-thm (Deﬁnition 5), a/precedesequalF′biff wam(Fa,a)≤wam(Fb,b), where Fa= (T,ca),ca(t) = 0 fort∈(bottom k(a)/uniontexttopk(a)), elseca(t) =c(t); Fb= (T,cb),cb(t) = 0 fort∈(bottom k(b)/uniontexttopk(b)), elsecb(t) =c(t). Note that because c(ft) =k′> k,ftcan not be ‘trimmed out’, i.e. for any a′∈A,ft/∈(bottom k′(a′)/uniontexttopk′(a′)). Thus, for any a′∈A,ca′(ft) =c(ft) =k′. By the above and by the deﬁnition of wam , for anya′∈ A, wam(Fa′,a′) =/summationdisplay t∈Tca′(t)t(a′) =k′ft(a′)+/summationdisplay t∈Tyca′(t)t(a′)(26) (recall that Ty=T\{ft}). Note that by deﬁnition of k-thm (Deﬁnition 5)/summationtext t∈Tyca′(t)t(a′) =wam(Fy a′,a′). By Eq. 24, we thus have/summationtext t∈Tyca′(t)t(a′) = (1−k′)tx(a′). Therefore, overall wam(Fa′,a′) =k′ft(a′)+(1−k′)tx(a′) (27) Note that the rhs, by deﬁnition of wam , is thus equal to wam(F′,a′)). Hence, wam(Fa′,a′) =wam(F′,a′) (28) By Eq. 28 and by the deﬁnition of k-thm , we have thus proven Eq. 25. Let/precedesequalmec=mec(F′,a). Then by deﬁnition (Deﬁnition 3) a/precedesequalmecb⇔wam(F′,a)≤wam(F′,b) (29) From Eq. 25 and Eq. 29, we can thus derive that /precedesequalmec=/precedesequalF′. From the proof of Theorem 1 we know that it’s possible to deﬁne ftsuch that Td={ft}is a dominant subset for /precedesequalmec. Since/precedesequalmec=/precedesequalF′,Tdmust be a dominant subset for /precedesequalF′too. Theorem 4. The social welfare functional k-thm (ktrimmed Highest Mean) is not k′-fanatical for any 0< k′≤ k.Proof. The intuition behind this proof is that theories with less than kcredence are ‘trimmed out’ if they have extreme values. As such, fanaticism is avoided. We prove that k-thm is notk′-fanatical for any k∈(0,k), by giving a counterexample such that there cannot exist a dominant subset. LetAbe a set of actions and Fy= (Ty,cy)an ethical framework. By the deﬁnition of fanaticism (Deﬁnition 10) if there is no framework Fsuch that: Fis obtained by extendingFywithT=Ty/uniontextTdandTdis any subset of theories with at most kcredence, and Tdis a dominant subset, then hmis notk-fanatical. LetTy={t}andA={a,b}such that cy(t) = 1 ,t(a) = 1andt(b) = 0 . LetFbe obtained by extending FywithT= Ty/uniontextTd, whereTdis any subset of theories with at most k′ credence, i.e./summationtext t′∈Tdc(t′)≤k′. Given that credences in T={t}/uniontextTdsum up to 1, we have that c(t)≥1−k′. Sincek′< k <0.5, we then have c(t)>0.5. We show that for any t′∈Tdand anya′∈Ait must be the case that either t′∈bottom k(a′)ort′∈topk(a′). First, leta′∈A. Because c(t)> k,t /∈bottom k(a′)and t /∈topk(a′); by their deﬁnition, either set contains theories of only up to total k′credence. Therefore, for any other theories t′∈Td,t′∈ bottom k(a′)ort′∈topk(a′). This is because the sum of the total credence of theories in Tdis less than k, i.e./summationtext t∈Tdc(t) =k′≤k. And so both k′endandk′start must be so that tis excluded but no other theory. Now, since for all t′∈Tdeithert′∈bottom k(a′)ort′∈ topk(a′)is true, it must be the case that they get ‘trimmed out’ by k-thm for anya′. Therefore, the theories in Tdare ignored and hence cannot form a dominant subset. Theorem 5. The social welfare functional hmis notkfanatical for any k∈(0,0.5). Proof. We prove that hmis notk-fanatical for any k∈ (0,0.5), by giving a counter-example such that there cannot exist a dominant subset. Let Abe a set of actions and Fy= (Ty,cy)an ethical framework. By the deﬁnition of fanaticism (Deﬁnition 10) if there is no framework Fsuch that:Fis obtained by extending FywithT=Ty/uniontextTdand Tdis any subset of theories with at most kcredence, and Td is a dominant subset, then hmis notk-fanatical. LetTy={t}andA={a,b}such that cy(t) = 1 ,t(a) = 1andt(b) = 0 . LetFbe obtained by extending FywithT= Ty/uniontextTd, whereTdis any subset of theories with at most kcredence, i.e./summationtext t′∈Tdc(t′)≤k. Given that credences in T={t}/uniontextTdsum up to 1, we have that c(t)≥1−k. Since k <0.5it must be that c(t)>0.5 We now show that by the properties of the weighted median, and given that c(t)>0.5, then it must be that for anya′∈A:wmedian (F,a′) =t(a′). Letse(F,a′)be a sorted list of evaluations of a′andst(F,a′)the corresponding sorted list of theories. Let jbe a number such that st(F,a′)j=t. The equations /summationdisplay i∈[1,m]c(st(F,a′))≤1/2and/summationdisplay i∈[m+1,n]c(st(F,a′))≤1/2 (30)are satisﬁed by m=j. This is because c(t)>0.5and so for any subset of theories T′⊆(T\{t}), it is the case that/summationtext t′∈T′c(t′)<0.5. Now either mis uniquely determined or there are two such values for m. Ifmis uniquely determined (and thus m=j) then by deﬁnition of wmedian for anya′, we have thatwmedian (F,a′) =st(F,a′)j=t(a′). Otherwise, if m is not uniquely determined, we arrive at a contradiction. To see this, assume that there exists l/\e}atio\slash=jsuch that lsatisﬁes Equations 30. Note that either j < l orj > l . Ifj < l , then /summationdisplay i∈[1,l]c(st(F,a′)) =c(t)+/summationdisplay i∈[1,l],i/negationslash=jc(st(F,a′))>0.5 (31) (givent=st(F,a′)jandc(t)>0.5). Therefore, j < l cannot be the case. But symmetrically, if j > l then /summationdisplay i∈[l+1,n]c(st(F,a′)) =c(t)+/summationdisplay i∈[l+1,n],i/negationslash=jc(st(F,a′))>0.5 (32) and soj > l also cannot be the case. Therefore, m must be uniquely determined and so for any a′∈A, wmedian (F,a′) =t(a′). Let/precedesequalFdenotehm(F,A). Note that wmedian (F,b) = t(b) = 0 andwmedian (F,a) =t(a) = 1 . Hence, by deﬁnition ofhm(Deﬁnition 6) it must be the case that b≺Fa. Lethm(Fy,A)=/precedesequaly. Trivially, the weighted median is such that wmedian (Fy,b) =t(b) = 0 and wmedian (Fy,a) =t(a) = 1 . Thus,b≺yaholds. Therefore,/precedesequalF=/precedesequaly. Sincehm(Fy,A) =hm(F,A),Tdcannot be a fanatical subset. Therefore, for arbitrary k∈(0,0.5),hmis notkfanatical.