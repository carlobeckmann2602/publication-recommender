id,src,src_id,doi,author,title,abstract,full_text
1,https://arxiv.org,2301.00001,10.48550/arXiv.2301.00001,"Jordan Thompson, Ryan Benac, Kidus Olana, Talha Hassan, Andrew Sward, Tauheed Khan Mohd",NFTrig,"NFTrig is a web-based application created for use as an educational tool to teach trigonometry and block chain technology. Creation of the application includes front and back end development as well as integration with other outside sources including MetaMask and OpenSea. The primary development languages include HTML, CSS (Bootstrap 5), and JavaScript as well as Solidity for smart contract creation. The application itself is hosted on Moralis utilizing their Web3 API. This technical report describes how the application was created, what the application requires, and smart contract design with security considerations in mind. The NFTrig application has underwent significant testing and validation prior to and after deployment. Future suggestions and recommendations for further development, maintenance, and use in other fields for education are also described.","NFTrig: Using Blockchain Technologies for Math Education JORDAN THOMPSON, Augustana College, USA RYAN BENAC, Augustana College, USA KIDUS OLANA, Augustana College, USA TALHA HASSAN, Augustana College, USA ANDREW SWARD, Augustana College, USA TAUHEED KHAN MOHD, Augustana College, USA NFTrig is a web-based application created for use as an educational tool to teach trigonometry and block chain technology. Creation of the application includes front and back end development as well as integration with other outside sources including MetaMask and OpenSea. The primary development languages include HTML, CSS (Bootstrap 5), and JavaScript as well as Solidity for smart contract creation. The application itself is hosted on Moralis utilizing their Web3 API. This technical report describes how the application was created, what the application requires, and smart contract design with security considerations in mind. The NFTrig application has underwent significant testing and validation prior to and after deployment. Future suggestions and recommendations for further development, maintenance, and use in other fields for education are also described. CCS Concepts: ‚Ä¢Computer systems organization ‚ÜíRedundancy ; Robotics; ‚Ä¢Networks ‚ÜíNetwork reliability. Additional Key Words and Phrases: Matic, Metamask, polygon, bootstrap5, Solidity 1 INTRODUCTION The purpose of this report is to describe the technical details involved in the development of the NFTrig application. This includes both the front end website design, the back end smart contract, and NFT creation. It will mainly focus on the technical details of the project outlining software requirements, design through programming languages, client and server side interactions, and validation testing. This allows the reader to undertake further development, fixes, or maintenance of the software, as this forms part of the documentation for the software. The NFTrig project is based around the creation of a web-based game application that allows interaction of NFTs (non-fungible token) with trigonometric function designs. NFts are digital assets, for example a picture, that has a unique identification and can generally be freely traded with cryptocurrency [ 33]. Through this application, users are able to purchase digital artwork of many different trigonometric functions and combine them using mathematical operations. Current supported operations include multiplication and division of the trigonometry functions, and the output of each operation is a new NFT card that would be the result of an operation. The old cards will then be removed from the user‚Äôs possession and burned using the smart contact. For example, if a user combined the two cards Sin(x) and Cos(x) using multiplication, they would lose their two old cards and receive the new card Tan(x). Further, the NFT cards are assigned one of the following rarity levels: common, uncommon, rare, and legendary. The probability of each of these levels is defined later in this report. The application also allows a user to connect to MetaMask, a digital wallet capable of storing a user‚Äôs cryptocurrency and NFTs as well as a way to connect to block chain. The NFTrig application Authors‚Äô addresses: Jordan Thompson, jordanthompson18@augustana.edu, Augustana College, Rock Island, USA; Ryan Benac, ryanbenac18@augustana.edu, Augustana College, Rock Island, USA; Kidus Olana, kidusolana18@augustana.edu, Augustana College, Rock Island, USA; Talha Hassan, talhahassan18@augustana.edu, Augustana College, Rock Island, USA; Andrew Sward, andrewsward@augustana.edu, Augustana College, Rock Island, USA; Tauheed Khan Mohd, tauheedkhanmohd@augustana.edu, Augustana College, Rock Island, USA.arXiv:2301.00001v1  [cs.HC]  21 Dec 20222 Jordan Thompson, Ryan Benac, Kidus Olana, Talha Hassan, Andrew Sward, and Tauheed Khan Mohd can also display the NFTs owned by the user and allow them to connect to OpenSea to sell the NFTrig cards on a public marketplace. The application is hosted on Moralis employing their Web3 API. Technical languages used in this project, which will be discussed in detail throughout this paper, include front end web development languages HTML, CSS (specifically Bootstrap5), and JavaScript as well as the back end smart contract development language Solidity. In order to attract users, this application also allows a user to answer trivia questions and gain experience points. These points can then be used to unlock new sets of NFT cards or upgrade existing cards in a user‚Äôs wallet. This game-like design should appeal to a younger audience and encourage them to answer trigonometry or math based questions. This will have an incredible educational benefit for the user because they will be both learning and playing a game simultaneously. 2 MOTIVATION The purpose of this application is as an educational tool for students who are attempting to understand the ways that trigonometric functions interact with each other. As opposed to just graphing these functions by hand, students will be able to generate new NFTs by combining whatever trigonometric functions they already own. In fact, using technology is shown to influence and better educational processes by increasing interaction between those in the classroom [ 9]. Technology is becoming increasingly prevalent in every sphere of daily life, so the use of technology in a classroom setting is not only logical, but it increases the educational benefit of students [ 29]. However, as the technology continues to evolve, ""the gap between traditional course material taught to students in B.S./M.S. programs at universities and the cutting edge of technology used in industry is widening at an unprecedented rate"" [30]. By creating this project, it will give students the opportunity to gain experience with block chain, and hopefully be a starting place for narrowing that ever growing gap. After much research, it is likely that this proposed application is the first of its kind that utilizes NFTs to teach mathematical concepts. Aside from user benefit of this application, there is also an intellectual merit in the block chain and education fields. Best described by Carmen Holotescu, ""As education becomes more open, diversified, democratised, and decentralised, the block chain technology is taken in consideration by researchers, teachers and institutions, to maintain reputation, trust in certification, and proof of learning"" [ 17]. Further, development of this project continues research on NFT and block chain technologies. This application can also serve as the boilerplate basis for other NFT-based educational tools and resources. Research for this project provides opportunities for training computer science students on how to use NFTs in general, but more specifically in educational contexts. NFTrig was developed by computer science students as a final senior inquiry project at Augustana College. In conjunction and with funding by the Department of Mathematics and Computer Science, this project employs a variety of software development skills and techniques that further the research and understanding of the block chain and web development field. 3 RELATED WORK Block chain technology has enabled the formation of decentralized distributed records of digital data which does not require any third party to moderate any transactions [ 34]. The decentralized nature of block chain also renders it easy for use in a ranging variety of applications in several fields such as healthcare [ 16], internet of things [ 7], gaming [ 2], banking [ 6], and education (explored in greater detail in subsection 3.1). Non Fungible token (NFTs) are a relatively new phenomena within the field of block chain based technologies, but its application in aforementioned fields are already being studied. Specifically within the healthcare context, NFT‚Äôs are solving long term issues such as storing patients‚Äô private data more safely as well as maintaining better records while giving better autonomy and privacy to both patients and healthcare providers [ 22]. The application of NFTs inNFTrig: Using Blockchain Technologies for Math Education 3 education is still an understudied area. These next related work sections explore the broader use of block chain based technologies for educational purposes, gamification, and overall collaborative learning. 3.1 Block chain Based Technologies for Educational Purposes There has been extensive work concerning how block chain based technologies are enabling better ownership and sharing of personal records for students and supporting collaborative learning environments. Yumna et al. conducted a systematic literature review of the use of block chain technologies in educational sector [ 35]. They also propose several uses of existing block chain based technologies in educational sector that leverage the decentralized and traceable consensus making mechanisms of block chain. Researchers have examined the use of block chain to allow students to maintain educational records such as transcripts, credentials, diplomas, and learning activities [ 5,14,31]. Similarly, research has also explored learning management systems design based on block chain based technology. The technology can potentially verify a students records as well as enable the design of an automatic decentralized enrollment system which does not require moderation from school staff [31]. Another elegant use of block chain in the field of education is the ability to support life-long learning applications. The educational sector is becoming more diverse with a variety of different types of classrooms and learning modalities. E-learning has also allowed students to acquire licences and accreditation online. Therefore, it is imperative to maintain the learning journeys of students over time to understand the different types of learning that they have been engaging in and improving on over time. The traceable nature of block chain based technologies (defined as one of the salient features in the aforementioned systematic review by [ 35]) enables all of these applications. The decentralized nature of block chains coupled with the consensus making algorithms also makes it suitable for collaborative environments. Prior research has looked at how block chain based technologies can enable better developmental experiences in the realm of business [ 11] but there is very minimal work on its application within the field of education application[3]. 3.2 Applications in Education Application and Collaborative Learning Although preliminary in nature, limited prior work has explored the utilization of NFTs for design- ing various different independent learning environments for students. There are some proposed commercial systems that have analogous functioning to some of the systems described in the prior section. For example, commercial systems are looking at leveraging NFTs to award ‚ÄúPass"" status to students for different courses1. NFTs enjoy a key advantage over conventional block chain technologies as they are typically designed using the more secure Ethereum block chain enabling an even more secure record and identity management. Researchers have shown that there is promise in using NFTs as academic tokens to represent student transcripts and other records as well that can be more easily verified [9]. However, there is still a dearth of academic literature in this field. Student incentivization is heavily advocated in pedagogical literature [ 12]. NFTs make it easier to tie incentivization to learning outcomes as they can be automatically acquired by students at any time upon completion of learning outcomes. This gives NFTs based certifications an advantage over the more traditional learning settings where students have to strongly adhere to semester timelines. Elmessiry et al. has looked at designing an incentive mechanism that can be used by teachers and students to achieve better learning outcomes in an effective and cost-efficient manner 1A teacher at Pepperdine University using NFTs to award course completion certifications to students: https://upcea.edu/tech- trends-in-higher-ed-metaverse-nft-and-dao/4 Jordan Thompson, Ryan Benac, Kidus Olana, Talha Hassan, Andrew Sward, and Tauheed Khan Mohd [9]. They also concluded there was better engagement outcomes for students. On several metrics of usability, the students reported more than 80% preference for buying, using, and collecting NFTs. Such independent learning methods were particularly more useful during the COVID-19 pandemic to accommodate the need of remote independent learning options. Architecturally, this project takes inspiration from [ 9], and applies it to a more narrower, focused domain of learning mathematical operations in this study. Further, these NFTs are also easier to share on social media [20]. Therefore, it also allows students to more readily share their accomplishments. 3.3 Gamification to Support Mathematical Learning Since the proposed application teaches mathematical and trigonometric formulas to students, the literature on use of gamification to support mathematical learning should be better described. Gamification, in combination with incentivization explained in the previous section, will allow for the success of this application. Gaming settings have traditionally been used to teach simple mathematical operations to students. More recently, researchers have also proposed systems that teach advanced concepts to students including College Algebra [ 10]. These learning environments make it easier for students to relate the learning concepts with more daily life phenomena. While gamification itself cannot guarantee better learning outcomes, it can improve students‚Äô interest and performance by encouraging them to engage with the content for a longer duration of time [18]. The simpler, more systematic, and operational nature of mathematics as a subject also makes it easier for incorporation in gaming environments because final answers are usually short and numerical as opposed to long and descriptive answer that might be found in social or natural sciences. Trigonometry especially can easily be broken down into a series of operations and steps which simulates a similar environment found in other online games where users play to find different ‚Äúrewards"" and ‚Äúcollectables"". Despite all these benefits there are some limitations of gamification as well. For example, it is hard to know how a student arrived a solution and give feedback [ 4]. Not being able to solve trigonometric equations can also lead to frustration and impeded learning experience. Foresight into the project‚Äôs future looks to mitigate these concerns by fostering better communication between different game players and providing links to useful learning resources in the application. Prior research has extensively explored the use of gamification in different mathematical fields. This application is likely the first to extend the use of NFTs and block chain to aid in teaching trigonometric equations. Research shows that technology, specifically games are shown to be excellent educational tools. In fact, ""one of the most successful positive reinforcement mechanisms [in education] known is gamification"" [ 9]. This includes taking a topic transforming it into a game with positive reinforce- ment. This leverages educational benefits in students and encourages them to continue playing the game to learn. Nftrig has future plans to add a game function which will allow the user to answer trigonometry trivia and math questions. This will aid in both their learning and the continued use of the NFTrig application. Further, the ability to combine owned NFTs with math functions also aids in the education of trigonometry for the student. 4 EXPERIMENTAL SETUP 4.1 Software Development Requirements The NFTrig application employs a variety of software development requirements that cover the range of the project. From front end web development to back end smart contract creation and NFT storage, this section describes the requirements and software used to complete the project. 4.1.1 Compiling IDE. The smart contracts created for NFTrig are hosted on Remix. Remix is an an open source online compiler IDE that can be used to test and deploy smart contracts [ 1]. TheNFTrig: Using Blockchain Technologies for Math Education 5 platform can be accessed by any browser, and it allows the developer to write and deploy smart contracts on an actual or test server simultaneously. The current deployment is on a test server. In order to test and debug the smart contract, Visual Studio Code is used. Visual Studio was found to be the best code editor because a developer can easily upload most file types, and edit them [19]. For NFTrig, it was used to develop front end HTML and CSS files, as well as back end solidity contract editing. The required installed plugins for Visual Studio (VS) include Solidity and Block chain development. [21] These allowed for simple, straightforward development of code. 4.1.2 Moralis. Moralis SDK is the primary back end platform for the project. The platform allows connection of the front end web application to the smart contract. [ 8] The Moralis platform uses a combination of server management and a JavaScript SDK to allow for maximum interaction and simplicity. A developer can do many tasks through this including authentication of users, getting necessary user data, and connecting with MetaMask in a non-complicated and simply coded process. The only expectation is that a developer will need to have programming knowledge in JavaScript as well as a familiarity with Moralis and MetaMask, experience querying a database, and some knowledge of Web3 development to ensure maximum results and efficiency. Moralis also has the ability to easily connect to MetaMask. 4.1.3 MetaMask. MetaMask is the digital wallet required for participation in the NFTrig game application. It allows the collection of purchases from the user, and it can be installed as an extension on a browser for increased ease of use [ 28]. MetaMask stores all NFTs owned by the user, and in connection with the NFTrig application, can view and upgrade or modify existing NFTs at a users discretion. Connection to the browser extension is required for the application to access anything owned by a user [ 24]. Because MetaMask is easily integrated into Moralis, and thus NFTrig, there is little a user needs to do to create a connection aside from installing the MetaMask extension, and clicking connect. 4.1.4 Front End Design. Front end design was accomplished primarily through Visual Studio. The Live Server extension was installed which allows each developer to ""host"" their developed website using a native web application. Doing so allowed simplified testing and front end development. Instead of creating CSS files from scratch, the NFTrig interface heavily employs Bootstrap5, which simplifies the process of modifying the content layout and design of buttons and other content [25]. Moralis and Bootstrap5 each have extensive documentation to understand and support front end web development. These tools have been utilized to a near maximum extent. 4.1.5 Web Hosting Platform. The initial testing of NFTrig, as previously explained, was hosted on a local live server through Visual Studio. After initial development, the project was moved to a web server hosted by Augustana College so that initial testing could begin. It is currently unclear how the site will ultimately be hosted. One option for hosting the web application is directly through Google [ 32]. This would allow the website to be named something easily searchable and accessible. A second option would be to host directly through Moralis, but a limitation of this would be a more diluted website naming convention along with a more confusion process of uploading and modifying website content. Currently, the NFTrig application will remain on the local Augustana College Server. 5 SOFTWARE DESIGN This section covers all of the decisions necessary to understand the development of NFTrig, as well as the technical implementation of each technology used in the design process.6 Jordan Thompson, Ryan Benac, Kidus Olana, Talha Hassan, Andrew Sward, and Tauheed Khan Mohd 5.1 Software Architecture The architecture of this project follows the model-server design architecture [ 27]. Using this model, the clients send transactions and requests to a proxy smart contract stored on the block chain which then makes the appropriate calls to the logic smart contract which is also stored on the block chain. This style of architecture is required for this project because the smart contracts must be stored on the server-side chain in order to be functional. The use of proxy contracts also allows our smart contracts to be fully upgradeable with any future updates that may need to be implemented. 5.2 Choice of Programming Language This section examines and explains the benefit of each chosen language employed in NFTrig. Front end languages include HTML and CSS and the back end includes Solidity and JavaScript. Each has been chosen because they were found to be the best option for development. 5.2.1 Solidity. Solidity is the programming language of choice when it comes to coding smart contracts. Solidity is ""similar to JavaScript and yet has some features of object-oriented languages such as Java and C++"" [ 26]. This is a leading language for the development of smart contracts and use on block chain technologies. This project utilizes the solidity library openzeppelin in order to create a solid foundation for the smart contracts. Hardhat and Node JS are then used for the testing and deployment of the smart contracts to the Polygon blockchain. 5.2.2 JavaScript. In the NFTrig application, JavaScript (JS) is primarily used in the front end application. The primary purpose of this language is generally to create dynamic and interactive web content [ 15]. For the client, JS was used in the navigation bar to allow for clickable links and resizing of the navigation bar in smaller screens. This language was also used to give buttons functionality ranging from logging in to MetaMask to purchasing NFTrig cards. Further, JS was used to test the logic of the front-end combination page until the smart contract was applied. Aside from augmenting HTML and CSS application pages, JavaScript is also used in this project to connect the back end smart contract with the from end web application. This application was also developed using Next JS and deployed via an application known as vercel. 5.2.3 HTML and CSS. Web development of the user interface was primarily completed using HTML and CSS (Bootstrap5). These languages are equally popular and necessary to develop the web pages [ 13]. Instead of creating all CSS requirements from scratch, Bootstrap5 was utilized to allow for cleaner design across web pages and better alignment of web page elements. Bootstrap5 also simplifies the need to explicitly code buttons and other interactive items. 5.3 Security Considerations Throughout this project, there have been several security considerations discovered that threatened the safety and use of the application. One such discovered issue was initially, there was no code written to block a user from looking at another users token. Further, before minting a new NFT card, the smart contracts check to ensure that the card does not already exist, the cards used for combining are owned by the user, and that the newly minted card follows the correct probabilities of outcomes shows in 2. These probabilities are coded into the smart contract. 5.4 Smart Contract Design The smart contract for this project is broken up into two separate contracts. The first of which is the NFTrig logic contract which contains the logic for purchasing packs of cards as well as the logic for how cards will interact with each other. The second contract is the marketplace contract which will allow users to trade their own NFTs with other users through the website. Within the NFTrigNFTrig: Using Blockchain Technologies for Math Education 7 contract, there are functions for multiplying and dividing cards, purchasing randomized packs of cards, and tracking the details of each individual token as transactions are made. The marketplace contract contains information about sale history as well as the functionality to post new sales and purchase items for sale. Both of these contracts were deployed as upgradeable contracts so they can have updates implemented in the future. 5.5 NFT Storage and Naming Conventions All NFT images are stored on the server with the HTML, CSS, and JS files. The naming convention for each image references what image it is in four numbers. The first number is the power of sin, the second is the power of cos, the third is the rarity or color of the card (0-3 is green, blue, purple, and red respectively), and the final number is the text variant (0-3). These files were named accordingly to better determine the output if cards were combined using a mathematical function. For example, a sin card might have the naming convention: 1023.jpg. 10 defines it is a sin card, 2 defines it is rarity purple, and 3 defines it is text variant 3. The purpose of naming the files in this way is so that the front end can easily determine which image corresponds to a particular NFT by simply looking at the four features of each token which match the four numbers in the file name. 5.6 Client Design The NFTrig application interface was designed using HTML and CSS. The primary use of CSS was often replaced by Bootstrap5. Bootstrap 5, a library for CSS, allows for easier scaling and alignment of objects in the HTML file, and thus the computer screen [ 23]. Documentation on the Bootstrap5 has utilized to a full extent. Each section examines the layout and use of each application page. 5.6.1 NFTrig Home. The interface is designed to allow a user to access the marketplace, their individual current collections, and their profile. The navigational bar contains links to the client-side facing pages: NFTrigHome, MyCards, CombineCards, Marketplace, and Game. We used a total of three colors to enable good contrast and make it easier for our users to view complex graphs and formula without a cluttered background2. The JavaScript elements declared are reusable across multiple screens. They support functions and interactions such as a user hovering over a cell or clicking a cell and providing both feedback and error handling to the user. The navigation bar is also, the top bar changes color to indicate the tab that the user is on. 5.6.2 Combination. The main purpose of the combination page is for users to choose cards that they currently own, and see options for combining them using either multiplication or division. Figure 1 displays the layout of the screen where user selected cards are shown on the left, and potential results are shown on the right. The page utilizes Bootstrap5 capabilities to format effectively to different screen sizes and resolutions. It connects with a back end script to the smart contract. This provides functionality to the buttons and easy generation of possible NFT results. Below shows the probabilities of generated NFT outcomes based on the selected input cards. 5.6.3 Marketplace and MyCards. Marketplace and MyCards are similar pages, as they connect to a data source and display NFTs. The Marketplace tab shows all NFT cards available for purchase both from other users who own NFTs and cards owned by the NFTrig project. MyCards however specifically shows all cards owned by a user. The layout for each generates all necessary NFT images and information about the rarity. The rarity is signified by the color and the text option of the card. Figure 3 shows the actual layout displayed on the page. 2Background-color:#333, Color: #f2f2f2,8 Jordan Thompson, Ryan Benac, Kidus Olana, Talha Hassan, Andrew Sward, and Tauheed Khan Mohd Fig. 1. Interface where users will combine NFTrigs Fig. 2. Probabilities of outcomes depending on rarity of selected cards 5.6.4 Quality attributes of client-side interface and code. In order to have an application of quality, consistency, and accuracy, the project followed the following guidelines: (1)The code is written in a manner that components and layouts can be rearranged to support any structural changes in the front end. (2)The code has consistent style and format, such as the padding used in individual NFTrig elements and the purchase page‚Äôs color. (3)The code contains comments and is well indented for easy maintenance and understanding. (4)Consistent colors and feedback systems are provided so the system is easy to learn for users. (5) Page-level styling was avoided when possible to keep design consistent. (6) Thorough testing was completed for basic accessibility features. 5.6.5 Testing the Client Design. Basic unit testing of different elements was initially conducted to ensure easy navigation between front end pages. In order to ensure that testing would coverNFTrig: Using Blockchain Technologies for Math Education 9 Fig. 3. Interface displaying NFTrig Marketplace most application uses, three user cases were devised: a user browsing NFTrigs, a user making a purchase, and a user combining NFTrigs. All assumptions and expected actions expected from the system were listed and analyzed through testing. Further, testing through some edge cases were also pursued. Currently, the application works as intended, however future plans involve rigorous testing with JavaScript code and external APIs (if any are devised). This will ensure a fully functional, secure, and usable application that can also be used as a boiler plate project for other educational blockchain technologies. 5.6.6 Future Work: Game. Future work for this project will include the ability for users to play a trivia and trigonometric equation game. This allows a user to gain experience points that they can then use to purchase new NFTs. This eliminates the need to always need cryptocurrency to purchase individual or group NFT cards. Although there is not currently an interface for this page written in HTML, functionality exists for the trivia game itself. The files are currently stored on the server, but they are disabled and there is no navigable way to get there through the application. 6 METHODS Most methods for completing this project have been thoroughly explained in the sections above. However, the final intended version of this project will be hosted in a different location than it resides currently. The initial portion of this project had the front end website hosted on a local Augustana College server and the back end smart contract hosted on the Polygon test net. This allowed initial testing and validation that the smart contract operated as expected, as well as give time and opportunity to discover security vulnerabilities. The future of this project will be hosted on a decentralized web application online so that users can access it and begin to interact with the smart contract. Further, a redesign of the website user interface is likely. This will require transition from BootStrap5 to NextJS which allows cards to be generated, displayed, and interactable through a version of JavaScript. 7 RESULTS This project successfully allowed the exploration and creation of applying NFT and block chain technology to math education. Although preliminary in use and nature, this project allows for initial project creation as a boiler plate project. The smart contract is currently deployed on the10 Jordan Thompson, Ryan Benac, Kidus Olana, Talha Hassan, Andrew Sward, and Tauheed Khan Mohd Polygon testnet and can be interacted with using test Matic. Each web page has functionality to display the user‚Äôs owned NFTs as well as the NFTs they have put for sale on the marketplace. Using NextJS will also allow the Combination page to have functionality and smart contract use. It is also worth noting that the created web page is not required to interact with the NFTrig smart contracts. 8 RECOMMENDATIONS FOR FUTURE WORK The goal for this project was a working Beta demo that shows application functionality, and correct smart contract execution. There are many other features planned for the continued work of this project. The first, as earlier explained, is a game option which challenges the user with trigonometry trivia and math problems. Answering these questions successfully will increase the experience points of a user. The user can then use these experience points to purchase individual or packs of NFTrig cards, or they can be used to combine cards. REFERENCES [1]Rana M Amir Latif, Khalid Hussain, NZ Jhanjhi, Anand Nayyar, and Osama Rizwan. 2020. A remix IDE: smart contract-based framework for the healthcare sector by using Blockchain technology. Multimedia Tools and Applications (2020), 1‚Äì24. [2]Mohsen Attaran and Angappa Gunasekaran. 2019. Blockchain for Gaming. In Applications of Blockchain Technology in Business . Springer, 85‚Äì88. [3]Rocsana Bucea-Manea- T,oni≈ü, Oliva Martins, Radu Bucea-Manea- T,oni≈ü, CƒÉtƒÉlin Gheorghi t,ƒÉ, Valentin Kuleto, Milena P Iliƒá, and Violeta-Elena Simion. 2021. Blockchain Technology Enhances Sustainable Higher Education. Sustainability 13, 22 (2021), 12347. [4]Juan Jos√© Bull√≥n, Ascensi√≥n Hern√°ndez Encinas, M. Jes√∫s Santos S√°nchez, and V√≠ctor Gayoso Mart√≠nez. 2018. Analysis of student feedback when using gamification tools in math subjects. In 2018 IEEE Global Engineering Education Conference (EDUCON) . 1818‚Äì1823. https://doi.org/10.1109/EDUCON.2018.8363455 [5]Guang Chen, Bing Xu, Manli Lu, and Nian-Shing Chen. 2018. Exploring blockchain technology and its potential applications for education. Smart Learning Environments 5, 1 (2018), 1‚Äì10. [6]Luisanna Cocco, Andrea Pinna, and Michele Marchesi. 2017. Banking on blockchain: Costs savings thanks to the blockchain technology. Future internet 9, 3 (2017), 25. [7]Marco Conoscenti, Antonio Vetro, and Juan Carlos De Martin. 2016. Blockchain for the Internet of Things: A systematic literature review. In 2016 IEEE/ACS 13th International Conference of Computer Systems and Applications (AICCSA) . IEEE, 1‚Äì6. [8]Oscar Delgado-Mohatar, Ruben Tolosana, Julian Fierrez, and Aythami Morales. 2020. Blockchain in the Internet of Things: Architectures and Implementation. In 2020 IEEE 44th Annual Computers, Software, and Applications Conference (COMPSAC) . 1072‚Äì1077. https://doi.org/10.1109/COMPSAC48688.2020.0-131 [9]A Elmessiry, M Elmessiry, and L Bridgesmith. 2021. NFT STUDENT TEACHER INCENTIVE SYSTEM (NFT-STIS). In Proceedings of EDULEARN21 Conference , Vol. 5. 6th. [10] Usef Faghihi, Albert Brautigam, Kris Jorgenson, David Martin, Angela Brown, Elizabeth Measures, and Sioui Maldonado- Bouchard. 2014. How Gamification Applies for Educational Purpose Specially with College Algebra. Procedia Computer Science 41 (2014), 182‚Äì187. https://doi.org/10.1016/j.procs.2014.11.102 5th Annual International Conference on Biologically Inspired Cognitive Architectures, 2014 BICA. [11] Julian Alberto Garcia-Garcia, Nicol√°s S√°nchez-G√≥mez, David Lizcano, Mar√≠a Jos√© Escalona, and Tom√°s Wojdy≈Ñski. 2020. Using blockchain to improve collaborative business process management: Systematic literature review. IEEE Access 8 (2020), 142312‚Äì142336. [12] Susan Gass, Koen Van Gorp, and Paula Winke. 2019. Using different carrots: How incentivization affects proficiency testing outcomes. Foreign Language Annals 52, 2 (2019), 216‚Äì236. [13] Ammar Yanuar Ghulam. 2021. Konseptual Desain Website Aplikasi Penyedia Jasa Kursus Mengemudi Mobil Di Purwokerto Menggunakan Framework Bootstrap 5. (2021). [14] Alexander Grech and Anthony F Camilleri. 2017. Blockchain in education . Luxembourg: Publications Office of the European Union. [15] Marijn Haverbeke. 2018. Eloquent javascript: A modern introduction to programming . No Starch Press. [16] Marko H√∂lbl, Marko Kompara, Aida Kami≈°aliƒá, and Lili Nemec Zlatolas. 2018. A systematic review of the use of blockchain in healthcare. Symmetry 10, 10 (2018), 470.NFTrig: Using Blockchain Technologies for Math Education 11 [17] Carmen Holotescu et al .2018. Understanding blockchain opportunities and challenges. In Conference proceedings of¬ª eLearning and Software for Education ¬´(eLSE) , Vol. 4. ‚Äù Carol I‚Äù National Defence University Publishing House, 275‚Äì283. [18] Tomislav Jagu≈°t, Ivica Botiƒçki, and Hyo-Jeong So. 2018. Examining competitive, collaborative and adaptive gamification in young learners‚Äô math learning. Computers Education 125 (2018), 444‚Äì457. https://doi.org/10.1016/j.compedu.2018. 06.022 [19] Bruce Johnson. 2012. Professional visual studio 2012 . John Wiley & Sons. [20] Arnav Kapoor, Dipanwita Guhathakurta, Mehul Mathur, Rupanshu Yadav, Manish Gupta, and Ponnurungam Ku- maraguru. 2022. TweetBoost: Influence of Social Media on NFT Valuation. arXiv preprint arXiv:2201.08373 (2022). [21] Parth Khandelwal, Rahul Johari, Varnika Gaur, and Dharm Vashisth. 2021. BlockChain Technology based Smart Contract Agreement on REMIX IDE. In 2021 8th International Conference on Signal Processing and Integrated Networks (SPIN) . 938‚Äì942. https://doi.org/10.1109/SPIN52536.2021.9565983 [22] Kristin Kostick-Quenet, Kenneth D. Mandl, Timo Minssen, I. Glenn Cohen, Urs Gasser, Isaac Kohane, and Amy L. McGuire. 2022. How NFTs could transform health information exchange. Science 375, 6580 (2022), 500‚Äì502. https: //doi.org/10.1126/science.abm2004 arXiv:https://www.science.org/doi/pdf/10.1126/science.abm2004 [23] J√∂rg Krause. 2020. Introduction to Bootstrap. In Introducing Bootstrap 4 . Springer, 1‚Äì17. [24] Wei-Meng Lee. 2019. Using the metamask chrome extension. In Beginning Ethereum Smart Contracts Programming . Springer, 93‚Äì126. [25] Raoul LePage and Lynne Billard. 1992. Exploring the limits of bootstrap . Vol. 270. John Wiley & Sons. [26] Debajani Mohanty. 2018. Basic solidity programming. In Ethereum for Architects and Developers . Springer, 55‚Äì103. [27] Haroon Shakirat Oluwatosin. 2014. Client-server model. IOSRJ Comput. Eng 16, 1 (2014), 2278‚Äì8727. [28] Deni Pramulia and Bayu Anggorojati. 2020. Implementation and evaluation of blockchain based e-voting system with Ethereum and Metamask. In 2020 International Conference on Informatics, Multimedia, Cyber and Information System (ICIMCIS) . 18‚Äì23. https://doi.org/10.1109/ICIMCIS51567.2020.9354310 [29] R Raja and PC Nagasubramani. 2018. Impact of modern technology in education. Journal of Applied and Advanced Research 3, 1 (2018), 33‚Äì35. [30] A Ravishankar Rao and Riddhi Dave. 2019. Developing hands-on laboratory exercises for teaching STEM students the internet-of-things, cloud computing and blockchain applications. In 2019 IEEE Integrated STEM Education Conference (ISEC) . IEEE, 191‚Äì198. [31] Diane J Skiba et al .2017. The potential of blockchain in education and health care. Nursing education perspectives 38, 4 (2017), 220‚Äì221. [32] Craig Standing. 2002. Methodologies for developing Web applications. Information and Software Technology 44, 3 (2002), 151‚Äì159. https://doi.org/10.1016/S0950-5849(02)00002-2 [33] Qin Wang, Rujia Li, Qi Wang, and Shiping Chen. 2021. Non-fungible token (NFT): Overview, evaluation, opportunities and challenges. arXiv preprint arXiv:2105.07447 (2021). [34] Hafiza Yumna, Muhammad Murad Khan, Maria Ikram, and Sabahat Ilyas. 2019. Use of Blockchain in Education: A Systematic Literature Review. In Intelligent Information and Database Systems , Ngoc Thanh Nguyen, Ford Lumban Gaol, Tzung-Pei Hong, and Bogdan Trawi≈Ñski (Eds.). Springer International Publishing, Cham, 191‚Äì202. [35] Hafiza Yumna, Muhammad Murad Khan, Maria Ikram, and Sabahat Ilyas. 2019. Use of blockchain in education: a systematic literature review. In Asian Conference on Intelligent Information and Database Systems . Springer, 191‚Äì202."
2,https://arxiv.org,2301.00002,10.48550/arXiv.2301.00002,"Henan Zhao, Garnett W. Bryant, Wesley Griffin, Judith E. Terrill, Jian Chen",Evaluating Alternative Glyph Design for Showing Large-Magnitude-Range Quantum Spins,"We present experimental results to explore a form of bivariate glyphs for representing large-magnitude-range vectors. The glyphs meet two conditions: (1) two visual dimensions are separable; and (2) one of the two visual dimensions uses a categorical representation (e.g., a categorical colormap). We evaluate how much these two conditions determine the bivariate glyphs' effectiveness. The first experiment asks participants to perform three local tasks requiring reading no more than two glyphs. The second experiment scales up the search space in global tasks when participants must look at the entire scene of hundreds of vector glyphs to get an answer. Our results support that the first condition is necessary for local tasks when a few items are compared. But it is not enough to understand a large amount of data. The second condition is necessary for perceiving global structures of examining very complex datasets. Participants' comments reveal that the categorical features in the bivariate glyphs trigger emergent optimal viewers' behaviors. This work contributes to perceptually accurate glyph representations for revealing patterns from large scientific results. We release source code, quantum physics data, training documents, participants' answers, and statistical analyses for reproducible science this https URL.","JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 1 Evaluating Glyph Design for Showing Large-Magnitude-Range Quantum Spins Henan Zhao, Garnett W. Bryant, Wesley GrifÔ¨Ån, Judith E. Terrill, Jian Chen Abstract ‚ÄîWe present experimental results to explore a form of bivariate glyphs for representing large-magnitude-range vectors. The glyphs meet two conditions: (1) two visual dimensions are separable; and (2) one of the two visual dimensions uses a categorical representation (e.g., a categorical colormap). We evaluate how much these two conditions determine the bivariate glyphs‚Äô effectiveness. The Ô¨Årst experiment asks participants to perform three local tasks requiring reading no more than two glyphs. The second experiment scales up the search space in global tasks when participants must look at the entire scene of hundreds of vector glyphs to get an answer. Our results support that the Ô¨Årst condition is necessary for local tasks when a few items are compared. But it is not enough for understanding a large amount of data. The second condition is necessary for perceiving global structures of examining very complex datasets. Participants‚Äô comments reveal that the categorical features in the bivariate glyphs trigger emergent optimal viewers‚Äô behaviors. This work contributes to perceptually accurate glyph representations for revealing patterns from large scientiÔ¨Åc results. We release source code, quantum physics data, training documents, participants‚Äô answers, and statistical analyses for reproducible science at https :==osf:io= 4xcf5=?view only= 94123139 df9c4ac984a1e0df811cd580. Index Terms ‚ÄîSeparable and integral dimension pairs, bivariate glyph, 3D glyph, quantitative visualization, large-magnitude-range. F 1 I NTRODUCTION BIVARATE glyph visualization is a common form of visual design in which a dataset is depicted by two visual variables, often chosen from a set of perceptually independent graphical dimensions of shape, color, texture, size, orientation, curvature, and so on [1], [2]. A bivariate glyph design [3] has been broadly applied to reveal atom spin behaviors for quantum physicists at National Institute of Standards and Technology (NIST) to examine experi- mental results; thanks to their team‚Äôs Nobel-prize-winning simulations [4]. Quantum physicists world-wide can now manipulate many individual quantum systems to study complex atom and sub-atom interactions. Because atoms can be in multiple states simultaneously and because these spin magnitudes are large in range and often vary greatly in local regions, computational solutions still do not exist to characterize the spin behaviors. Today‚Äôs quantum physicists rely on visualization to interpret simulation results. On the visualization side, the initial design and eval- uation of large-magnitude-range spin vector visualizations use scientiÔ¨Åc notation to depict digit and exponent as two concentric cylinders [3]: inside and outside tube-lengths (lengthylengthyorLyLyorsplitVectors ) are mapped to digit and power of spin magnitude accordingly (Figure 1e). A three-dimensional (3D) bivariate glyph scene of this splitVec- tors design (Figure 2e) achieves up to ten times greater accuracy than the traditional direct approach ( Linear , Fig- ure 2f) for reading a vector magnitude of a single spin or deriving ratios between two spin magnitudes. However, this Henan Zhao is with University of Maryland, Baltimore County. E-mail: henan1@umbc.edu. Garnett W. Bryant and Judith E. Terrill are with the National Institute of Standards and Technology. E-mail: fgarnett.bryant, ju- dith.terrillg@nist.gov. Wesley GrifÔ¨Ån is with Stellar Science. E-mail: grifÔ¨Ån5@umbc.edu. Jian Chen is with The Ohio State University. E-mail: chen.8028@osu.edu.design also increases task completion time for an apparently simple comparison task between two magnitudes in three dimensions (3D): the traditional direct approach of Linear (Figure 2f) is signiÔ¨Åcantly faster than splitVectors (Figure 2e). One may frame this large-magnitude-range issue as a visual design problem: how can we depict a scalar value using bivariate visual features to help quantum physicists examine com- plex spatial data? Intuitively, since all tasks in previous study involve a single or at most two spin locations, human visual system would integrate the two component parts (digit and exponent terms) of a quantum spin into one gestalt before comparing the results [5]. Since relating the digit and expo- nent to the two size features demands a focused attention mode of visual processing, a viewer would take longer to process two component parts in splitVectors compared to a single linear mapping. We term this explanation the object- level hypothesis where a viewer responds to combine two component parts of a value represented in a glyph to its original scalar value (here the magnitude). However, the object-level processing may be neither efÔ¨Å- cient nor necessary. For example, Borgo et al. [6] state that ‚Äú...effective glyph design should encompass a non-conÔ¨Çicting set of separable retinal variables ‚Äù. Now, for our examples, if we increase the bivariate feature separability by replacing the exponent-to-length mapping in Figures 1e and 2e to the exponent-to-color mapping in Figures 1c and 2c for compar- ison tasks, it would be counterproductive for our attention Ô¨Årst to visit each glyph to compute the magnitude. Instead, the global categorical color (hue) can guide our attention to Ô¨Årst compare the exponent, prior to compare vector lengths (digit). In these cases, no object-level attentive processing of bivariate features is needed as long as the two color hues can be easily recognizable. Further considering the quantum physicists‚Äô task rele- vant to multiple objects (e.g., Ô¨Ånd maximum among hun-arXiv:2301.00002v1  [cs.HC]  25 Dec 2022JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 2 Fig. 1: Illustration of Ô¨Åve bivariate conÔ¨Ågurations of vector magnitudes 2(0;9;999]. Three examples show vector magnitudes 440 (4:4102),9;999 (9:9103), and 1(1100). Take 440 as an example, lengthylengthx(a) maps 4:4 (digit) and 2(exponent) to lengths along the y and x axes accordingly ( lengthylengthx); (b)-(e) have the same digit- to-lengthyrepresentation as (a). The exponent representations are manipulated to be (1) more integral or separable from lengthyand (2) more or less categorical. (b) lengthycolor=length xuses color to double-code exponent compared to (a). The exponents in (c), (d), and (e) use color, texture, or outer cylinder length accordingly. Our experimental results support that more separable dimensions lead to more perceptually accurate glyphs. The higher the separability, the higher the accuracy. Also, using a more categorical feature (e.g., color in (c)) of one of the variables beneÔ¨Åted efÔ¨Åciency and accuracy. dreds of vectors) (Figure 2), viewers are likely to check the color legend and then use color to Ô¨Årst divide the scene into subregions, prior to use length for detailed comparisons within the yellow region (Figures 2b and 2c). The colorful scene context beneÔ¨Åts the reduction of search to a much smaller scale via global statistics of the scene. Coinciden- tally, this Ô¨Årst impression of the data to drive structural and statistical information is also called scene-level processing [7]; Wolfe called features guiding this top-down task-driven attention behaviors as scene features. Scene features are also preattentive and can guide attention in visual search toward a target [8], perhaps due to fast ensemble processing [9]. Taken together, an effective design of bivariate glyphs is likely to be inÔ¨Çuenced by two conditions: separable dimensions, with one of them being a pre-attentive scene feature. These two factors are not necessarily independent. For example, For the Ô¨Årst factor, we can follow Borgo et al. [6] and Ware [10] for ‚Äú a non-conÔ¨Çicting set of separable retinal variables ‚Äù. To meet the both conditions to choose the scene feature, we can give preferences of the separable pair when one of the variables is categorical. This is because categorical features are likely to be better at facilitating the perception of a group of objects in the scene [7], [11], [12]. We in this work compared several separable-integral pairs, length-color (Figures 1b, 2b, 1c, 2c), length-texture (Figures 1d,2d), and length-length (Figures 1a, 2a). Among the three features of color, texture, and size, color is categorical and thus ‚Äúmost recognizable‚Äù. Color ensembles are preattentive and permit visual selection at a glance [13]. We purposefully select texture patterns by varying the amount of dark on white, thus introducing luminance variations when many vectors are examined together (Figure 2d). Compared to the continuous random noise in Urness et al. [14], ours is for discrete quantities and thus uses regular scale variations. When coupled with separable features, we hypothesize that highly distinguishable separable dimension pairs, with one being categorical might encourage preattentive global processing to reduce task completion time and be more accurate. We tested this hypothesis in two experiments with six tasks using four pairs to compare against thelengthylengthy(separable) in Zhao et al. [3]: lengthylengthx (integral), lengthycolor (separable), lengthytexture (separable), and lengthycolor=length x (redundant and separable). Since we predicted that separable dimensions with more preattentive features would reduce the task completion time, lengthycolor andlengthycolor=length xmight achieve more efÔ¨Åciency without hampering accuracy than other bivariate pairs. This work makes the following contributions: Empirically validates that bivariate-glyphs encoded byJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 3 (a)Length ylength x(LyLx) (integral)  (b)Length ycolor=length x(LCL ) (redun- dant encoding) (c)Length ycolor (LC) (separable) (d)Length ytexture (LT) (separable)  (e)Length ylength y(splitVectors ,LyLy) [3]  (f)Linear Fig. 2: Real-world large-magnitude-range quantum physics simulation results shown using (a)-(e) Ô¨Åve bivariate feature- pairs and (f) a traditional linear representation. LC,LCL , andLTcan reveal scene spatial structures. We anticipate that two conditions determine the glyph efÔ¨Åciency: (1) the bivariate glyph uses two separable dimensions; and (2) one of the two dimensions uses a categorical representation thus can reveal global structures in data. The Ô¨Årst condition is necessary for local tasks when a few items are compared. The second condition is needed for inspecting the entire scene. highly separable dimensions would improve compari- son task completion time (Exp I). Is the Ô¨Årst to evaluate categorical features in bivirate- glyphs to leverage the beneÔ¨Åts of the global scene features (Exp II). Offers a rank order of separable variables for 3D glyph design and shows that the separable pairs lengthycolor andlengthytexture are among the most effective and efÔ¨Åcient feature pairs. Reveals a novel visual design method for scalable search in big-data. 2 T HEORETICAL FOUNDATIONS IN PERCEPTION AND VISION SCIENCES At least four perceptual and vision science theories have inspired our work: integral and separable dimensions [15], preattentive scene features [7], [8], [16], [17], feature ranking, and monotonicity [2]. Integral and Separable Dimensions. Garner and Felfoldy‚Äôs seminal work on integral and separable dimen- sions [15] has inspired many visualization design guide- lines. Ware [10] suggests a continuum from more inte- gral to more separable pairs: (red-green) -(yellow-blue) ,sizex- sizey,color-shape/size/orientation ,motion-shape/size/orientation ,motion-color , and group position-color . His subsequent award- winning bivariate study [2] using hue-size ,hue-luminance , and hue-texton (texture) supports the idea that more sep- arable dimensions of hue-texton lead to higher accuracy. Our work follows the same ideas of quantifying integral and separable dimensions but differs from Ware‚Äôs texton selection in two important aspects. First, the Ware study focuses on Ô¨Ånding relationships between two independent data variables. In contrast, ours demands participants to examine a complex scene for item discrimination when the two variables are component parts of a vector magnitude. Second, our texture uses the amount of black and white to show luminance variations, in contrast to the discrete shape variation in textons. We anticipate that ours will be more suitable to continuous quantitative values so it is easier to compare large and small to divide the regions [18]. No existing work we know of has studied whether or not one of the separable features being categorical can facilitate global comparisons and can be scaled to large and more complex 3D vector magnitude analysis. Scene-Guidance and Feature Distance. In order to rec- ognize items, viewers do not ‚Äúsee‚Äù features and instead ‚Äúbind‚Äù these features to objects. This binding studies how our visual systems separate object features such as shape, color, motion trajectories, sizes, and distances into the wholeJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 4 object [5]. What we ‚Äúsee‚Äù also depends on our goals and expectations. Wolfe et al. propose the theory of ‚Äú guided search ‚Äù [8], a Ô¨Årst attempt to incorporate users‚Äô goals into viewing. For example, if the viewer‚Äôs goal is to search largest values, s/he can just check the yellow ones in Figure 2. Wolfe et al. [8] further suggest that color, texture, size, and spatial frequency are among the most effective features in attracting the user‚Äôs attention. When we combine features together, Duncan and Humphreys articulate some of the most basic principles. In general, guidance to a target will be stronger when the feature differences between the target (T) and distractor (D) are larger (TD differences), and when the feature differ- ences amongst distractors are smaller (DD similarity) [19]. For example, Ts are 2.3 (digit) and 2 (exponent) for 230 (2:3102). Ds include all numbers but 2.3 and 2. Using the TD differences between features may explain why splitVec- tors was time consuming. For example, to compare 230 (2:3102) to 2,300 ( 2:3103), viewers have to differentiate the two lengths of 2 (T) and 3 (T) from other distractors (Ds other than 2 or 3). The heterogeneity of Ds or small DD distances from 3D lengths may make the use of splitVectors challenging, thus introducing temporal cost. Preattentive and Attentive Feature Ranking. Human visual processing can be faster when it is preattentive. Wolfe called a feature preattentive when it guides attention in search and cannot be decomposed into simpler features [7]. The idea of preattentive pop-out has historically highlighted that a single object has been considered compelling because it captures the user‚Äôs attention against a background of other objects (e.g., in showing spatial highlights [20]). Visual features such as orientation and color (hue, saturation, light- ness) can generate pop-out effects [21]. This type of pop- out was also used visualizations. For example, Ropinski, Oeltze, and Preim [22] summarized two groups of glyph design: ‚Äúparameter mapping‚Äù from shape and appearance (color, transparency, and texture) and ‚Äú placement ‚Äù driven by features or data. Our study concerns appearance. Recent vision science development also suggests that the preattentive feature is not limited to single items but expanded to high-level structures . Global statistical and structural features can be also preattentive [7]. Unlike the now outdated Treisman‚Äôs 1988 preattentive processing [23], where preattentive features were considered to be perceived before it is given focused attention [23], these preattentive features are persistent during viewers‚Äô data exploration thus can continue to provide guidance [7], [8]. Viewers can use peripheral vision to compare in parallel to conÔ¨Ådently tell apart regions relevant or irrelevant to tasks [24]. Visual features also can be responsible for different at- tention speeds, and color (hue) and size (length and spatial frequency) are among those that guide attention [9], [18]. Healey and Enns [25] in their comprehensive review further remark that these visual features are not popped-out at the same speed: hue has higher priority than shape and tex- ture [26]. Also, when data size increased, some preattentive features diminished [27] [28]. For visualizing quantitative data, MacKinlay [29] and Cleveland and McGill [30] leverage the ranking of visual features and suggest that position and size are quantitative and can be compared in 2D. For example, MacKinlay‚ÄôsA Presentation Tool (APT) [29] automatically recommends visualizations using effectiveness and expressive criteria and outputs a ranked set of encoding to enumerate candidate visualizations based on data types. Casner [31] expands MacKinlay‚Äôs APT by incorporating user tasks to guide visualization generation. McColeman et al. [32] revise the ranking of visual features based on the number of items. All these studies almost exclusively consider only single item mappings. Demiralp et al. [33] evaluate a crowdsourc- ing method to study subjective perceptual distances of 2D bivariate pairs of shape-color, shape-size, and size-color. When adopted in 3D glyph design, the authors further suggest that the most important data attributes should be displayed with the most salient visual features, to avoid sit- uations in which secondary data values mask the informa- tion the viewer wants to see. Ours also emphasizes the use of global scene features to optimize viewing experiences. Monotonicity. Quantitative data encoding must nor- mally be monotonic, and various researchers have recom- mended a coloring sequence that increases monotonically in luminance [34]. In addition, the visual system mostly uses luminance variation to determine shape information [35]. There has been much debate about the proper design of a color sequence for displaying quantitative data, mostly in 2D [36] and in 3D shape volume variations [37]. Our primary requirement is to use categorical colormaps that users be able to read large or small exponents at a glance. We used four color steps in the Ô¨Årst study and up to seven steps in the second study from ColorBrewer [36] for showing areas of large and small exponents that are mapped to a hue- varying sequence. We claim not that these color sequences are optimal, only that they are reasonable solutions to the design problem. 3 E XPERIMENT I: E FFECT OF SEPARABLE PAIRS ONLOCAL DISCRIMINATION AND COMPARISON The goal in this Ô¨Årst experiment is to quantify the beneÔ¨Åts of separable pairs with preattentive features for visual process- ing of a few items. This section discusses the experiment, the design knowledge we can gain from it, and the factors that inÔ¨Çuence our design. 3.1 Methods 3.1.1 Bivariate Feature-Pairs We chose Ô¨Åve bivariate feature-pairs to examine the com- parison task efÔ¨Åciency of separable-integral pairs. Lengthylengthx(integral ) (Figure 1a). Lengths encoded digits and exponents shown as the height and radius of cylinder glyphs. Lengthycolor=length x(redundant and separable ) (Fig- ure 1b). This pair compared to lengthylengthxadded a redundant color (luminance and hue variations) dimension to the exponent and the four sequential colors were chosen from Colorbrewer [36] (Appendix A shows the sequences.) Lengthycolor (separable ) (Figure 1c). This pair mapped exponents to color. Pilot testing showed that the least incor- rect exponent levels were selected among these Ô¨Åve feature- pairs.JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 5 Lengthytexture (separable ) (Figure 1d). Texture repre- sented exponents. The percentage of black color (Bertin [38]) was used to represent the exponential terms 0(0%),1(30% ), 2(60% ) and 3(90% ), wrapped around the cylinders in Ô¨Åve segments to make them visible from any viewpoint. Lengthylengthy(splitVectors [3],separable )(Figure 1e). This glyph used splitVectors [3] as the baseline and mapped both digit and exponent to lengths. The glyphs were semi- transparent so that the inner cylinders showing the digit terms were legible. Feather-like Ô¨Åshbone legends were added at each location when the visual variable length was used. The tick-mark band was depicted as subtle light-gray lines around each cylinder. Distances between neighboring lines show a unit length legible at certain distance (Figure 1, rows 1and2). 3.1.2 Hypotheses Given the analysis below and recommendations in the liter- ature, we arrived at the following working hypotheses: Exp I. H1. (Overall). The lengthycolor feature-pair can lead to the most accurate answers. Exp I. H2. (Integral-separable). Among the three separable dimensions,lengthycolor may lead to the greatest speed and accuracy and lengthytexture will be more effective than lengthylengthy(splitVectors). Exp I. H3. (Redundancy on time). The redundant pair lengthycolor=length xwill reduce task completion time compared to splitVectors . Several reasons led to H1 and H2. They are related to the two conditions of glyph design we evaluate. Color and length were separable dimensions, so comparing length to color is simple (condition 1). And color was preattentive and could be detected quickly (condition 2). Compared to the redundant lengthycolor=length x,lengthycolor reduced crowding since the feature-pairs were generally smaller than those in lengthycolor=length x. Also, distinguishing two lengths in splitVectors might be less efÔ¨Åcient than lengthytexture . H3 could be supported because redun- dancy increased information processing capacity [10]. Re- dundancy contributes to efÔ¨Åciency by increasing the feature distances between exponents. We did not expect accuracy gain from redundancy because splitVectors achieved the same level of accuracy as reading texts in Zhao et al. [3]. It may not be useful to decode quantitative data in this experiment at least for showing a few items. 3.1.3 Tasks Participants performed the following three task types as in Zhao et al. [3] so that results were comparable. They had unlimited time to perform these three tasks. Exp I. Task 1 (MAG): magnitude reading (Figure 3a) . What is the magnitude at point A? One vector was marked by a red triangle labeled ‚ÄúA‚Äù, and participants should report the magnitude of that vector. This task required precise numerical input. Exp I. Task 2 (RATIO): ratio estimation (Figure 3b) . What is the ratio of magnitudes of points A and B? Two vectors are marked with two red triangles labeled ‚ÄúA‚Äù and ‚ÄúB‚Äù, and participants should estimate the ratio of magnitudes of these two vectors. The ratio judgment is the most challenging (a) MAG task: What is the magnitude of the vector at point A? (answer: 636:30) (b) RATIO task: What is the ratio of the magnitude between the vectors at points A and B? (answer: 3:60) (c) COMP task: Which magnitude is larger, point A or point B? (answer: A on the right.) Fig. 3: Experiment I: Local discrimination and comparison tasks. These two red equilateral triangles are rendered on the screen coordinate and are thus always visible. quantitative task [29]. Participants could either compare the glyph shapes or decipher each vector magnitude and compute the ratio mentally. Exp I. Task 3 (COMP): comparison (Figure 3c) .Which magnitude is larger, point A or B? Two vectors are marked with red triangles and labeled ‚ÄúA‚Äù and ‚ÄúB‚Äù. Participants select their answer by directly clicking the ‚ÄúA‚Äù or ‚ÄúB‚Äù answer buttons. This task was a simple comparison between two values and offered a binary choice of large or small. 3.1.4 Data Selection Because we were also interested in comparing our results to those in Zhao et al. [3], we replicated their data selection method by randomly sampling some quantum physics sim- ulation results and produce samples within 3D boxes of sizeJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 6 TABLE 1: Experiment I design: 20participants are as- signed to one of the Ô¨Åve blocks and use all Ô¨Åve bivari- ate pairs. Here, LyLy:lengthylengthy(splitVectors ),LyLx: lengthylengthx,LC:lengthycolor ,LT:lengthytexture , andLCL :lengthycolor=length x. Block Participant Feature-pair 1 P1, P6, P11, P16 splitVectors ,LyLx,LC,LT,LCL 2 P2, P7, P12, P17 LyLx,LC,LT,LCL ,splitVectors 3 P3, P8, P13, P18 LC,LT,LCL ,splitVectors ,LyLx 4 P4, P9, P14, P19 LT,LCL ,splitVectors ,LyLx,LC 5 P5, P10, P15, P20 LCL ,splitVectors ,LyLx,LC,LT 533. There were 445to455sampling locations in each selected data region. We selected the data satisfying the same following con- ditions: (1) the answers must be at locations where some context information was available, i.e., not too close to the boundary of the testing data; (2) no data sample was repeated to the same participant; (3) since data must include a broad measurement, we selected the task-relevant data from each exponential term of 0to3. 3.1.5 Empirical Study Design Design and Order of Trials. We used a within-subject de- sign with one independent variable of bivariate quantitative feature-pair (Ô¨Åve types). Dependent variables were error and task completion time. We also collected participants‚Äô conÔ¨Ådence levels. Table 1 showed that participants were assigned into Ô¨Åve blocks in a Latin-square order, and within one block the order of the Ô¨Åve feature-pair types is the same. Participants performed tasks with randomly selected datasets. Each participant performed 60trials ( 3tasks4 random data5feature-pairs). These four random data were from four exponent ranges. Participants. We diversiÔ¨Åed the participant pool as much as possible, since all tasks could be carried out by those with only some science background. Twenty participants (15male and 5female, mean age = 23:3, and standard deviation = 4:02) participated in the study, with ten in com- puter science, three in engineering, two in chemistry, one in physics, one in linguistics, one in business administration, one double-major in computer science and math, and one double-major in biology and psychology. The Ô¨Åve females were placed in each of the Ô¨Åve blocks (Table 1). On average, participants spent about 40minutes on the tasks. Procedure. Participants were greeted and completed an Institutional Review Board (IRB) consent form (which described the procedure, risks and beneÔ¨Åts of the study) and the demographic survey. All participants had nor- mal or corrected-to-normal vision and passed the Ishihara color-blindness test. We showed feature-pair examples and trained the participants with one trial for every feature-pair per task. They were told to be as accurate and as quickly as possible, and that accuracy was more important than time. They could ask questions during the training but were told they could not do so during the formal study. Participants practiced until they fully understood the feature-pairs and tasks. After the formal study, participants Ô¨Ålled in a post- questionnaire asking how these feature pairs supportedtheir tasks and were interviewed for their comments. Pilot studies were conducted to examine the procedures. Environment. Participants sat at a 2700BenQ GTG XL 2720 Z, gamma-corrected display with resolution 1920 1080 to ensure the colors were displayed properly. The distance between the participants and the display was about 50cm. The minimum visual angle of task-associated glyphs was0:2in the default view where all data points were visible and the scene Ô¨Ålled the screen. Interaction. Participants could rotate the data and zoom in and out. Lighting placement and intensity were chosen to produce visualization with contrast and lighting properties appropriate for human assumptions and the spatial data. The screen background color was neutral stimulus-free gray background to minimize the discriminability and appear- ance of colors [10]. Using black or white background colors makes the black and white texture stimuli disappear thus bias the results (See Appendix B for examples). 3.2 Experiment I: Results and Discussion 3.2.1 Analysis Approaches We collected 400 data points for each task. In preparing the accuracy and task completion time for analysis, we differentiated two error metrics related to the perceptual accuracy of the bivariate pairs: Correspondence error (C-Error): A trial is considered to have an answer of C-Error if response‚Äôs exponent value does not match the correct one. Having a C-Error would mean that participants have trouble differentiating the exponent levels within a glyph. Relative error (R-Error): This R-Error follows Zhao et al. [3] to study how sensitive a method is to error uncertainty based on fractional uncertainty, calculated asR-Error =jcorrect answer - participant answer j/ (correct answer) . This measure was used for MAG and RATIO tasks. The beneÔ¨Åt of this metric was that it took into account the value of the quantity being compared and thus provided an accurate view of the overall errors. In subsequent analysis, we separated these two error measurements since Combining these two errors in the analysis would also be problematic. The C-Errors are at least one order of magnitude larger or smaller than the ground truth. We also did not remove participants‚Äô data with C-Errors, since the source of errors was caused by glyph design methods independent of trials. A post-hoc analysis using Tukey‚Äôs Studentized Range test (HSD) was performed when we observed a signiÔ¨Åcant main effect on R-Errors. When the dependent variable was binary (i.e., answer correct or wrong), we used a logistic regression and reported the pvalue from the Wald 2test. When thepvalue was less than 0.05, variable levels with 95% conÔ¨Ådence interval of odds ratios not overlapping were considered signiÔ¨Åcantly different. All error bars represent 95% conÔ¨Ådence intervals. We also evaluated effect sizes using eta-square , labeled ‚Äúsmall‚Äù (0:01 0:06), ‚Äúmedium‚Äù [0:06 0:14), and ‚Äúlarge‚Äù (0:14) effects following Co- hen [39].JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 7 (a) Task 1 (MAG)  (b) Task 2 (RATIO)  (c) Task 3 (COMP) Fig. 4: Experiment I task completion time and relative error or accuracy by tasks. The horizontal axis represents the mean task completion time while the vertical axis showing the accuracy or relative error. Same letters represent the same post-hoc analysis group. Colors label the feature-pair types. All error bars represent 95% conÔ¨Ådence interval. TABLE 2: Summary statistics by tasks. The signiÔ¨Åcant main effects and the high effect size (ES) are in bold (none in these observations) and the medium effect size is in italic . Effect size is eta-square labeled ‚Äúsmall‚Äù (0:01 0:06), ‚Äúmedium‚Äù [0:06 0:14), and ‚Äúlarge‚Äù (0:14) effects following Co- hen [39]. Post-hoc Tukey grouping results are reported for signiÔ¨Åcant main effects, where >means statistically signif- icantly better and enclosing parentheses mean they belong to the same Tukey group. Task Variables SigniÔ¨Åcance ES MAG time F(4;384) = 6.8, p<0.0001 0.07 (LC,LT,LCL ,splitVectors )>LyLx relative error F (4;384) = 0.9, p= 0.46 0.01 RATIO time F(4;395) = 6.2, p<0.0001 0.06 Three groups: A:LC,splitVectors ,LT B:splitVectors ,LT,LCL C:LT,LCL ,LyLx relative error F (4;395) = 0.8, p= 0.50 0.01 COMP time F(4;395) = 10.4, p<0.0001 0.09 Three groups: A:LCL ,LC,LT B:LC,splitVectors C:splitVectors ,LyLx accuracy 2= 0.4, p= 0.98 0.03 3.2.2 Overview of Study Results Figure 5 show all C-Error occurrences. Table 2 and Fig- ure 4 show the Fandpvalues computed with SAS one- way measures of variance for task completion time and relative error. Our results clearly demonstrated the beneÔ¨Åts in terms of task completion time of separable dimensions for comparison. We observed a signiÔ¨Åcant main effect of feature-pair type on task completion time for all three tasks MAG, RATIO, and COMP , and the effect sizes were in the medium range. Lengthycolor was the most efÔ¨Åcient approach. For COMP, lengthycolor ,lengthytexture and lengthycolor=length xwere most efÔ¨Åcient for simple two- point comparisons (Figure 4c). 3.2.3 Separable Dimension Coupled with Categorical Fea- tures had the Least Correspondence Errors. We only observed C-Errors in MAG, but not in the RATIO and COMP tasks. The total count was relatively small ( 11 instances of 400 data points). They came from 9partic- ipants (error mean = 1:22and95% conÔ¨Ådence intervals Fig. 5: Experiment I (Task MAG): All instances of cor- respondence errors by participant. The most separable lengthycolor glyph had no instances of correspondence er- ror whilst the lengthylengthxhad the most. The redundant color dimensions helped removed some correspondence errors (Two instances of lengthycolor=length xvs. Ô¨Åve in- stances oflengthylengthx). (CI)= [0:96;1:48]). Figure 5 shows all instances of these er- rors by participant and by encoding methods. It appeared that the degree of separability of integral-separable dimen- sions inÔ¨Çuenced the errors: the most integral dimension lengthylengthxhad the highest number ( 5instances) of C- Errors and the most separable lengthycolor had none. 3.2.4 Separable Dimensions Are Better Than Integral Di- mensions for Local Comparisons. But Categorical Feature was not a Statistically SigniÔ¨Åcant Effect. Our Ô¨Årst two hypotheses H1 and H2 are supported. In the MAG task, the integral lengthylengthxwas the least ef- Ô¨Åcient and all other separable-pairs were in a separate group, the most efÔ¨Åcient one (Figure 4a). In RATIO, lengthycolor ,lengthytexture , and splitVectors were the most efÔ¨Åcient group (Figure 4b); in COMP , the redundant lengthycolor=length x,lengthycolor , andlengthytexture were in the most efÔ¨Åcient group (Figure 4c). SplitVectors was not as bad as we originally thought in perceiving correct exponents. SplitVectors belonged to the same efÔ¨Åcient post-JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 8 hoc group as lengthycolor andlengthytexture for RATIO and these three were also most efÔ¨Åcient for MAG. 3.2.5 Separable Pairs of Lengthycolor And Lengthycolor=length xAchieved Comparable EfÔ¨Åciency To Direct Linear Glyph One aspect for motivating this experiment was to quantify the beneÔ¨Åts of separable pairs [6], [10]: whether the sepa- rable pairs supported COMP and how the separable pairs compared in efÔ¨Åciency to the direct mapping (Figure 2(f)). Since our study had the same numbers of sample data as Zhao et al. [3], we then performed a one-way t-test to compare against the direct linear encoding in Zhao et al. [3]. Our results indicated that results for COMP (judging large or small) from separable variables was no more time- consuming than direct linear glyphs, and our post-hoc anal- ysis showed that lengthycolor ,lengthycolor=length x, and linear were in the same post-hoc group. We also observed that splitVectors dropped to the least efÔ¨Åcient post-hoc group (Figure 4c). This result replicated the former study results in Zhao et al. [3] by showing that splitVectors impaired comparison efÔ¨Åciency. 3.2.6 Redundant Feature-Pairs Were EfÔ¨Åcient We also conÔ¨Årmed hypothesis H3. We were surprised by the large performance gain with the redundant encoding lengthycolor=length xof mapping color andlength to the exponents in splitVectors . With the redundant encoding, the task completion time was signiÔ¨Åcantly shorter than lengthylengthxfor MAG and COMP tasks. While Ware [10] conÔ¨Årmed that the efÔ¨Åciency might not be improved by using separable dimensions, in our case, where color and size (separable) represent the same quantitative value, we suggested that the redundancy worked because participants could use either length or color in different task conditions. We could also consider that lengthycolor=length xis a re- dundant encoding of lengthycolor , and those two feature- pairs had similar efÔ¨Åciency and accuracy for all local tasks. 3.3 Summary The separable-pair condition is necessary for effective glyph design because all separable pairs were more efÔ¨Åcient than the integral ones. The pre-attentive condition enabled by categorical encoding among the separable pairs may be not since not all conditions were statistically different performance-wise. All tasks (MAG, RATIO, and COMP) lacked of signiÔ¨Åcant main effect on relative errors (in MAG or RATIO) or accuracy (in COMP). Note that none of these three tasks required initial visual search, and target answers were labeled. Wolfe called this type of task-driven with known target guided tasks [8]. Lengthycolor was the most accurate in all tasks. We also did not see the needs for the second condition for perceptually accurate glyphs in this experiment. We did not observe differences among categorical dimensions color, texture, and length. We suspect that the reason for this lack of signiÔ¨Åcance could well be their similarities in mentally computing load. The load was relatively small when com- paring two values. We suspected that when search-spaceset-size increases, and when tasks are more complex in- volving all items, participants will need preattentive global scene features to guide their search. We subsequently ran the second experiment to increase the set size in tasks to the entire scene to study the beneÔ¨Åts of categorical features to show quantitative exponent values to beneÔ¨Åt global search. 4 E XPERIMENT II: S CALABILITY OF GLOBAL SCENE FEATURES The goal in this second experiment is to quantify the beneÔ¨Åts of separable feature-pairs when they introduce categorical features of scene guidance for global tasks in search spaces, as large as the entire dataset of several hundreds items. In other word, we measure scene feature scalability of global tasks. 4.1 Overview We had three design considerations for us to carefully choose the categorical features in setting up this experiment, concerning the use of glyphs for showing complex simula- tion results. Intriguingly, all of these considerations support our second glyph design consideration of using a categorical variable in one of the separable pairs. The Ô¨Årst reason is that the initial at-a-glance global statistical summary of the scene depends on categorical information [7]. One of the most important advances in vision science is to Ô¨Ånd that viewers can summarize the scene without attending to the speciÔ¨Åc items [40]. Visual dimensions facilitating this summary process become global scene features and these features are pre-attentive [8]. While visualization is mainly about mapping data values to visual variables, the new theory concerns how features form the structural and content of the scene that can affect efÔ¨Åciency. If the quantum spins contain one object at a time, then the Ô¨Årst condition of glyph design considering integral and separable dimensions is sufÔ¨Åcient to explain the experience as we have shown in Experiment I. For complex tasks, in general, our visual system has a limited capacity. To cope with this limit, humans Ô¨Årst visually summarize the scene to Ô¨Ånd speciÔ¨Åc regions of interests [6], [8]. If categorical fea- tures stimulate population responses from multiple items, we should observe fewer errors and better efÔ¨Åciency. For ex- ample, we have exempliÔ¨Åed in the Introduction section for search of ‚Äúlargest‚Äù values by looking up ‚Äúyellow‚Äù regions, without attending to every single items of ‚Äúyellow‚Äù. The second concerns scalability to feature distances . Here feature distance is meant to represent target-distractor simi- larity. It is not the absolute features (e.g., yellow) that direct our attention towards the answer; rather, what determines performance is the result of a comparison between target (yellow) and other data features (such as pink and orange) in the scene (e.g., yellow is different from other colors and the yellow regions stand out) [8]. In other words, one must also look at feature distractors [14], [41], [42], whether or not they are heterogeneous, and that the efÔ¨Åciency of a scene guidance will decline as a function of the degree of distractor variation [19], [24], [43]. While generally, subjec- tive reports from Experiment I indicate that lengthycolor andlengthytexture show the similar perceptual speed.JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 9 Fig. 6: Visual mapping using color and texture in Experi- ment II. From the top to bottom, colors and texture segments are mapped to exponent values from the largest to the smallest. The three numbers next to the 7-level colormap are the RGB values. The numbers next to the texture columns are the proportion of black-on-white for the last 7-level texture conÔ¨Åguration. Performance of texture may decline faster than color as the exponent range increases because our vision is not as sensitive to luminance-variation as to hues. For example, at the exponent-range of 7 in Figure 6, the differences between yellow and pink could be more differentiable than the two top-level textures of different amount of black. In this study, we expanded the data range from the single level in Experiment I to Ô¨Åve ranges 2[3;7]to understand feature- pair scalability to feature distances. The efÔ¨Åciency of color in Experiment I could well arise because the range (of 4) was not large enough. The third concerns the density effects on color choices. Figure 7 shows two densities and two colormaps (a cate- gorical colormap from Colorbrewer [36] and a segmented continuous colormap by the number of exponents generated from the extended blackbody colormap). For a feature to actually guide attention, we can see from Figure 7, the boundary detection with these colormaps is associated with data density. Unless the data density was reasonably high, detecting the boundaries using continuous colormaps (Fig- ures 7a, 7b) is harder than the ColorBrewer colormaps (Figures 7c, 7d). 4.2 Method 4.2.1 Feature-Pairs We used lengthycolor ,lengthytexture , and baseline splitVectors in Experiment II. These three visualizations were chosen because lengthycolor andlengthytexture are among the best feature-pairs from Experiment I and because color and texture are among the most separable features ac- cording to Ware [10]. To introduce a ‚Äúdistractor‚Äù experience to measure scalability to feature distances , we vary the data range from the 4levels in Experiment I to 3 7levels in Experiment II (See mapping in Figure 13, Appendix C.) 4.2.2 Hypotheses We had the following hypotheses: Exp II.H1 (Accuracy). More categorical feature in the sep- arable pairs will be more effective. We thus anticipate a rank order of effectiveness from high to low: lengthycolor , lengthytexture , and splitVectors.Exp II.H2 (Correspondence Errors). More categorical feature of color in the separable pairs will reduce C-Errors, when participants will choose the correct exponent level. Exp II.H3 (User behavior). More categorical dimension in the separable feature-pairs will lead to optimal users‚Äô behaviors: i.e., participants can quickly locate task-related regions for tasks that demand looking among many vectors due to global scene features. 4.2.3 Tasks Participants performed three tasks in which they had to compare all vectors to obtain an answer. Exp II. Task 1 (SEARCH): visual search. A vector search within 20seconds (Figure 8a). Find the vector with magnitude Xwithin 20seconds. The target vector was shown at the bottom-right corner of the screen. Participants were asked to Ô¨Ånd this vector. Exp II. Task 2 (MAX): Ô¨Ånd maximum. An extreme value search within 20seconds (Figure 8b). Within 20seconds, lo- cate the point of maximum magnitude when the exponent is X.X in the study was a number from 0to the maximum exponent (2[2;6]). This was a global task requiring participants to Ô¨Ånd the extremum among many vectors. Exp II. Task 3 (NUMEROSITY): estimate the total number of unique vector exponents (Figure 8c). Estimate the total number of unique vector exponents in the entire vector Ô¨Åeld within 2seconds. Data are randomly chosen and modiÔ¨Åed to produce the 3to7range. 4.2.4 Task Choices Tasks are use-inspired by real-world quantum physics data analyses. Experiment I drilled down to a single or at most two spins. But global tasks are also of quantum physicists‚Äô interests, such as those involving understanding the dis- tributions of quantum spin magnitudes. Practically, a spin represents charge density or the measure of the probability of an electron being present at an inÔ¨Ånitesimal element of space surrounding any given point. This probability varies due to electron traveling from one grid point to another and is often interpreted together with its neighbors. Quantum physicists are thus interested in searches for regions, where local regions are deÔ¨Åned by spin magnitude and different regions would correspond to changes in exponent. Often the most interesting regions are also those with speciÔ¨Åc charge densities (Task 1) or largest magnitudes (Task 2) . The regional task is related to learning the number of interesting regions or magnitude exponent clusters (Task 3). Performing tasks was limited to 20seconds as a pilot study showed that it took participants about 2[15;25] seconds or on average about 20seconds to Ô¨Ånish search tasks 1 and 2. Also, preattentive processing when used for scene guidance involving a group of similar objects are often fast for viewers to see and increasing the number of items should not signiÔ¨Åcantly impair the search time. From the practical side for the last experiment, participants who would want a perfect score could just spend time counting. Constraining the time allowed us to measure the accuracy when they may have to use the scene feature.JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 10 (a) Continuous colormap and high-density data  (b) Continuous colormap and low-density data (c) Categorical colormap and high-density data  (d) Categorical colormap and low-density data Fig. 7: Density effects on color choices to justify the use of dense sampling and categorical colormap (c) in Experiment II. This example dataset shows two colormaps : ( segmented-continuous (a and b) and categorical (c and d) colormaps), at two different data densities. (a) and (c) show data with the raw density from the simulation results; (b) and (d) were produced by removing around 70% vector glyphs. The boundaries between the data categories are more recognizable when the data are dense in (a) and (c) (comparing the 1st column and the 2nd column). At the same density (comparing the 1st and 2nd row), the boundaries between levels are easier to recognize when spin vectors are rendered using a categorical colormap of (c) and (d). We thus use the raw dense and categorical colormaps (c) in Experiment II. 4.2.5 Data Choices Data were Ô¨Årst sampled using the same approach as Exper- iment I, and no data is used repeatedly in this experiment. We then modiÔ¨Åed the exponent range from 3to7for the three tasks by normalizing the data to the desired new data range. Prior literature used both synthetic data and real-world data to construct the data visualization as test scenarios, en- abling tight control over the stimulus parameters (e.g., [44]). Most of the synthetic data in literature were to replicate real-world data characteristics; and others were explained in Ô¨Åctitious use scenarios. The goal was primarily to prevent preconceived user knowledge about the domain-speciÔ¨Åc attributes. As a result, the synthetic data strike the right bal- ance between real-world uses and the data characteristics. In our cases, replicating characteristics in quantum physics data was challenging and indeed impossible, since atom behaviors in high-dimensional space were largely unknown and thus were not easily simulated. Our approach was therefore to randomly sample quantum physics simu- lation results to capture domain-speciÔ¨Åc attributes and then modify the data to suit evaluation purposes. We showed our data to our physicist collaborators to ensure their va- lidity. We conÔ¨Årmed that these modiÔ¨Åcations preserved the domain-speciÔ¨Åc schema of a scene in terms of the domain- speciÔ¨Åc structures and complexity from real simulations.These modiÔ¨Åcations represented less than 4%of overall data points in each scene. Finally, It improves the reuse of our study results. 4.2.6 Empirical Study Design Dependent and Independent Variables. We used a within- subject design with two independent variables of feature- pair (three levels: baseline splitVectors ,lengthycolor , and lengthytexture ) and exponent range (Ô¨Åve levels: 3 7). The dependent variable was relative error. We did not measure time since all tasks were time-constrained. Participants performed 3(feature-pairs)5(magnitude- ranges)3(repetitions) = 45 trials for the Ô¨Årst two tasks. Three repetitions were used to give participants enough time to develop strategies. For NUMEROSITY tasks, the design runs 4repetitions, resulting in 3(feature-pairs) 5(exponent-ranges) 4(repetitions) = 60 trials. Each par- ticipant thus executed 45+45+60 = 150 trials. Completing all tasks took about 32minutes. Self-Reporting Strategies. Several human-computer inter- action (HCI) approaches can help observe users‚Äô behaviors. Answering questions can assist us to determine not just which technique is better but also the strategies humans adopt. For example, cognitive walkthrough (CTW) mea- sures whether or not the users‚Äô actions match the designers‚Äô pre-designed steps. Here we predicted that participantsJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 11 (a) SEARCH: Find the vector with magnitude X. (X:731, answer: the point marked by two yellow triangles.) (b) MAX: Which point has the maximum magnitude when the exponent is X? (X:1, answer: the point marked by two yellow triangles.) (c) NUMEROSITY (NUM): Estimate the total number of unique vector exponents of the entire vector Ô¨Åeld within 2 seconds. (answer: 7) Fig. 8: Experiment II three task types. The callouts show the task-relevant feature-pair(s). would use the global scene-features as guidance to accom- plish tasks. We interviewed participants and asked them to verbalize their visual observations in accomplishing tasks. 4.2.7 Participants Eighteen new participants ( 12male and 6female, mean age = 23:8, and standard deviation = 4:94) of diverse backgrounds participated in the study (seven in computer science, four in computer engineering, two in information systems, three in engineering, one in business school, and one in physics). Procedure, interaction, and environment were the same as those in the Experiment I.4.3 Experiment II: Results and Discussion We collected 810 data points per task for the Ô¨Årst two tasks of SEARCH and MAX and 1080 points for the third NUMEROSITY task. 4.3.1 Analysis Approaches For SEARCH and MAX tasks, we measured relative error (which was the percentage the reported value was away from the ground truth and the same as that of Experiment I) with SAS repeated measure. The last NUMEROSITY task used error rate which was the percentage of incorrect answers of all trials for each participant. We also used the same outlier removal methods to remove instances of correspondence errors for SEARCH and MAX. 4.3.2 Overview of Study Results Table 3 and Figure 10 show the summary statistics; And all error bars again represent 95% conÔ¨Ådence intervals. We observed a signiÔ¨Åcant main effect of feature-pair type on all three tasks. For the Ô¨Årst two tasks, the post-hoc analysis revealed that lengthycolor andlengthytexture were in the same group, the most efÔ¨Åcient one and that relative errors were statistically signiÔ¨Åcantly lower than those of thesplitVectors .Lengthycolor remained the most accurate pair for the NUMEROSITY tasks. Exponent-range was only a signiÔ¨Åcant main effect for NUMEROSITY, with power ranges 3and4were signiÔ¨Åcantly better than 5, which was better than 6and7. 4.3.3 More Categorical Features of Separable Dimensions Improved Accuracy We were interested to see if we could observe signiÔ¨Åcant main effects of categorical features in the separable pairs in this experiment. Here we did observe the signiÔ¨Åcant main effect and conÔ¨Årmed our Ô¨Årst hypothesis (H1) for both SEARCH and MAX: in the general trend, more separa- blelengthycolor was more effective than lengthytexture which was better than splitVectors , andlengthycolor and lengthytexture were in the same Tukey group, when view- ers were in the correct data sub-categories. Lengthycolor led to the most accurate answers, and splitVectors was better than lengthytexture for NUMEROS- ITY task. This result can be explained by participants‚Äô be- haviors - more than half the participants suggested they simply look for the longest cylinder from splitVectors since they know the numerical values in the test were continu- ous. This behavior deviated from our original purpose of testing the global estimate but did show two perspectives in favor of this work: (1) participants developed task-speciÔ¨Åc strategies during the experiment for efÔ¨Åciency; (2) 3D length still supported judging large and small and it was not as effective as color perhaps due to ensemble perception from categorical features. 4.3.4 Color Categories of Separable Pairs Reduced Corre- spondence Errors by a Large Margin Our second hypothesis H2 was also supported. We Ô¨Årst tested the number of correspondence errors in SEARCH and MAX in the same way as in Experiment I. These results when combined with those in Experiment I conÔ¨Årmed againJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 12 Fig. 9: Experiment II (Tasks SEARCH and MAX): All in- stances of correspondence errors by participant. Again, the lengthycolor has the least instances of correspondence error whilst thelengthytexture had the most. that thelengthycolor reduced correspondence errors. For SEARCH, There were only a single instance of correspon- dence error. 36instances of correspondence errors came from 14participants (mean = 2:57,95% CIs=[2:1;3:04]) (Figure 9 top). Another 59instances for MAX came from 16of18participants, mean = 3:68,95% CIs= [2:85;4:51]) (Figure 9 bottom ). 4.3.5 Compensating The Cost of Search in Complex Data through Preattentive Scene Feature The visualizations in our study contained hundreds of items from realistic uses. Subjective behaviors through self- report suggested that they adopted a sequential task-driven viewing strategy to Ô¨Årst obtain gross regional distribution of task-relevant exponents. After this, a visual comparison within the same exponent region were achieved. With these two steps, judging large or small or perceiving quantitiesTABLE 3: Exp II: Summary statistics by tasks. The signiÔ¨Åcant main effects and the high effect size are in bold and the medium effect size is in italic . Effect size is Cohen‚Äôs d for tasks SEARCH and MAX, and Cramer‚Äôs V for task NUMEROSITY (NUM). Post-hoc Tukey grouping results are reported for signiÔ¨Åcant main effects, where >means statistically signiÔ¨Åcantly better and enclosing parentheses mean they belong to the same Tukey group. Here, LC: lengthycolor andLT:lengthytexture . Task Variables SigniÔ¨Åcance ES SEARCH feature-pair F(2;261) = 18.4 ,p<0.0001 0.46 (LC,LT)>splitVectors power-range F (4;261) = 3.0, p = 0.20 0.86 MAX feature-pair F(2;261) = 15.4 ,p<0.0001 0.47 (LC,LT)>splitVectors power-range F (4;261) = 0.3, p= 0.87 0.11 NUM feature-pair 2= 63.2 ,p<0.0001 0.25 LC > splitVectors >LT power-range 2= 47.4 ,p<0.0001 0.35 (3, 4)>5>(6, 7) Fig. 10: Relative error for Tasks SEARCH and MAX was the percentage the reported value was away from the ground truth. Error rate for NUMEROSITY was the percentage of wrong answers of all trials for each participant. The vertical axis shows the relative error or error rate. Same letters represent the same post-hoc analysis group. All error bars represent 95% conÔ¨Ådence intervals.JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 13 accurately from separable variables would not use object- level information process. Many participants commented on how the number of powers in the data affected their effectiveness. For lengthytexture ,10participants remarked that it was dif- Ô¨Åcult to differentiate adjacent powers when the total power level is around 4-5forlengthytexture . The white and black textures were very easy to perceive. All but two participants agreed that lengthycolor could perhaps support up to 6. Chung et al. [42] studied ordering effects and it would be challenging to compare ours to their results because their visual features were not shown as a scene but an isolated feature. More than half of the participants felt that effec- tiveness of lengthylengthywas not affected by changing the number of powers, since they looked for the longest outer cylinder to help Ô¨Ånd the answer. These results may suggest that subregion selection with lengthytexture can perhaps be better designed with interfaces when the users can interactively select a texture level. 5 G ENERAL DISCUSSION We discuss the results from both experiments and suggest future directions. 5.1 Separable Dimensions with Preattentive Guidance for Large-Magnitude-Range Quantum Physics Spins Our Ô¨Årst principle in glyph design is to follow the conven- tion to use separable variable pairs [6], [10]. The results of Experiment I showed that separable dimensions could achieve the same efÔ¨Åciency as direct linear visualizations for COMP and was always more efÔ¨Åcient than integral pairs. For these local-tasks, we didn‚Äôt observe signiÔ¨Åcant error reduction. Our second principle in glyph design is to include cate- gorical features in separable pairs. The results from Exper- iment II studied the rank order of the separable pairs and found that they indeed improved accuracy for global tasks. Lengthytexture and splitVectors in both experiments led to more correspondence errors than lengthycolor . Achieving integrated numerical readings by combining two separable visual features at object level seems not necessary. The separable-dimension pairs of lengthycolor and lengthytexture worked because they were preattentive scene features. Our experiments show that viewers adopted a sequential task-driven viewing strategy based on a view hierarchy: viewers Ô¨Årst obtain global distributions of the scene. Then, a visual scrutiny is possible within a subregion. Although splitVectors are separable, visual search for length among length would be unguided because both targets and distractors contained the same visual variable. The more separable, the easier it would be to guide the attention. Using coloring to provide some initial regional division may be always better than not. Texture (luminance) could achieve similar accuracy and efÔ¨Åciency as long as the task-relevant regions could be detected. 5.2 Feature Guidance vs. Scene Guidance Taking into account both study results, we think an impor- tant part of the answer to visualization design is guidanceof attention. It is guided to some objects or locations over others by two broad methods: feature guidance (seeing objects) and scene guidance (seeing global structures). Feature guidance refers to guidance by properties of the task-target as well as the distractors (leading to correspon- dence errors). These features are limited to a relatively small subset of visual dimensions: color, size, texture, orientation, shape, blur or shininess and so on. These features have been broadly studied in 3D glyph design (see reviews by Healey and Enns [25], Borgo et al. [6], Lie et al. [46], Ropinski et al. [22], and McNabb and Laramee [28]). Take one more example from quantum physics simulation results, but with a different task of searching for the structural distributions in the power of 3in Figure 11 will guide attention to either the fat cylinders (Figure 11a) or the bright yellow color (Figure 11d, 11b) or the very dark texture (Figure 11c), depending on the feature-pair types. Working with quantum physicists, we have noticed that thestructure and content of the scene strongly constrain the possible location of meaningful structures, guided ‚Äúscene guidance‚Äù constraints [8], [47]. ScientiÔ¨Åc data are not ran- dom and are typically structured. Contextual and global structural inÔ¨Çuences can arise from different sources of visual information. If we return to the MAX search task in Figure 11 again, we will note that the chunk of darker or lighter texture patterns and colors on these regular contour structures strongly inÔ¨Çuence our quick detection. This is a structural and physical constraint that can be utilized effectively by viewers. This observation coupled with the empirical study results may suggest an interesting future work and hypothesis: adding scene structure guidance would speed up quantitative discrimination, improve the accuracy of comparison tasks, and reduce the perceived data complexity . Another structure acting as guidance is the size itself. It was used by participants seeking to resolve the NU- MEROSTIY tasks to look for the longest outside cylinders. We have showed several examples like Figure 11, our collaborator suggested that the cylinder-bases of the same size with the redundant encoding (Figure 11b) also helped locate and group glyphs belonging to the same magnitude. This observation agrees with the most recent literature that guidance-by-size in 3D must take advantage of knowledge of the layout of the scene [45]. Though feature guidance can be preattentive and fea- tures are detected within a fraction of a second, scene guidance is probably just about as fast (though precise experiments have not been done and our Experiment II only merely shows this effect). Scene ‚Äògist‚Äô can be extracted from complex images after very brief exposures [47] [48]. This doesn‚Äôt mean that a viewer instantly knows, say, where the answer is located. However, with a fraction of a second‚Äôs exposure, a viewer will know enough about the spatial lay- out of the scene to guide his or her attention towards vector groups in the regions of interest. For example, categorical color becomes scene features since these colorful glyphs were perceived as a whole A future direction, and also an approach to understand- ing the efÔ¨Åciency and the effectiveness of scene guidance, is to conduct an eye-tracking study to give viewers a Ô¨Çash- view of our spatial structures and then let the viewer see theJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 14 (a)Length ylength xfeature-pair  (b)Length ycolor=length xfeature-pair (c)Length ytexture feature-pair  (d)Length ycolor feature-pair Fig. 11: Contours of simulation data. Size from this viewpoint can guide visual grouping and size in 3D must take advantage of knowledge of the layout of the scene [45]. display only in a narrow range around the point of Ô¨Åxation: does this brief preview guide attention and the gaze effectively? Work in vision and visualization [49], [50], [51], [52] domain has measured and correlated performance on the glance or global structure formation. Vision science discovered long ago that seeing global scene structures in medical imaging decision making guides experts‚Äô attention (experts always know where to look) [53] [54].5.3 Redundancy and Ensemble Graphical Perception Our results showed that adding categorical colors, in which the correspondence parts could be quickly discriminated, is scalable to a large number of items. Our result agrees with that of Northelfer and Gleicher [55]. They observed that redundant encoding using color and shape could strengthen grouping when searching for targets from multiple objects. Their explanation was a race model [55]: for separable dimensions, the performance of a glyph with the redundant encoding might be dominated by the feature with greaterJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 15 efÔ¨Åciency. We did not Ô¨Ånd efÔ¨Åciency improvement - this suggested that the grouping is generally fast. So it might notbe the redundancy itself that contributed to scene un- derstanding. Another possible theory is perhaps ensemble perception , i.e., ‚Äúthe visual system‚Äôs ability to extract summary sta- tistical information from groups of similar objects - often in a brief glance‚Äù [40]. Also ensemble features are best represented using the categorical features. To model parallel processing, the target contrast signal theory by Buetti et al. [24] may suit our scenario better. It describes more speciÔ¨Åc time estimate it takes to evaluate items in parallel . In visualization, we just began to understand the ensemble averages (e.g., Chen [11] and Alberts et al. [56]) but have limited understanding of ensemble visual encoding choices to guide attention to optimize behaviors. We leave this to future work. 5.4 Use Our Results in Visualization Tools and Limita- tions of Our Work Visualization is used when the goal is to augment human capabilities in situations where the problems might not be sufÔ¨Åciently deÔ¨Åned for algorithms to communicate certain information. One of our showcase areas is quantum physics. We believe that the design principle of prompting the ad- dition of categorical features in bivariate glyphs would be broadly applicable to glyph design. Also, application do- mains carrying similar data attributes could reuse of work. Our current study concerns bivariate data visualization in which the bivariate variables are component parts of scalar variables. Our design could have been improved by following advanced tensor glyph design methods. Both generic [57] and domain-speciÔ¨Åc requirements for glyph designs [37] [58] [59] have led to the summary of glyph properties (e.g., invariant, uniqueness, continuity) to guide design and to render 2D and 3D tensors. A logic step is to truly un- derstand the quantum physics principles to combine data attributes and human perception to improve our domain- speciÔ¨Åc solutions. One limitation of this work is that we measure only a subset of tasks crucial to showing structures and omit- ted all tasks relevant to orientation. However, one may argue that the vectors naturally encode orientation. When orientation is considered, we could address the multiple- channel mappings in two ways. The Ô¨Årst solution is to use thelengthytexture to encode the quantitative glyphs and color to encode the orientation clusters. The second solution is to treat magnitude and orientation as two data facets and use multiple views to display them separately, with one view showing magnitude and the other for orientation (using Munzner‚Äôs multiform design recommendations [60]). The second limitation here was that our experiments were limited to a relatively small subset of visual dimensions: color, texture, and size. A future direction would be to try shapes and glyphs to produce novel and useful design. 6 C ONCLUSION Our Ô¨Åndings in general suggest that, as we hypothe- sized, distinguishable separable dimensions with preatten- tive categorical features perform better. The separable pairlengthycolor was the most efÔ¨Åcient and effective for both local and global tasks. The categorical features enable effec- tive complex scene inspections. Our empirical study results provide the following recommendations for designing 3D bivariate glyphs. . Highly separable pairs can be used for quantitative comparisons as long as these glyphs could guide at- tention (i.e., category forming). We recommend using lengthycolor . Texture-based glyphs ( lengthytexture ) that introduces luminance variation will only be recommended when task-relevant structures can be isolated. Integral and separable bivariate feature-pairs have the similar accuracy when the tasks are local. When the search tasks are more complex, introducing categorical features in the separable feature-pairs will lead to per- ceptually accurate glyphs. 3D glyph scene would shorten task completion time by combing two glyph design factors: separability and visual guidance from categorical features. The redundant encoding ( lengthycolor=length x) greatly improved on task completion time of integral dimensions ( lengthylengthx) by adding separable and preattentive color features. ACKNOWLEDGMENTS The work is supported in part by NSF IIS-1302755, NSF CNS-1531491, and NIST-70NANB13H181. The user study was funded by NSF grants with the OSU IRB approval number 2018B0080. Non-User Study design work was sup- ported by grant from NIST-70NANB13H181. The authors would like to thank Katrina Avery for her excellent editorial support and all participants for their time and contributions. Any opinions, Ô¨Åndings, and conclusions or recommen- dations expressed in this material are those of the author(s) and do not necessarily reÔ¨Çect the views of the National Science Foundation. Certain commercial products are iden- tiÔ¨Åed in this paper in order to specify the experimental procedure adequately. Such identiÔ¨Åcation is not intended to imply recommendation or endorsement by the National Institute of Standards and Technology, nor is it intended to imply that the products identiÔ¨Åed are necessarily the best available for the purpose. REFERENCES [1] J. Fuchs, P . Isenberg, A. Bezerianos, and D. Keim, ‚ÄúA systematic review of experimental studies on data glyphs,‚Äù IEEE Transactions on Visualization and Computer Graphics , vol. 23, no. 7, pp. 1863‚Äì1879, 2017. [Online]. Available: http://doi.org/10.1109/TVCG.2016.2549018 [2] C. Ware, ‚ÄúQuantitative texton sequences for legible bivariate maps,‚Äù IEEE Transactions on Visualization and Computer Graphics , vol. 15, no. 6, pp. 1523‚Äì1529, 2009. [Online]. Available: https://doi.org/10.1109/tvcg.2009.175 [3] H. Zhao, G. W. Bryant, W. GrifÔ¨Ån, J. E. Terrill, and J. Chen, ‚ÄúValidation of SplitVectors encoding for quantitative visualization of large-magnitude-range vector Ô¨Åelds,‚Äù IEEE Transactions on Visualization and Computer Graphics , vol. 23, no. 6, pp. 1691‚Äì 1705, 2017. [Online]. Available: https://doi.org/10.1109/tvcg. 2016.2539949 [4] D. J. Wineland, ‚ÄúNobel lecture: Superposition, entanglement, and raising schr ¬®odinger‚Äôs cat,‚Äù Reviews of Modern Physics , vol. 85, no. 3, p. 1103, 2013. [Online]. Available: https: //doi.org/10.1103/RevModPhys.85.1103JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 16 [5] A. Treisman and G. Gelade, ‚ÄúA feature-integration theory of attention,‚Äù Cognitive Psychology , vol. 12, no. 1, pp. 97‚Äì136, 1980. [Online]. Available: http://doi.org/10.1016/0010-0285(80)90005-5 [6] R. Borgo, J. Kehrer, D. H. Chung, E. Maguire, R. S. Laramee, H. Hauser, M. O. Ward, and M. Chen, ‚ÄúGlyph-based visualization: Foundations, design guidelines, techniques and applications,‚Äù Eurographics State of the Art Reports , pp. 39‚Äì63, 2013. [Online]. Available: http://diglib.eg.org/handle/10.2312/ conf.EG2013.stars.039-063 [7] J. M. Wolfe and I. S. Utochkin, ‚ÄúWhat is a preattentive feature?‚Äù Current Opinion in Psychology , vol. 29, pp. 19‚Äì26, 2019. [Online]. Available: https://doi.org/10.1016/j.copsyc.2018.11.005 [8] J. M. Wolfe, ‚ÄúGuided search 6.0: An updated model of visual search,‚Äù Psychonomic Bulletin & Review , pp. 1‚Äì33, 2021. [Online]. Available: https://doi.org/10.3758/s13423-020-01859-9 [9] D. Ariely, ‚ÄúSeeing sets: Representation by statistical properties,‚Äù Psychological Science , vol. 12, no. 2, pp. 157‚Äì162, 2001. [Online]. Available: https://doi.org/10.1111/1467-9280.00327 [10] C. Ware, Information Visualization: Perception for Design , 3rd ed. Elsevier, 2012. [Online]. Available: https://www.elsevier.com/ books/information-visualization/ware/978-0-12-381464-7 [11] Z. Chen, R. Zhuang, X. Wang, Y. Ren, and R. A. Abrams, ‚ÄúEnsemble perception without attention depends upon attentional control settings,‚Äù Attention, Perception, & Psychophysics , vol. 83, pp. 1240‚Äì1250, 2021. [Online]. Available: https://doi.org/10.3758/ s13414-020-02067-2 [12] T. Sekimoto and I. Motoyoshi, ‚ÄúEnsemble perception without phenomenal awareness of elements,‚Äù ScientiÔ¨Åc Reports , vol. 12, no. 1, pp. 1‚Äì8, 2022. [13] J. Maule, C. Witzel, and A. Franklin, ‚ÄúGetting the gist of multiple hues: metric and categorical effects on ensemble perception of hue,‚Äù J. Opt. Soc. Am. A , vol. 31, no. 4, pp. A93‚ÄìA102, Apr 2014. [Online]. Available: http://www.osapublishing.org/josaa/ abstract.cfm?URI=josaa-31-4-A93 [14] T. Urness, V . Interrante, I. Marusic, E. Longmire, and B. Ganapathisubramani, ‚ÄúEffectively visualizing multi-valued Ô¨Çow data using color and texture,‚Äù IEEE Visualization , pp. 115‚Äì 121, 2003. [Online]. Available: https://doi.org/10.1109/visual. 2003.1250362 [15] W. R. Garner and G. L. Felfoldy, ‚ÄúIntegrality of stimulus dimensions in various types of information processing,‚Äù Cognitive Psychology , vol. 1, no. 3, pp. 225‚Äì241, 1970. [Online]. Available: https://doi.org/10.1016/0010-0285(70)90016-2 [16] C. G. Healey and J. T. Enns, ‚ÄúLarge datasets at a glance: Combining textures and colors in scientiÔ¨Åc visualization,‚Äù IEEE Transactions on Visualization and Computer Graphics , vol. 5, no. 2, pp. 145‚Äì167, 1999. [Online]. Available: https://doi.org/10.1109/2945.773807 [17] C. G. Healey, K. S. Booth, and J. T. Enns, ‚ÄúVisualizing real-time multivariate data using preattentive processing,‚Äù ACM Transactions on Modeling and Computer Simulation , vol. 5, no. 3, pp. 190‚Äì221, 1995. [Online]. Available: http://doi.org/10.1145/ 217853.217855 [18] J. M. Wolfe and T. S. Horowitz, ‚ÄúWhat attributes guide the deployment of visual attention and how do they do it?‚Äù Nature Reviews Neuroscience , vol. 5, no. 6, pp. 1‚Äì7, 2004. [Online]. Available: http://doi.org/10.1038/nrn1411 [19] J. Duncan and G. W. Humphreys, ‚ÄúVisual search and stimulus similarity.‚Äù Psychological review , vol. 96, no. 3, p. 433, 1989. [Online]. Available: https://doi.org/10.1037/0033-295x.96.3.433 [20] H. Strobelt, D. Oelke, B. C. Kwon, T. Schreck, and H. PÔ¨Åster, ‚ÄúGuidelines for effective usage of text highlighting techniques,‚Äù IEEE Transactions on Visualization and Computer Graphics , vol. 22, no. 1, pp. 489‚Äì498, 2016. [Online]. Available: https: //doi.org/10.1109/TVCG.2015.2467759 [21] C. G. Healey, K. S. Booth, and J. T. Enns, ‚ÄúHigh-speed visual estimation using preattentive processing,‚Äù ACM Transactions on Computer-Human Interaction , vol. 3, no. 2, pp. 107‚Äì135, 1996. [Online]. Available: http://doi.org/10.1145/230562.230563 [22] T. Ropinski, S. Oeltze, and B. Preim, ‚ÄúSurvey of glyph-based visualization techniques for spatial multivariate medical data,‚Äù Computers & Graphics , vol. 35, no. 2, pp. 392‚Äì401, 2011. [Online]. Available: https://doi.org/10.1016/j.cag.2011.01.011 [23] A. Treisman and S. Gormican, ‚ÄúFeature analysis in early vision: evidence from search asymmetries,‚Äù Psychological Review , vol. 95, no. 1, pp. 15‚Äì48, 1988. [Online]. Available: https: //doi.org/10.1037/0033-295X.95.1.15[24] S. Buetti, J. Xu, and A. Lleras, ‚ÄúPredicting how color and shape combine in the human visual system to direct attention,‚Äù ScientiÔ¨Åc Reports , vol. 9, no. 1, pp. 1‚Äì11, 2019. [Online]. Available: https://doi.org/10.1038/s41598-019-56238-9 [25] C. G. Healey and J. T. Enns, ‚ÄúAttention and visual memory in visualization and computer graphics,‚Äù IEEE Transactions on Visualization and Computer Graphics , vol. 18, no. 7, pp. 1170‚Äì1188, 2012. [Online]. Available: https://doi.org/10.1109/tvcg.2011.127 [26] T. C. Callaghan, ‚ÄúInterference and dominance in texture segregation: Hue, geometric form, and line orientation,‚Äù Perception, & Psychophysics , vol. 46, no. 4, pp. 299‚Äì311, 1989. [Online]. Available: https://doi.org/10.3758/bf03204984 [27] J. Fuchs, P . Isenberg, A. Bezerianos, and D. Keim, ‚ÄúA systematic review of experimental studies on data glyphs,‚Äù IEEE Transactions on Visualization and Computer Graphics , vol. 23, no. 7, pp. 1863‚Äì1879, 2017. [Online]. Available: https://doi.org/10.1109/tvcg.2016.2549018 [28] L. McNabb and R. S. Laramee, ‚ÄúSurvey of surveys (SoS)-mapping the landscape of survey papers in information visualization,‚Äù Computer Graphics Forum , vol. 36, no. 3, pp. 589‚Äì617, 2017. [Online]. Available: https://doi.org/10.1111/cgf.13212 [29] J. Mackinlay, ‚ÄúAutomating the design of graphical presentations of relational information,‚Äù ACM Transactions on Graphics , vol. 5, no. 2, pp. 110‚Äì141, 1986. [Online]. Available: https: //doi.org/10.1145/22949.22950 [30] W. S. Cleveland and R. McGill, ‚ÄúGraphical perception: Theory, experimentation, and application to the development of graphical methods,‚Äù Journal of the American Statistical Association , vol. 79, no. 387, pp. 531‚Äì554, 1984. [Online]. Available: https://doi.org/10.2307/2288400 [31] S. M. Casner, ‚ÄúTask-analytic approach to the automated design of graphic presentations,‚Äù ACM Transactions on Graphics , vol. 10, no. 2, pp. 111‚Äì151, 1991. [Online]. Available: https: //doi.org/10.1145/108360.108361 [32] C. M. McColeman, F. Yang, T. F. Brady, and S. Franconeri, ‚ÄúRe- thinking the ranks of visual channels,‚Äù IEEE Transactions on Visu- alization and Computer Graphics , 2021. [33] C ¬∏ . Demiralp, M. S. Bernstein, and J. Heer, ‚ÄúLearning perceptual kernels for visualization design,‚Äù IEEE Transactions on Visualization and Computer Graphics , vol. 20, no. 12, pp. 1933‚Äì1942, 2014. [Online]. Available: https://doi.org/10.1109/tvcg.2014.2346978 [34] B. E. Rogowitz and A. D. Kalvin, ‚ÄúThe ‚ÄùWhich Blair Project‚Äù: A quick visual method for evaluating perceptual color maps,‚Äù IEEE Visualization , pp. 183‚Äì191, 2001. [Online]. Available: https://doi.org/10.1109/visual.2001.964510 [35] J. P . O‚ÄôShea, M. Agrawala, and M. S. Banks, ‚ÄúThe inÔ¨Çuence of shape cues on the perception of lighting direction,‚Äù Journal of Vision , vol. 10, no. 12, pp. 1‚Äì21, 2010. [Online]. Available: https://doi.org/10.1167/10.12.21 [36] M. Harrower and C. A. Brewer, ‚ÄúColorbrewer.org: An online tool for selecting colour schemes for maps,‚Äù The Cartographic Journal , vol. 40, no. 1, pp. 27‚Äì37, 2003. [Online]. Available: https://doi.org/10.1002/9780470979587.ch34 [37] C. Zhang, T. Schultz, K. Lawonn, E. Eisemann, and A. Vilanova, ‚ÄúGlyph-based comparative visualization for diffusion tensor Ô¨Åelds,‚Äù IEEE Transactions on Visualization and Computer Graphics , vol. 22, no. 1, pp. 797‚Äì806, 2016. [Online]. Available: https: //doi.org/10.1109/TVCG.2015.2467435 [38] J. Bertin, Semiology of Graphics: Diagrams, Networks, Maps . Univer- sity of Wisconsin Press, 1967. [39] J. Cohen, Statistical power analysis for the behavioral sciences . New York: Academic Press, 1988. [Online]. Available: https: //doi.org/10.4324/9780203771587 [40] D. Whitney and A. Yamanashi Leib, ‚ÄúEnsemble perception,‚Äù An- nual review of psychology , vol. 69, pp. 105‚Äì129, 2018. [Online]. Avail- able: https://doi.org/10.1146/annurev-psych-010416-044232 [41] D. Acevedo, J. Chen, and D. H. Laidlaw, ‚ÄúModeling perceptual dominance among visual cues in multilayered icon-based scientiÔ¨Åc visualizations,‚Äù IEEE Visualization Posters , 2007. [Online]. Available: https://vis.cs.brown.edu/docs/pdf/ Acevedo-2007-MPD.pdf [42] D. H. Chung, D. Archambault, R. Borgo, D. J. Edwards, R. S. Laramee, and M. Chen, ‚ÄúHow ordered is it? on the perceptual orderability of visual channels,‚Äù Computer Graphics Forum , vol. 35, no. 3, pp. 131‚Äì140, 2016. [Online]. Available: https://doi.org/10.1111/cgf.12889JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 17 [43] A. Lleras, Z. Wang, G. J. P . Ng, K. Ballew, J. Xu, and S. Buetti, ‚ÄúA target contrast signal theory of parallel processing in goal-directed search,‚Äù Attention, Perception, & Psychophysics , vol. 82, no. 2, pp. 394‚Äì425, 2020. [Online]. Available: https://doi.org/10.3758/s13414-019-01928-9 [44] A. Forsberg, J. Chen, and D. H. Laidlaw, ‚ÄúComparing 3D vector Ô¨Åeld visualization methods: A user study,‚Äù IEEE Transactions on Visualization and Computer Graphics , vol. 15, no. 6, pp. 1219‚Äì1226, 2009. [Online]. Available: https://doi.org/10.1109/TVCG.2009. 126 [45] M. P . Eckstein, K. Koehler, L. E. Welbourne, and E. Akbas, ‚ÄúHumans, but not deep neural networks, often miss giant targets in scenes,‚Äù Current Biology , vol. 27, 2017. [Online]. Available: https://doi.org/10.1016/j.cub.2017.07.068 [46] A. E. Lie, J. Kehrer, and H. Hauser, ‚ÄúCritical design and realization aspects of glyph-based 3D data visualization,‚Äù Proceedings of the Spring Conference on Computer Graphics , pp. 19‚Äì26, 2009. [Online]. Available: https://doi.org/10.1145/1980462.1980470 [47] I. Biederman, ‚ÄúOn processing information from a glance at a scene,‚Äù ACM SIGGRAPH Workshop on User-oriented Design of Interactive Graphics Systems , 1977. [Online]. Available: https://doi.org/10.1145/1024273.1024283 [48] A. Oliva, ‚ÄúGist of the scene,‚Äù Neurobiology of Attention , vol. 696, no. 64, pp. 251‚Äì258, 2005. [Online]. Available: https://doi.org/10.1016/B978-012375731-9/50045-8 [49] G. Ryan, A. Mosca, R. Chang, and E. Wu, ‚ÄúAt a glance: Pixel approximate entropy as a measure of line chart complexity,‚Äù IEEE Transactions on Visualization and Computer Graphics , vol. 25, no. 1, pp. 872‚Äì881, 2019. [Online]. Available: https://doi.org/10.1109/TVCG.2018.2865264 [50] Z. Bylinskii, P . Isola, C. Bainbridge, A. Torralba, and A. Oliva, ‚ÄúIntrinsic and extrinsic effects on image memorability,‚Äù Vision Research , vol. 116, pp. 165‚Äì178, 2015. [Online]. Available: https://doi.org/10.1016/j.visres.2015.03.005 [51] M. A. Borkin, A. A. Vo, Z. Bylinskii, P . Isola, S. Sunkavalli, A. Oliva, and H. PÔ¨Åster, ‚ÄúWhat makes a visualization memorable?‚Äù IEEE Transactions on Visualization and Computer Graphics , vol. 19, no. 12, pp. 2306‚Äì2315, 2013. [Online]. Available: https://doi.org/10.1109/tvcg.2013.234 [52] R. Li and J. Chen, ‚ÄúToward a deep understanding of what makes a scientiÔ¨Åc visualization memorable,‚Äù IEEE VIS and arXiv preprint , 2018. [Online]. Available: https://arxiv.org/abs/1808.00607 [53] H. L. Kundel, C. F. Nodine, E. F. Conant, and S. P . Weinstein, ‚ÄúHolistic component of image perception in mammogram interpretation: gaze-tracking study,‚Äù Radiology , vol. 242, no. 2, pp. 396‚Äì402, 2007. [Online]. Available: https://doi.org/10.1148/ radiol.2422051997 [54] T. Drew, M. L.-H. V Àúo, and J. M. Wolfe, ‚ÄúThe invisible gorilla strikes again: Sustained inattentional blindness in expert observers,‚Äù Psychological Science , vol. 24, no. 9, pp. 1848‚Äì1853, 2013. [Online]. Available: https://doi.org/10.1177/0956797613479386 [55] C. Nothelfer, M. Gleicher, and S. Franconeri, ‚ÄúRedundant encoding strengthens segmentation and grouping in visual displays of data,‚Äù Journal of Experimental Psychology: Human Perception and Performance , vol. 43, no. 9, p. 1667, 2017. [Online]. Available: https://doi.org/10.1037/xhp0000314 [56] D. Albers, M. Correll, and M. Gleicher, ‚ÄúTask-driven evaluation of aggregation in time series visualization,‚Äù in Proceedings of the SIGCHI Conference on Human Factors in Computing Systems , 2014, pp. 551‚Äì560. [Online]. Available: https://doi.org/10.1145/ 2556288.2557200 [57] T. Gerrits, C. R ¬®ossl, and H. Theisel, ‚ÄúGlyphs for general second- order 2D and 3D tensors,‚Äù IEEE Transactions on Visualization and Computer Graphics , vol. 23, no. 1, pp. 980‚Äì989, 2017. [Online]. Available: https://doi.org/10.1109/TVCG.2016.2598998 [58] H.-J. Schulz, T. Nocke, M. Heitzler, and H. Schumann, ‚ÄúA design space of visualization tasks,‚Äù IEEE Transactions on Visualization and Computer Graphics , vol. 19, no. 12, pp. 2366‚Äì2375, 2013. [Online]. Available: http://doi.org/10.1109/TVCG.2013.120 [59] G. Kindlmann and C.-F. Westin, ‚ÄúDiffusion tensor visualization with glyph packing,‚Äù IEEE Transactions on Visualization and Computer Graphics , vol. 12, no. 5, pp. 1329‚Äì1336, 2006. [Online]. Available: https://doi.org/10.1109/tvcg.2006.134 [60] T. Munzner, Visualization Analysis and Design . A K Peters Visualization Series. CRC Press, 2014. [Online]. Available: https://doi.org/10.1201/b17511 Henan Zhao was a PhD student in Department of Computer Science and Electrical Engineer- ing at University of Maryland, Baltimore County. She received B.E. degree in Computer Science and Information Security from Nankai University, China. Her research interests include design and evaluation of perceptually accurate visualization techniques. This work was conducted while she was visiting The Ohio State University. Garnett Bryant received his PhD at Indiana Uni- versity in theoretical condensed matter physics. After research positions at Washington State University, the National Bureau of Standards, McDonnell Research Labs and the Army Re- search Laboratory, he has worked at the Na- tional Institute of Standards and Technology (NIST) since 1994. He is directing the Quan- tum Processes and Metrology Group at NIST with experimental and theoretical programs on nanoscale, condensed matter systems for quan- tum information science and metrology. He is a Fellow of the Joint Quan- tum Institute of NIST/University of Maryland, a Fellow of the American Physical Society and a member of the IEEE. His theoretical research program focuses on nanosystems, nanooptics and quantum science. Wesley GrifÔ¨Ån received his PhD degree in Com- puter Science from the University of Maryland, Baltimore County. He is a developer at Stellar Science. His research interests include real-time graphics and graphics hardware. He is a mem- ber of ACM SIGGRAPH, the IEEE and the IEEE Computer Society. Judith E. Terrill is a Computer Scientist and the Leader of the High Performance Computing and Visualization Group at the National Institute of Standards and Technology. She is a member of the IEEE Computer Society, the Association for Computing Machinery, and the Association for the Advancement of ArtiÔ¨Åcial Intelligence. Jian Chen is an Associate Professor in Com- puter Science and Engineering at The Ohio State University. She received her PhD degree in Computer Science from Virginia Tech, and her MS degree in Mechanical Engineering jPre- cision Instrument from Tianjin University jTs- inghua University, China. She was a postdoc- toral fellow at Brown University and a visiting researcher at Harvard University. Her current research interests include visual design, 3D in- teraction, and human-AI teaming.JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 18 Evaluating Glyph Design for Showing Large-Magnitude-Range Quantum Spins Additional Material Empirical study training documents, source code, study data, and results are online at https :==osf:io= 4xcf5=?viewonly= 94123139df9c4ac984a1e0df811cd580. A. B ACKGROUND COLOR Fig. 12 shows an example represented by lengthytexture with gray, white, and black background colors. Gray background color was selected for the experiments. We could observe that both white and black cylinders with lengthytexture encoding could be displayed more clearly in the gray background (Fig. 12, left). B. V ISUAL MAPPING FOR COLOR AND TEXTURE IN THE Lengthycolor ANDLengthytexture PAIRS Fig. 6 shows the visual mapping using color and texture in Experiment II. The horizontal axis represents the exponent range2[3;7]. We selected those categorical colors from ColorBrewer [36]. For texture, the percentage of black is mapped to the exponent-range. Examples with three different exponent-ranges of 3, 5, and 7 are shown in Fig. 13, in which color and texture are used for the visual mapping of study data. C. V ISUAL FEATURES AND EXPONENT -RANGE Fig. 13 shows examples for visual features and three exponent-ranges of 3,5, and 7. The Ô¨Ågures with the same exponent- range were generated using the same data and different visual features. The dataset used in this Ô¨Ågure is for illustration purpose only and does not necessarily reÔ¨Çect all image features used in the vector magnitude experiments. Fig. 12: Examples using different background colors: gray, white, and black. Figures on the top row are magniÔ¨Åed views of region 1, marked by orange-box on the left image, and the bottom row shows region 2. With white background, the white cylinders would be washed out (top right image). With black background, the black cylinders would be washed out (bottom right image). In this study, the neutral stimulus-free gray background was chosen.JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 19 (a)Length ylength y(splitVectors ) (b)Length ycolor (c)Length ytexture Fig. 13: Experiment II: examples of selected exponent ranges of 3,5, and 7(from the second left to right). We could see that the pattern of magnitude distribution is more revealing by categorical colors than by texture glyphs. Coloring may show more steps with large exponent ranges and also give us a better understanding of data distribution. For example, we could quickly focus on the orange region.JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 20 D. S PATIAL PROXIMITY Figures 14 and 15 show spatial distributions of the identiÔ¨Åed targets (participants‚Äô answers) to the correct targets in the search and max tasks in Experiment II. Here locations of the correct targets are translated to the origin (0, 0, 0). Participants‚Äô answers are depicted in green and each dot represents a trial. Dots may overlap. Dots in orange illustrate some of the nearest spins whose exponent values differ from the target (located at the origin). Comparing the distribution of participants‚Äô answers and the orange dot locations illustrates one of the key quantum physics data attributes: quantum physics data are discrete; and spatial proximity is not correlated with the spin magnitude proximity. For complex data like this, using the structural features (e.g., from color) in search will help them be more efÔ¨Åcient and reduce errors. (a)length ycolor  (b)length ytexture  (c)length ylength y Fig. 14: Experiment II: Search task. The spatial proximity of the locations of the identiÔ¨Åed targets, to the ground truth, for all trials in the study. Here the ground truth locations are translated to the origin (0, 0, 0). This task was time- constrained. among the 810 trials (or 270 trials for each bivariate glyph type), participants completed 262 lengthycolor , 261 lengthytexture , and 251lengthylengthytrials. (a)length ycolor  (b)length ytexture  (c)length ylength y Fig. 15: Experiment II: Max task. The spatial proximity of the locations of the identiÔ¨Åed targets, to the ground truth (centered at the origin (0, 0, 0), for all trials in this task. The yellow dots show the closest points from other-than-target-exponent regions. Here the ground truth locations are translated to the origin (0, 0, 0). Among the 810 trials, participants gave an answer to 270 trials for each bivariate glyph type. Among each of these 270, participants completed 269 lengthycolor , 269 lengthytexture , and 259lengthylengthytrials in total."
3,https://arxiv.org,2301.00003,10.48550/arXiv.2301.00003,Junya Morita,Emotion in Cognitive Architecture: Emergent Properties from Interactions with Human Emotion,"This document presents endeavors to represent emotion in a computational cognitive architecture. The first part introduces research organizing with two axes of emotional affect: pleasantness and arousal. Following this basic of emotional components, the document discusses an aspect of emergent properties of emotion, showing interaction studies with human users. With these past author's studies, the document concludes that the advantage of the cognitive human-agent interaction approach is in representing human internal states and processes.","Emotion in Cognitive Architecture: Emergent Properties from Interactions with Human Emotion Junya Morita j-morita@inf.shizuoka.ac.jp Shizuoka University Hamamatsu, Shizuoka, Japan ABSTRACT This document presents endeavors to represent emotion in a com- putational cognitive architecture. The first part introduces research organizing with two axes of emotional affect: pleasantness and arousal. Following this basic of emotional components, the docu- ment discusses an aspect of emergent properties of emotion, show- ing interaction studies with human users. With these past author‚Äôs studies, the document concludes that the advantage of the cogni- tive human-agent interaction approach is in representing human internal states and processes. CCS CONCEPTS ‚Ä¢Human-centered computing ‚ÜíHCI theory, concepts and mod- els. KEYWORDS cognitive architecture, emotion, arousal, valence 1 INTRODUCTION This document begins by asking the following question: How can emotion emerge in a computational system? This is a kind of ultimate question that has attracted enormous numbers of scientists and engineers, including the author himself. Toward the complete answer to the question, the author has devel- oped several models of mental functions (possible components of emotion process) in ACT-R (Adaptive Control of Thought-Rational [1]), which is one of the most widely used cognitive architectures in the world. Cognitive architectures generally integrate knowledge concern- ing the human mind in the form of computer programs. The knowl- edge accumulated in ACT-R ranges from perceptual-motor com- ponents to abstract and goal-related concepts. Varieties of mental functions are controlled by symbols stored in modules (correspond- ing brain regions) and subsymbolic parameters (corresponding neu- rotransmitters). By utilizing these, this architecture aims to realize human-level activities in every field of human life. The author considers that the above characteristic of the archi- tecture is crucial to answering the question. Thus, this document Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). CHAI ‚Äô22, December 5, 2022, Christchurch, New Zealand ¬©2022 Copyright held by the owner/author(s).presents the author‚Äôs works utilizing ACT-R as ingredients of dis- cussions on the conditions for enabling emotion in a computational system. 2 ISSUES OF EMOTIONAL PROCESS As the background of the discussion, this section presents the au- thor‚Äôs view on the emotion process. It is assumed that the emotion process relies on subcortical brain regions, such as the amygdala, insula, and basal ganglia. The process was considered to be the product of an adaptation to ancestors‚Äô surrounding environments, because these regions were formed early in the evolution of the brain [ 4,9]. However, modern human emotion is more complex than the purely physical processes in the following senses: (1)Emotion is not a static entity but rather emergent properties accompanied by dynamic interaction with the environment. This means a human‚Äôs emotional response always fluctuates. (2)Our environment has drastically changed from the environ- ment in which our ancestors lived. Therefore, many emo- tional mechanisms have become maladaptive in the modern age. (3)This biological process is somehow modulated by an inten- tional strategy, as many theories of emotion have suggested [3,4]. Therefore, there is room for intervention in the emo- tion process by our will or technology. 3 REPRESENTING EMOTION IN ACT-R Even through such complexities, it is possible to model the basis of emotion (affect) using the fundamental axes, arousal and pleasant- ness, as presented in Russel [ 16]‚Äôs circumplex model (Fig 1). The present author‚Äôs approach begins with these simple components. The following studies represent these components with primitive cognitive functions, such as activation noise and pattern matching, implemented in ACT-R. 3.1 Representation of Emotional Arousal According to a dictionary in psychology [ 17], arousal is defined as follows: a state of physiological activation or cortical respon- siveness, associated with sensory stimulation and ac- tivation of fibers from the reticular activating system. As indicated by this definition, past researchers have frequently con- nected arousal with the activation (attentional) process, especially the degree of concentration (e.g., [ 8]). In a concentrated situation, humans can continue monotonous tasks accurately. However, as such a process long lasts long, people get bored and begin to think about things outside of the task (i.e., mind-wandering).arXiv:2301.00003v1  [cs.HC]  28 Dec 2022CHAI ‚Äô22, December 5, 2022, Christchurch, New Zealand Morita High arousal Low arousalUnpleasantRewardsFluctuation Figure 1: Axes of emotional affect and model parameters. From this phenomenon and the consideration that emotional arousal relates to biological fluctuations affecting the cognitive process, the author and their colleagues have represented arousal as the noise factor for memory activation in ACT-R. Controlling this subsymbolic parameter, the authors have demonstrated changes in memory recollection [ 10] and task goal switching [ 14]. This view is consistent with the discussion in which emotion is a modulator of cognitive architecture [5, 15]. 3.2 Representation of Pleasantness People feel joy when receiving a reward. Thus, the pleasantness axis can be considered as a reward in reinforcement learning. Various triggering events for rewards can be assumed in the real world. Among them, internally generated ones are important to developing autonomous agents. Based on the above assumptions, Nagashima et al. [ 12,13] devel- oped a model of intrinsic motivation, assuming a pattern discovery to be a source of curiosity. In the ACT-R model, patterns in the data are discovered with the process of pattern matching, and the experience of the pattern matching is utilized to build procedural- ized rules (the compilation of production rules). As a task proceeds, opportunities for pattern matching (internal rewards) gradually decrease. Thus, the model can explain how intrinsic motivation decreases with experience and increases with discovering novel patterns. This pattern-discovery-focused view of intrinsic motiva- tion is consistent with discussions in the entertainment industry [7]. In addition, it is supported by the theory emphasizing the role of pattern-seeking in the history of human civilization [2]. 4 NEEDS OF INTERACTION From the discussion so far, the importance of environment for emotion stands out. Capturing the full complexities of the real- world dynamics of the two mechanisms (arousal and pleasantness) requires environmental changes. Among various environmental factors, the existence of other organisms seems most crucial. In other words, the author considers that human emotion can be modeled only when the computational system actually interacts Figure 2: Framework of interacting human emotion with machine emotion [11] with humans. By interacting with humans, the system is able to learn human‚Äôs emotion generation and expression. Based on this consideration, the author and colleagues have developed several interactive systems [ 6,11], implementing an emotion model as a component. Those systems receive the user‚Äôs biological signals such as heart rates to automatically modulate the abovementioned ACT-R parameters (noises and rewards) for guiding the user‚Äôs emotion to an optimal state. Especially, Morita et al. [11] demonstrated that a web advertisement system containing an ACT-R memory model could prevent human repetitive thinking (rumination) when the model behaves in a counterbalanced manner (Fig 2). The author believes that such a system will eventually lead to a new human homeostatic process with the help of artificial emotional systems. 5 CONCLUSION This document presents the author‚Äôs attempts to answer the ul- timate question about the computational conditions that enable emotion. Future work must represent the ideas presented in the document as a general framework and evaluate it in human exper- iments. The author considers that such a cognitive-architecture- based framework is advantageous in constructing trustful human- agent relations. Academic knowledge implemented in architecture is the result of continuous endeavors in human history. Including the formal knowledge agreed in human society is an essential in- gredient of making common ground between humans and artifacts. ACKNOWLEDGEMENT This document summarizes ideas obtained from past collaborative studies with colleagues at Nagoya University, Shizuoka University, collaborators from Panasonic Corp. and Mazda Corp., and mem- bers of the Applied Cognitive Modeling Lab (ACML) at Shizuoka university. The author thanks everyone for valuable discussions.Emotion in Cognitive Architecture: Emergent Properties from Interactions with Human Emotion CHAI ‚Äô22, December 5, 2022, Christchurch, New Zealand REFERENCES [1]John R Anderson. 2007. How Can the Human Mind Occur in the Physical Universe . Oxford University Press, New York. [2]Simon Baron-Cohen. 2020. The pattern seekers: How autism drives human invention . Basic Books. [3]Lisa Feldman Barrett. 2017. How emotions are made: The secret life of the brain . Pan Macmillan. [4] Antonio Damasio. 1994. Descartes‚Äô Error . Random House, New York. [5]Christopher L Dancy, Frank E Ritter, Keith A Berry, and Laura C Klein. 2015. Using a cognitive architecture with a physiological substrate to represent ef- fects of a psychological stressor on cognition. Computational and Mathematical Organization Theory 21, 1 (2015), 90‚Äì114. [6]K. Itabashi, J. Morita, T. Hirayama, K. Mase, and Yamada K. 2020. Interactive Model-based Reminiscence Using a Cognitive Model and Physiological Indices. InProceedings of the 18th International Conference on Cognitive Modeling . [7]Raph Koster. 2013. Theory of Fun for Game Design . O‚ÄôReilly Media, Sebastopol. [8]D. M. Landers. 1980. The arousal-performance relationship revisited. Research Quarterly for Exercise and Sport 51, 1 (1980), 77‚Äì90. [9]Joseph LeDoux. 2020. The deep history of ourselves: The four-billion-year story of how we got conscious brains . Penguin. [10] Junya Morita, Takatsugu Hirayama, Kenji Mase, and Kazunori Yamada. 2016. Model-Based Reminiscence: Guiding Mental Time Travel by Cognitive Modeling. InProceedings of the Fourth International Conference on Human Agent Interaction(Biopolis, Singapore) (HAI ‚Äô16) . 341‚Äì344. [11] Junya Morita, Thanakit Pitakchokchai, Giri Basanta Raj, Yusuke Yamamoto, Hiroyasu Yuhashi, and Teppei Koguchi. 2022. Regulating Ruminative Web Brows- ing Based on the Counterbalance Modeling Approach. Frontiers in Artificial Intelligence 5 (2022). [12] Kazuma Nagashima, Junya Morita, and Yugo Takeuchi. 2020. Modeling intrin- sic motivation in ACT-R : Focusing on the relation between pattern matching and intellectual curiosity. In Proceedings of the 18th International Conference on Cognitive Modelling . Applied Cognitive Science Lab, 167‚Äì173. [13] Kazuma Nagashima, Junya Morita, and Yugo Takeuchi. 2021. Curiosity as pattern matching: Simulating the effects of intrinsic rewards on the levels of processing. InProceedings of the 19th International Conference on Cognitive Modelling . 197‚Äì 203. [14] K Nagashima, J Nishikawa, R Yoneda, J Morita, and T Terada. 2022. Model- ing optimal arousal by integrating basic cognitive components. In International Conference on Cognitive Modeling 2022 . [15] F. E. Ritter. 2009. Two cognitive modeling frontiers Emotions and usability. Transactions of the Japanese Society for Artificial Intelligence 24, 2 (2009), 241‚Äì 249. [16] James A Russell. 2003. Core affect and the psychological construction of emotion. Psychological Review 110, 1 (2003), 145. [17] Gary R VandenBos. 2007. APA dictionary of psychology. American Psychological Association."
4,https://arxiv.org,2301.00004,10.1186/s13321-023-00688-x,"Mingchen Li, Liqi Kang, Yi Xiong, Yu Guang Wang, Guisheng Fan, Pan Tan, Liang Hong",SESNet: sequence-structure feature-integrated deep learning method for data-efficient protein engineering,"Deep learning has been widely used for protein engineering. However, it is limited by the lack of sufficient experimental data to train an accurate model for predicting the functional fitness of high-order mutants. Here, we develop SESNet, a supervised deep-learning model to predict the fitness for protein mutants by leveraging both sequence and structure information, and exploiting attention mechanism. Our model integrates local evolutionary context from homologous sequences, the global evolutionary context encoding rich semantic from the universal protein sequence space and the structure information accounting for the microenvironment around each residue in a protein. We show that SESNet outperforms state-of-the-art models for predicting the sequence-function relationship on 26 deep mutational scanning datasets. More importantly, we propose a data augmentation strategy by leveraging the data from unsupervised models to pre-train our model. After that, our model can achieve strikingly high accuracy in prediction of the fitness of protein mutants, especially for the higher order variants (> 4 mutation sites), when finetuned by using only a small number of experimental mutation data (<50). The strategy proposed is of great practical value as the required experimental effort, i.e., producing a few tens of experimental mutation data on a given protein, is generally affordable by an ordinary biochemical group and can be applied on almost any protein.","             SESNet : sequence -structure  feature -integrated deep  learning method for data -efficient protein  engineering   Mingchen Li1,4‚Ä†, Liqi Kang1,2‚Ä†, Yi Xiong5, Yu Guang Wang1, Guisheng  Fan4, Pan  Tan1*, Liang Hong1,2,3*  1. Shanghai National Center for Applied Mathematics (SJTU Center), & Institute of  Natural Sciences, Shanghai Jiao Tong University, Shanghai 200240, China   2. School of Physics and Astronomy & School of Pharmacy, Shanghai Jiao Tong  Univers ity,200240, China   3. Shanghai Artificial Intelligence Laboratory, Shanghai 200240, China   4. School of Information Science and Engineering, East China University of Science and  Technology, Shanghai 200240, China   5. School of Life Sciences and Biotechnology, Shanghai  Jiao Tong University, Shanghai 200240,  China   Abstract   Deep  learning has been wide ly used for protein engineering. However, it  is limited by  the lack of sufficient experimental data to train an accurate model for predicting the  functional fitness of high -order mutants. Here, we  develop  SESNet, a supervised deep - learning model  to predict the fitness for protein mutants  by leveraging both sequence   and structure information , and exploiting attention mechanism . Our model  integrates  local evolutionary context from homologous sequences , the global evolutionary context  encod ing rich semantic from the universal protein sequence  space and the structure   information accounting for the  microenvironment around each residue in a protein . We  show that SESNet  outperforms state-of-the-art models for predict ing the sequence - function relationship on 26 deep mutational scanning  datasets . More importantly , we  propose a data augmentation strategy by leveraging the data from unsupervised models  to pre -train our model.  After that, our model can achieve strikingly high accuracy in  prediction of the fitness of protein mutants, especially for the high er order variants (>  4 mutation sites) , when finetuned by using only  a small number of experimental  mutation data (<50) . The strategy proposed is of great practical value as the required  experimental effort, i.e., producing a few tens of experimental mutation data on a given  protein, is generally affordable by an ordinary biochemical group and can be applied  on almost any protei n.    Introduction   Proteins are workhorses of the life activit ies. Their various function s such as   catalysis, binding, and transport ation  undertake  most of the metabolic activities in cells.  In addition,  they are the key components of the cytoskeleton , support ing the stable and  diverse form of organisms . Nature provides numerous proteins with great potential             value  for practical  applications.  However, the natural proteins often do not have the  optimal function to meet the demand of bioengineering . Directed evolution is a widely  used experimental method  to optimize  proteins ‚Äô function ality, namely fitness , by  employing a greedy local search  to optimize protein fitness [1, 2] . During  this process,   gain-of-function  mutants  are achieved and optimized via mutating several  Amino Acid s  (AA) in the protein , which  were selected  and accumulated through the iterative  process es of mutation  by testing hundreds to thousands of variants  in each generation .  Despite the  great success  directed evolution has achieved , the phase  space  of the protein  fitness landscape can be screened by this method is rather limited . Furthermore , to  acquire  a mutant of excellent fitness, especially a high-order  mutant with multiple  AA  being mutated, the directed evolution often needs to develop an effective high - throughput screening  or conduct  a large number of experimental tests , which is   experimentally  and economically  challenging [3].  Since experimental  screening  for directed evolution  is largely costing , particularly  for high -order mutations , prediction of  the fitness of  protein variants in silico are highly  desirable . Recently,  deep learning methods have been applied for predicting the fitness  landscape of the protein variants [2]. By building models trained to learn  the sequence - function relationship , deep  learning can predict the fitness of each mutant in the whole  sequence space and give a list of the most favorable candidate  mutants  for experimental  tests. Generally, th ese deep  learning models  can be classified into  protein language  models [4-11], learn ing the representations from the global unlabeled sequences [6, 7,  12] and multiple  sequence  alignment  (MSA ) based m odel, capturing the feature  of  evolutional information within the family of the protein targeted [13-16]. And more  recent work s have proposed to combine these  two strategies : learning  on evolutionary   information together  with global natural  sequences  as the  representation [17, 18] , and  trained the model on the label led experimental  data of screened variants to predict the  fitness of all possible sequences.  Nevertheless, all these models are focused on protein  sequence, i.e., using protein sequence as the input of the model . Apart from sequence  information, protein structure can provide additional information  on function.  Due to  the experimental challenge of  determining the  protein structure, the number  of reported  protein structures is orders of magnitude smaller than that of known protein sequences ,  which hinders the development of geometric deep learning model to leverage protein  structural feature. Thanks  to the dramatic  breakthrough in deep learning -based  technique for predicting protein structure [19, 20] , especially AlphaFold 2,  it is now  possible to efficiently predict  protein structures from sequences at a large scale  [21].  Recently, some research es directly take the protein structure feature as input to train the  geometric deep learning model,  which has been proved to achieve better or similar  performance in prediction of protein function compared to language models  [22-24].  However, the fused deep -learning method which can make the use of both sequence  and structural information of the protein to map the sequence -function is yet much to  be explored [25].   Recently , both supervised and unsupervised  models have been developed for  protein engineering , i.e.,  prediction of the fitness of protein mutants [24, 26]. Generally  speaking,  the supervised mod el can often achieve better performance as compared to            the unsupervised model [26], but the former  requires a great amount ( at least hundreds  to thousands) of experimental  mutation  data of the protein s tudied for training, which  is experimentally challenging [18]. In contrast, the unsupervised model does not need  any of such experimental data, but its performance is relatively worse, especially for  the high-order  mutant, which is often the final product  of a direct -evolution project . It  is thus highly desirable to  develop a deep -learning algorithm, which can efficiently and  accurately predict the fitness of protein variants, especially the high-order  mutant,  without the need of a large size of  experimental mutation data of the protein concerned.  In the present work , we built a supervised deep learning model (SESNet) , which can  effectively fuse the protein sequence and structure information together to predict the  fitness of variant sequence s (Fig 1A) . We demonstrated that SESNet outperforms  several  state-of-the-art models on 26 metagenesis datasets . More over, to reduce the  dependence of the model on the quantity of experimental mutation data, we proposed a  data-augmentation strategy  (Fig 1B) , where the model was firstly pre -trained using a  large quantity of the low -quality results derived from the unsupervised model and then  finetuned by a small  amount of the high -quality experimental results.  We showed that  the proposed  model can achieve very high accuracy in predicting the fitness of high - order variants  of a protein , even  for those  with more than four  mutation sites , when the  experimental dataset  used for finetuning  is as small as 40.  Moreover , our model can  predict the key AA sites, which are cr ucial for the protein fitness, and thus the protein  engineer can focus o n these key sites for mutagenesis. This can great ly reduce the  experiment cost  of trial and error .    Results   Deep learning -based architecture of SESNet for predicting protein fitness.    To exploit the diverse  information from  protein sequence , coevolution  and  structure , we fuse three encoder modules  into our model . As shown in Fig 1 A: the first  one (local encoder) account s for residue inter dependence  in a specific protein learned  from evolution -related sequence s[15, 16] ; the second one (global encoder) captur es the  sequence feature in global protein sequence  universe [6, 12] ; and the  third one (structure  module) captures  local structural feature around  each residue learned from 3D  geometric structure of the protein [23, 24] . To integrate the information of different  modules, we first concatenate  representations of local and global encoders  and get a n  integrated  sequence representation . This integrated sequence representation is then sent  to an attention layer and becomes the sequence attention  weights , which will be further  averaged  with the structure attention weights  derived from structure module , leading to   the combined  attention  weights . Finally, t he product of combined  attention weights  and  the integrated  sequence representation  is then fed  into a fully connected layer to  generate  the predicted fitness.  The combined  attention weights  can also be used to  predict the key AA site s, critical for the protein fitness, details of which is discussed in  the section of Method .              Figure 1.  Architecture of model  and the s chematic of data -augmentation strategy . Architecture  of SESNet  (A): The local encoder account s for the inter-residue  dependence in a protein learned  from MSA of homologous sequences using a Markov random field [27]. The global  encoder captur es  the sequence feature in global protein sequence universe  using protein language model [6]. The  structure module  account s for the microscopically environmental feature  of a residue learned from  3D geometric structure of  the protein [23, 28] . Schematic of d ata-augmentation strategy . (B): We  first build a mutant library containing all of the single -site mutants and numerous double -site  mutants . Then, a ll of these mutated sequences are scored by the unsupervised model . After that,   these mutants are used to pre -train the initial model  (SESNet) , which will be further finetuned on a  small number  of low -order experimental mutational data.       SESNet outperforms state -of-the-art methods for predicting fitness  of variants  on  deep mutation scan  (DMS)  datasets    We compared our supervised  model  against the existing state-of-the-art supervised  models, ECNet [17], ESM -1b [6]; and unsupervised model s, ESM -1v [9], ESM -IF1[23]  and MSA transformer [15]. As can be seen  in Fig 2 A, in 19 out of 20 dataset s, the  supervised models generally outperform the unsupervised ones as expected, and our  model  (SESN et) achieves  the best performance  among all the models . Moreover , we             further  explore d the ability of our model  to predict the fitness of higher -order variants   by training it using the experimental results of the low-order variants  on 6 datasets of   DMS.  As shown in Fig 2B&C, our model  outperform s all the other models.  Data in Fig  2 is presented  in Supplementary Tables 1,2&3. These datasets cover various  proteins  and different  types  of functionalities , includ ing catalytic rate , stability, and binding   affinity to peptide, DNA, RNA and antibody, as well as  fluorescence  intensity  (Table  4). While most of the datasets contain only single -site mutants , five of them involve  both single -site and double -site mutants , and the dataset of GFP contains data up to 15 - site mutants .     All three components contribute positively to the  performance  of SESNet .   As described  in the above  architecture  (Fig.1 A), our model integrates three  different encoders or modules  together . To investigate how much contribution each of  the three parts makes , we performed ablation studies in 20 dataset s of single -site  mutant s. Briefly , we removed  each of the  three components  and compared the  performance  to that of the original model. As shown in Supplementary  Table 5, t he  average spearman correlation of the original model  is 0.6 72, much  higher than that   without local encoder (0.639), that without global encoder ( 0.247 ) and th at without  structur e module ( 0.630 ). The ablation study reveals  that all three comp onents   contribute to the improvement of model performance , and the contribution from  the  global encoder, which captures the sequence feature in global protein sequence universe,   is the most significant .     The combined attention weights guide the finding of the  key AA site.   The combined  attention weights  can be used to measure  the importance of each AA  site on protein fitness  when  mutated . To the first approximation, higher the attention  score is, more important the AA site is.  To test this approximation , we trained our model  on the experimental data of 1084 single -site mutant s in the  dataset of GFP [29], a green  fluorescent protein from Aequorea victoria . The ground truth of the key sites of GFP  are defined here as the experimentally discovered top 20 sites, which exhibit the largest  change of protein  fitness  when mutated, or the AAs forming and stabilizing the  chromophore, which are known to significantly affect the fluorescent function of the  protein [30], but lack the fitness results in the experimental  dataset.  Indeed , one can  observe that, at least  4 out of 7 top attention -score  AA sites predicted by our model  are  the key sites  as two of them (AG65 and T201 ) are located at the chromophore, and the  other two  (P73 and R71 ) were among the top 20 residues  discovered in experiment to   render the highest change of fitness when mutated (Fig 3A and Fig S1 A). Interestingly,  when we removed the structure module from the model,  only one residue  in the  predicted top-7 attention -score  AA is the key site  (Fig 3B and Fig S1B).   To further verify this discovery, we also performed these tests on the dataset of  RRM, the RNA recognition motif of the Saccharomyces cerevisiae  poly(A) -binding  protein [31]. The key sites of RRM are defined as the experimentally discovered top 20  sites, which render  the largest change of fitness of the protein when mutated, or the  binding sites , which are within 5 √Ö of the RNA molecules as revealed in the structure            of PDB 6R5 K. Fig 3C and Fig S2A show that 4 out of 7 top attention -score AA sites  predicted by our model are the key AAs. One of them (I12) is among the top 20 residues  and three of them (N7, P10 and K39) are binding sites. Whereas, no key residue can be  found in th e predicted top-seven attention -score AAs, when we removed the structure  module. (Fig 3D and Fig S2B) .   The result s in Fig. 3 demonstrate  that the structural module which learns  the  micro scopically structural  information  around each residue  makes important   contribution to identify the key AAs , which are crucial for the protein fitness . Although  the ablation study (Supplementary  Table 5) reveals that the addition of the structural  module improves  the average spearman correlation  over 20 datasets  only by 4 percent,  Fig. 3 demonstrates an important role of the structural module, which can guide the  protein engineer to identify the important AA site s in a protein for mutagenesis  .    Data -augmentation strategy  boosts the performance of the fitness prediction   when finetuned by a small size of labelled experimental data .   Supervised model is normally performing better than the unsupervised models  (see Fig. 2) [26]. But the accuracy of the supervised model is highly affected by the  amount  of input experimental results used for training . However, it is experimentally  challenging and cost ly to generate sufficient data ( many hundreds or even thousands )  for such purpose on every protein  studied . To address this challenge, we propose a  simple strategy  of data augmentation  by using  the result generated by one unsupervised  model  to pre-train our model  on a given protein , and then  finetuning it using a limited  number of experimental results on the same protein . We call it a pre -trained  model . We  note that  data-augmentation strategy has been applied in various earlier work and has  achieve d good  success in protein design [23, 32, 33] . In particular, to improve the  accuracy of inverse folding,  ref [23] used 16153  experimentally determined 3 -D  structure s of proteins  and 12 million  structures predicted by the AlphaFold 2  [19] to  train the model ESM -IF1[23] . In the present w ork, the data augmentation strategy is  used for a different purpose that it can reduce the dependence of the supervised model  on the size of the experimental data when predicting the fitness of protein mutants. We  took GFP as an example  to illustrate our data -augmentation strategy  as GFP has a large  number  of experimental data for testing, particularly the experimental data for high- order  mutants  (up to 15 -site mutant) . We used the fitness results of low -order mutants  predicted by the un supervised model, ESM -IF1, to pre -train our model. The pre - training dataset contains the fitness of all single -site mutants and 30,000 double -site  mutants randomly selected out of ten s of million double -site variants. Then , we  finetuned the pre -trained model by a certain number of experimental results of single - site mutants. The resulting model was used to predict the fitness of  high-order muta nts.  As can be seen in Fig. 4A -D, when comparing with the  original  model without pre - training (blue bars) , the performance of the pre-trained model is significantly improved   (red bars) . Such improvement is particularly large when only a small number of  experimental data (40) is fed for training, and it will be gradually reduced when feeding  more experimental data, e ventually disappear ing when more than 1000 experimental  data were used for training.  Here, we would like to particularly  highlight the case  when            the finetun ing experimental dataset contains only 40 data points. As can be seen in Fig.  4A, the pretrained model can achieve high spearman correlation of 0.5 -0.7 for multisite - mutants, even  for high-order  mutants with 5-8 mutation sites. This is remarkably  important for most protein engineers , as such experimental workload  (40 data points)   is ge nerally affordable in  an ordinary biochemical research group. However, without  pre-training, the performance  of the supervised model  is rather low (~0.2). This  comparison demonstrates  the advantage of the data augmentation  strategy proposed in  the present work .   Moreover, we also compared the performance of the pretrained model with respect  to the unsupervised model (green bars), which were used for generating the low -quality  pretraining datasets. As can be seen, when only 40 experimental data w ere used for  training, the pretrained model has similar performance as compared to the unsupervised  model for low -order mutants (< 4 mutation sites), but clearly outperforms the latter for  high-order mutants (>4 mutation sites). When feeding more experimental data,  especially a couple of hundreds, the pretrained model will outperform the unsupervised  model regardless of how many si tes of the protein were mutated.    The unsupervised model used for analysis in Fig. 4 is ESM -1F1, which captures  the local structural information of a residue . To demonstrate the general superiority of  data-augmentation strategy proposed here, we also tested the results using other  unsupervised model to generate the augmented datasets for GFP. As can be seen  in Fig.  S3, we used ProGen2  [8], an unsupervised model  to learn the global sequence  information , for data augmentation,  and still derived the similar conclusion as in Fig. 4 .  That is, the pretrained model outperforms the original model without pretraining  especially when a s mall experimental dataset is used for training, and it also beats the  unsupervised model  particularly for the high-order mutants.   To further validate the generality  of the data augmentation strategy proposed here,  we did the analysis on  the dataset of other proteins: toxin -antitoxin complex (F7YBW8)  [34]containing data up to 4 sites mutants, and Adeno -associated virus cap sids  (CAPSD_AA V2S )[35], a deep mutational dataset includ ing data up to 23 -site mutants.  We used the unsupervised model ProGen2 [8] to generate the low -quality data  of  F7YBW8  for pretraining , since we found ProGen2 performs better than ESM -IF1 on  this dataset. As shown in Fig 5A, the pre -trained model outperforms both the original  model without pretraining and the unsupervised model in the fitness prediction of all  multi -site mutants  (2-4 sites) after finetuned by using only  37 experimental data  points .  In addition , in the dataset of CA PSD_AA V2S (Fig 5B), the pre -trained model  also  achieves the best performance in all of the high -order mutants ranging from 2 to 23  sites, when finetuned by only 20 experimental data points . These results  further support   the practical use of our data augmentation  strategy , as the required experimental effort  is largely affordable on most proteins .     Learned models provide insight into protein fitness .   SESNet projects a protein sequence into a high dimensional latent space and  represents each mutant as a vector by the last hidden layer . Thus, w e can visualize the  relationships between  sequences in these latent spaces to reveal how the networks learn             and comprehend protein fitness . Specifically, we trained SESNet on the experimental  data of single -site mutants from the datasets  of GFP and RRM , then we used the trained  model  and untrained model  to encode each variant and extracted the output of the last  hidden layer as a representation of the variant sequence. Fig S 4 shows  a two - dimensional projection of the high dimensional latent space using t -SNE [36]. We found   that the representations of positive and negative variants , i.e., the  experimental  fitness  values being larger or smaller than that of wildtype , generated by the trained SESNet  are clearly clustered into distinct groups  (Fig S 4A and Fig S 4B). In contrast, the  representations from untrained model cannot provide a distinguishable boundary   between positive and negative  variants  (Fig S 4C and Fig S 4D). Therefore, SESNet can  learn to distinguish functional fitness of mutants  into a latent representation space with  supervised training.    Furthermore, to explore why the data-augmentation strategy works, we performed  a case study on GFP dataset. Here, w e compared the latent -space representation from   the last hidden layer generated by our model  with and without pre-training using the  augmented data from the unsupervised model . As seen in Fig. S5A, after pretraining  even without finetuning by the experimental data , SESNet can  already  roughly  distinguish the negative and positive mutants. One thus can deduce that  the pre -training  can furnish a good  parameter initialization for SESNet. After further finetuning the pre - trained SESNet by only 40  experimental data  points  of single -site mutants, a rather  clear boundary between negative and positive high -order mutants is further outlined  (Fig S 5B). In cont rast, when we skip ped the pretraining process,  i.e., directly train ing  the model on 40 experimental data points , the separation between the  positive and  negative high -order mutants is rather ambiguous  (Fig S 5C). This comparison  demonstrates  the superiority of our data -augmentation strategy in distinguishing   mutants  of distinct fitness values , when the number of available experimental data is  limited.               Figure  2. Spearman correlation of predicted fitness.  A: Comparison of our model to other models  on the predicted fitness  of the single -site muta nts on 20  datasets . We performed five -fold cross - validation with 7:1:2 as the ratio of train  versus  validation  versus  test set. B: comparison of predicted  fitness of  double -site mutants of our model to other unsupervised  models  (ESM -1v, ESM -IF1 and  MSA transformer) , or supervised models (ECNet and ESM -1b). Here, our model and other  supervised models were trained on the data of single -site mutants . We used 10% of doubl e-site  mutants as validation set and the remaining 90% as test set . C: Comparison of our model to other  models on fitness p rediction of quadruple -site muta nts of GFP. Here, our model and other  supervised model were trained using the single, double, triple -site mutants and all the three together.  We used 10% of quadruple -site mutants as validation set and the remaining 90% as test set . The  error bar in  single -site mutant was got from the five-fold cross -validation.  Since we cannot  do five- fold cross -validat ion in the fitness prediction  of high -order mutant s trained on low-order mutants ,  we don‚Äôt put error bar for those data.                    Figure 3.  The sites with the top 7 largest  attention score s on the wildtype sequence.  A&B:  The key sites of GFP have been marked as red spheres. A: 4 key  sites were recovered by our model.  G65 and T201 are the active residues helping to  form and stabilize the  chromophore in GFP as  described by Ref [30]. P73 and R71 are among the experimentally -discovered t op 20 sites, which  render  the highest change of fitness when mutated.  B: Only one key site was identified by the model  when removing the structure module  and it is Y37, which is among the experimentally -discovered  top 20 AA sites . C&D: The key sites of RRM  have been marked as red spheres. C: 4 key sites were  recovered by the original model. N7, P10 and K39 are the binding sites which are  within 5√Ö of  the  RNA  molecules . I12 is among the experimentally -discovered top 20 sites, which render  the highest  change of fitness when mutated. D: There is no key site identified by the model when removing the  structure module .                 Figure 4. Results of models  trained  on different num ber of experimental variants . A-D:  The s pearman correlation of fitness prediction on multiple sites  (2-8 sites)  mutants by finetuning  using 40, 100, 400, 1084 single -site experimental mutation results  from dataset of GFP . Where the  red and blue bars represent the  results of the pre -trained model and the original model without  pretraining, respectively . And the green bar s correspond  to the results of the unsupervised model  ESM -IF1 as a control.       Figure 5.  Results of models trained on different datasets.  A-B: The spearman correlation of  fitness prediction on  high-order  mutants by finetuning on 37 experimental  single -site mutation  results  from datasets  of F7YBW8  and on 20 experimental  single -site mutation results  of  CAPSD_AA V2S, respectively.  Where the red and blue bars represent the results of the pre -trained  model and the original model without pretraining . And the green bars correspond to the results of  the unsupervised mode l, which is  ProGen2 for F7YBW8 and ESM -IF1 for CAPSD_AA V2S ,  respect ively .               Discussion   In this study, we present  a supervised deep learning model, which leverages the  information of both sequence and structure of protein to predict the fitness of variants.  And this model is found to  outperform  the existing state -of-the-art ones for protein  engineering. Moreover, we proposed a data augmentation  strategy, which pretrains our  model using the results predicted by other unsupervised model, and then finetunes  the  model with only a small number of experimental results. We demonstrated that such  data augmentation  will significantly  improve the accuracy of the model when the  experimental results are very limited  (~40), and also for high-order mutants  with >4  muta tion sites . We noted that our work , especially the data -augmentation strategy  proposed  here, will be of great practical importance  as the experimental effort it requires  is generally affordable  by an ordinary biochemical  research group and can be applied  on most  protein.     Method   Details of Model Architecture   Local encoder . Residue interdependencies are crucial  to evaluate if a mutation is  acceptable. Several models, includ ing ESM -MSA -1b [37], Deep Sequence [14],  EVE [38] and the Potts model [27], such as EVmutation [16] and ECNet [39], utilize  multiple sequence alignment (MSA) to dig the constraints of evolutionary process in  the residues level. In the present work,  we use Potts model  to establish the local encoder.  This method first searches for the homologous sequences and builds MSA of the given  protein with HHsuite [40]. After that, a statistical model is used to identif y the  evolutionar y couplings by  learning a generative model of the MSA of homologous  sequences using a Markov  random field.  In the model, the probability of each sequence  depends on an energy function , which  is defined as  the sum of single -site constraints  ùëíùëñ and all pairwise coupling constraints ùëíùëñùëó:   ùê∏(ùë•)=‚àëùíÜùëñ(ùë•ùëñ)+‚àë ùíÜùëñùëó(ùë•ùëñ,ùë•ùëó) ùëñ‚â†ùëó ùëñ                   (1)  Where ùëñ  and ùëó  are position indices along the sequence. The i-th amino acid xi is  encoded by a vector, in whi ch elements are set to the single -site term ei(xi) and pairwise  coupling terms eij(xi, xj) for j=1,‚Ä¶,n, n is the number of residues  in the sequence . These  coupling parameters ei and eij can be estimated using regularized maximum  pseudolikelihood  algorithm [41, 42] . As the result, each amino acid in the sequence  is  represented by a vector whose length is (ùêø+1) , and the whole input sequence is  encoded as a matrix whose size is (ùêø+1)√óùêø . Since the length of the local  evolutionary representation of each amino acid is close to the length of the sequence,  the (ùêø+1)-long vector would be transformed into a new vector with fixed length ùëëùëô  (in our local encoder, ùëëùëô=128) through a fully connected layer to avoid the overfitting  issue. Sequence of protein would also pass a Bi -LSTM layer and be transformed into  an ùêø√óùëëùëô matrix for random initialization. By concatenating two matrices above, we  obtain the output of local encoder ùíÜ‚Ä≤=<ùíÜùüè‚Ä≤,ùíÜùüê‚Ä≤,‚Ä¶ùíÜùë≥‚Ä≤>, whose size is ùêø√ó2ùëëùëô.              Global Encoder . Recently, the large scale pre -trained model s have been successfully  applied in diverse  tasks  for inferring protein structure or function based on sequence  information . Such as prediction of secondary structure, contact prediction and  prediction of mutational effects. Thus, we take a pre -trained protein language model as  the global encoder which is responsible to extract biochemical properties and evolution  information of the protein sequences. There are some effective language models such  as UniRep [12], TAPE [43], ESM -1v [44], ESM -1b [37], ProteinBERT [11] etc. We test  these language models on our validation datasets, and results show that ESM -1b  performs better than others. Therefore, we chose to use ESM -1b as the global encoder.  The model is a bert -based [45] context -aware language model for protein, trained on  the protein sequence dataset of UniRef 50  (86 billion amino acids across 250 million  protein sequences). Due to its ability to represent the biological properties and  evolutionary diversity of proteins, we utilize this model as our global encoder to encode  the evolutionary protein sequence. Forma lly, given a protein sequence ùíô=< ùíôùüè,ùíôùüê,‚Ä¶,ùíôùë≥> ‚àà ùë≥ùëµ as input, where ùíôùíä is the one -hot representation of ùíäùíïùíâ amino  acids in the evolutionary sequence, ùë≥ is the length of the sequence, and ùëµ is the size  of amino acids alphabet. The global encod er first encodes each amino acid and its  context to ùíà=<ùíàùüè,ùíàùüê,‚Ä¶,ùíàùë≥>  , where ùíàùíä‚ààùëπùíè , (in ESM -1b, ùëõ=1420  ). Then  ùíàùíä is projected to ùíàùíä‚Ä≤ of a hidden space ùëπùíâ with a lower dimension (in our default  model config uration , ‚Ñé=256), ùíàùíä‚Ä≤=ùëæùëÆùíàùíä+ùíÉ, where ùëæùëÆ‚ààùëπùíè√óùíâ is a learnable  affine transform parameter matrix and ùíÉ‚ààùëπùíâ  is the bias. The output of global  encoder is ùíà‚Ä≤=<ùíàùüè‚Ä≤,ùíàùüê‚Ä≤,‚Ä¶ùíàùë≥‚Ä≤> ‚ààùëπùë≥√óùíâ. We integrate the ESM -1b architecture into  our model i.e.; we update the parameters of ESM -1b dynamically during the training  process.     Structure module . Structure module utilizes the micro environmental information to  guide the fitness prediction. In this part, we use the ESM -IF1 model [23] to generate  the scores of mutant sequences, which evalua te their ability to be folded to the wildtype  structure of the given protein. Higher scores mean these mutations are more favorable  than others. Specifically, all possible single mutants at each position of a sequence  would obtain the corresponding scores.  The prediction sequence distribution is an  (ùêø√ó20) matrix. Then we calculated the cross -entropy at each position of the sequence  between the matrix above and one -hot encoding matrix of mutant sequence. After  passing the results through a SoftMax  function,  we obtained an (ùêø√ó1) output vector,  which is the reconstruction perplexities ùíë‚Ä≤=<ùëù1‚Ä≤,ùëù2‚Ä≤,‚Ä¶ùëùùêø‚Ä≤> align the evolutionary  sequence.  In the present work, w e do not directly encode distance map or the 3D  coordinate of mutated protein . Since before that encoding process , we need to fold  every specific mutant  from their sequences , which will lead to unaffordable  computational cost  and is unpractical for the task of fitness prediction .    Intra -Attention.  The outputs of local encoder and global encoder are embedding  vectors , aligning all positions of input sequence. We utilize intra -attention mechanism  to compress the whole embeddings to a context vector. The inputs of attention layer are:  (1) the global representations ùíà‚Ä≤=<ùíàùüè‚Ä≤,ùíàùüê‚Ä≤,‚Ä¶ùíàùë≥‚Ä≤>  (2) the local  representations            ùíÜ‚Ä≤=<ùíÜùüè‚Ä≤,ùíÜùüê‚Ä≤,‚Ä¶ùíÜùë≥‚Ä≤> (3) the reconstruction perplexities ùíë‚Ä≤=<ùëù1‚Ä≤,ùëù2‚Ä≤,‚Ä¶ùëùùêø‚Ä≤>. Firstly,  the local representations and global representations are normalized by layer  normalization [46] over the length dimension respectively for stable training. That is,  ùíà‚Ä≤=ùë≥ùíÇùíöùíÜùíìùëµùíêùíìùíé (ùíà‚Ä≤)  and ùíÜ‚Ä≤=ùë≥ùíÇùíöùíÜùíìùëµùíêùíìùíé (ùíÜ‚Ä≤) . Secondly, the normalized  global representations and local representations are concatenated to joint - representations ùíì=<ùíìùüè,ùíìùüê,‚Ä¶ùíìùë≥>, where ùíìùíä=[ùíàùíä‚Ä≤;ùíìùíä‚Ä≤]‚ààùëπùüêùíâ. Then we use an dot  attention la yer to compute the sequence attention weights ùëé=<ùëé1,ùëé2,‚Ä¶,ùëéùêø> ‚ààùëÖùêø,  where ùëéùëñ‚ààùëÖ  is the attention weight on the ùëñùë°‚Ñé  position, ùëéùëñ=exp  (ùëüùëñ ‚àôùëäùëéùëüùëñ) ‚àë exp (ùëüùëò‚àôùëäùëéùëüùëò)ùëõ ùëò=1 ,  ùëäùëé‚ààùëÖ‚Ñé√ó1 is the learnable parameter. Besides the sequence attention wei ghts, there is  structure attention weights called structure attention ùë†=<ùë†1,ùë†2,‚Ä¶,ùë†ùêø> ‚ààùëÖùêø, which  are calculated by reconstruction perplexities, ùë†ùëñ=exp  (ùëùùëñ‚Ä≤) ‚àë exp (ùëùùëò‚Ä≤)ùëõ ùëò=1. We use the average of  sequence attention and struct ure attention as the final combined attention weights, that  is ùë§=<ùë§1,ùë§2,‚Ä¶,ùë§ùêø> , where ùë§ùëñ=ùëéùëñ+ùë†ùëñ 2 . According to the combined attention  weights, we get the context vector ùíÑ=‚àë ùë§ùëñùíìùíäùêø ùëñ=1  as the embedding vector of the entire  sequence.     Output layer.  The input of output layer is the context vector ùíÑ  from the output of  attention aggregator, and an evolutionary score ùëë from the unsupervised  model [23].  While the evolutionary score may not be trusted in many  cases, we use a dynamic  weight to take the score into account. The context vector ùíÑ was firstly transformed to  a hidden vector ùíâ , where ùíâ = ùëÖùëíùêøùëà (ùëä‚Ñéùëê + ùëè) , ùëä‚Ñé  and ùëè  are learnable  parameters, and ReLU [47] is the activation function. Then, the hidden vector ùíâ is  used to calculate  the weight ùëù‚àà(0,1) on ùëë: ùëù = ùëÜùëñùëîùëöùëúùëñùëë (ùëäùëù[ùíâ;ùëë]). The scale of  ùíë quantifies  how much should the model trust the score from the zero -shot model. At  last, we use a linear layer to compute a fitness score ùë¶ùëû‚ààùëÖ according to the hidden  vector ùíâ  directly, where ùë¶ùëû = ùëäùëû‚Ñé + ùëè . The output of our model, i.e., the  prediction fitnes s ùë¶‚ààùëÖ is computed as:   ùë¶ = (1‚àíùëù) √ó ùë¶ùëù + ùëù √ó ùë¶ùëû.                  (2)  We utilize the mean square error (MSE) as the loss function to update model parameters  during back -propagation:   ùëôùëúùë†ùë† =1 ùëÅ‚àë (ùë°ùëñ‚àíùë¶ùëñ)2 ùëÅ ùëñ=1                        (3)  , where ùëÅ is the number of samples in a mini -batch, ùë°ùëñ is the target fitness and ùë¶ùëñ is  the output fitness .    Dataset and experimental settings   Benchmark dataset collection . We first collected  20 multiple deep mutational scanning    datasets from Ref [14]. Most of them  only contain the fitness data of single -site mutants,  while one of them (RRM) [31] also provide s data of high -order mutants. The fitness  data measured in these datasets include  enzyme function, growth rate, peptide binding,  viral replication and protein stability. We also collected the mutant data of the WW            domain of human Yap1, GB1 domain of protein G in Streptococcus sp. group G  and  FOS-JUN heterodimer from Ref [48], and the prion -like domain of TDP -43 from Ref  [49] to evaluate the ability of our model to predict the effect of double -sites mutant by  learning from  the data of single -site mutant . Besides, the ability to predict the fitness of  higher order mutants (larger than 2) is tested in the dataset from Ref [29]. This study   analyzed  the local fitness landscape of the green fluorescent protein from Aequorea  victoria  (avGFP) by measuring the native function (fluorescence) of tens of thousands  of derivative genotypes of avGFP . The detailed information on these datasets are  provided in Table 4 in the Supplement Information.     Prediction of single -site mutation effects . We compared our model to ECNet , ESM -1b,  ESM -1v and MSA transformer model on the DMS datasets. For the supervised models  (ECNet and ESM -1b), we performed five -fold cross -validation on these datasets, and  12.5% of each train set are randomly selected as  valid set. Spearman correlation was   used to evaluate the performances of different models.     Prediction of High -order mutation effects . We evaluated the performance for  predicting the fitness of high -order mutants  by the model trained on low -order mutants .  The training set for the prediction  of double -site mutants only contains the experimental  fitness of single -site mutants. The m odels used to predict the fitness of quadruple  mutants of avGFP are trained on single, double, triple, and all the three types of mutants,  respectively. Both in the  prediction of effect of double mutants and quadruple mutants,  we chose 10% of the high -order mutant data as valid set. The performances of models  were evaluated by Spearman correlation.     Data -augmentation strategy. The data augmentation was conducted  by pre-training our  model on the results predicted by the unsupervised model. To be specific, we first built  a mutant library , which contains all of the single -site mutants and 30,000 double -site  mutants randomly selected from tens of millions of saturated dou ble-site mutants.  Then ,  we used ESM -IF1 (or ProGen2) to score all of these sequences. Those sequence -score  data were used to pre -train our model. While we used 90% of the data as training test,  10% as validation  set. After that, we finetuned the pre -traine d model on single -site  mutants from experiment  with the high -order mutants as test set.     Training details.  SESNet was trained with adam  optimizer with weight decay (equals  to L2 norm). Hyperparameters of the model were tuned with a local grid search on the  validation set. Since conducting 5 -fold cross -validation and grid search on 20 datasets  is costly, we only searched on two representat ive datasets. We performed grid search  on GFP dataset for multi -sites dataset and RRM dataset for single -site dataset to obtain  the best hyperparameters configuration and apply the search results in other datasets.  We tested the hidden size of [128, 256, 5 12], learning rate of [1e -3, 5e -4, 1e -4, 5e -5,  1e-5], and dropout of [0.1, 0.2, 0.4]. Table 7 in SI shows the details of the  hyperparameters configuration. All experiments are conducted on a GPU server with  10 RTX 3090 GPUs (24GB VRAM) and 2 Intel Gold 622 6R CPUs with 2TB RAM.             Model contrast . The source code of ECNet model for contrast is downloaded from the  GitHub website (https://github.com/luoyunan/ECNet) provided by Ref [17]. The ESM - 1b model is also reproduced in our local computers with architecture that is described  in their publication [6]. The code of ESM -IF1, ESM -1v and MSA transformer (ESM - MSA -1b) are got from the GitHub website of Facebook research  (https://github.com/facebookresearch/esm ). For each assay, all experiments of three  different models are performed in the same dataset.     Ethical Approval   Not applicable     Competing interests   The authors declare no competing interests.     Authors‚Äô contributions   LH and PT designed this project, PT and ML proposed this model, ML, LQ, YX, YGW  and GF  implemented the method, performed the calculations . All of the authors read  and approved the final manuscript .  Author‚Äôs notes   ‚Ä†These authors contributed equally to this work .  *To whom correspondence should be addressed : tpan1039@ alumni. sjtu.edu.cn ;  hongl3liang@sjtu.edu.cn.     Funding   This work was financially supported by the Natural Science Foundation of China (Grant  No. 12104295 , 11974239, 31630002, 61872094, 61832019 ), the Innovation Program  of Shanghai Municipal Education Commission, and Shanghai Jiao Tong university  Multidisciplinary research fund of medicine and engineering YG 2016QN13 . The  computing hardware resource  was supported by the Center for High Performance  Computing at  Shanghai Jiao Tong University .    Availability of data and materials   Source c ode for SESNet and all the datasets used in the present work can be found in  the supplemental materials . Where the original sources of dataset s have been declaimed  and cited in the main text.               Reference   1. Arnold, F.H., Design by directed evolution.  Accounts of chemical research, 1998. 31(3): p. 125 - 131.  2. Wu, Z., et al., Machine learning -assisted directed protein evolution with combinatorial libraries.   Proceedings of the National Acade my of Sciences, 2019. 116(18): p. 8852 -8858.   3. Cui, Y ., et al., Computational Redesign of a PETase for Plastic Biodegradation under Ambient  Condition by the GRAPE Strategy.  ACS Catalysis, 2021. 11(3): p. 1340 -1350.   4. Hie, B., et al., Learning the languag e of viral evolution and escape.  Science, 2021. 371(6526):  p. 284 -288.  5. Hie, B.L., K.K. Yang, and P.S. Kim, Evolutionary velocity with protein language models  predicts evolutionary dynamics of diverse proteins.  Cell Systems, 2022.   6. Rives, A., et al., Biological structure and function emerge from scaling unsupervised learning  to 250 million protein sequences.  Proceedings of the National Academy of Sciences, 2021.  118(15): p. e2016239118.   7. Rao, R., et al., Evaluating protein transfer learning with TAPE.  Advances in neural information  processing systems, 2019. 32: p. 9689.   8. Nijkamp, E., et al. ProGen2: Exploring the Boundaries of Protein Language Models . 2022.   9. Meier, J., et al., Language models enable zero -shot prediction of the effects of mutations on  protein function.  bioRxiv, 2021: p. 2021.07.09.450648.   10. Elnaggar, A., et al., ProtTrans: towards cracking the language of Life's code through self - supervised deep learning and high performance computing.  arXiv preprint arXiv:2007.06225,  2020.   11. Brandes, N., et al., ProteinBERT: A universal deep -learning model of protein sequence and  function.  Bioinformatics, 2022. 38(8): p. 2102 -2110.   12. Alley, E.C., et al., Unified rational protein engineering with sequence -based deep  representation learning.  Natu re methods, 2019. 16(12): p. 1315 -1322.   13. Russ, W.P., et al., An evolution -based model for designing chorismate mutase enzymes.  Science,  2020. 369(6502): p. 440 -445.  14. Riesselman, A.J., J.B. Ingraham, and D.S. Marks, Deep generative models of genetic v ariation  capture the effects of mutations.  Nature Methods, 2018. 15(10): p. 816 -822.  15. Rao, R.M., et al., MSA Transformer , in Proceedings of the 38th International Conference on  Machine Learning , M. Marina and Z. Tong, Editors. 2021, PMLR: Proceedings of  Machine  Learning Research. p. 8844 --8856.   16. Hopf, T.A., et al., Mutation effects predicted from sequence co -variation.  Nature Biotechnology,  2017. 35(2): p. 128 -135.  17. Luo, Y ., et al., ECNet is an evolutionary context -integrated deep learning framework for protein  engineering.  Nature Communications, 2021. 12(1): p. 5743.   18. Biswas, S., et al., Low-N protein engineering with data -efficient deep learning.  Nature Methods,  2021. 18(4): p. 389 -396.  19. Jumper, J., et al., Highly accurate protein st ructure prediction with AlphaFold.  Nature, 2021.  596(7873): p. 583 -589.  20. Baek, M., et al., Accurate prediction of protein structures and interactions using a three -track  neural network.  Science, 2021. 373(6557): p. 871 -+.            21. Varadi, M., et al., AlphaFo ld Protein Structure Database: massively expanding the structural  coverage of protein -sequence space with high -accuracy models.  Nucleic Acids Research, 2022.  50(D1): p. D439 -D444.   22. Zhang, Z., et al., Protein Representation Learning by Geometric Structur e Pretraining.  arXiv  preprint arXiv:2203.06125, 2022.   23. Hsu, C., et al., Learning inverse folding from millions of predicted structures.  bioRxiv, 2022.   24. Lu, H., et al., Machine learning -aided engineering of hydrolases for PET depolymerization.   Nature,  2022. 604(7907): p. 662 -667.  25. Wang, Z., et al., LM-GVP: an extensible sequence and structure informed deep learning  framework for protein property prediction.  Scientific Reports, 2022. 12(1): p. 6832.   26. Gelman, S., et al., Neural networks to learn pr otein sequence ‚Äìfunction relationships from deep  mutational scanning data.  Proceedings of the National Academy of Sciences, 2021. 118(48): p.  e2104878118.   27. Ekeberg, M., et al., Improved contact prediction in proteins: Using pseudolikelihoods to infer  Potts models.  Physical Review E, 2013. 87(1): p. 012707.   28. Shroff, R., et al., Discovery of novel gain -of-function mutations guided by structure -based deep  learning.  ACS synthetic biology, 2020. 9(11): p. 2927 -2935.   29. Sarkisyan, K.S., et al., Local fit ness landscape of the green fluorescent protein.  Nature, 2016.  533(7603): p. 397 -401.  30. Zimmer, M., Green fluorescent protein (GFP): applications, structure, and related  photophysical behavior.  Chemical reviews, 2002. 102(3): p. 759 -782.  31. Melamed, D.,  et al., Deep mutational scanning of an RRM domain of the Saccharomyces  cerevisiae poly (A) -binding protein.  Rna, 2013. 19(11): p. 1537 -1551.   32. Minot, M. and S.T. Reddy, Nucleotide augmentation for machine learning -guided protein  engineering.  bioRxiv, 20 22: p. 2022.03.08.483422.   33. Hsu, C., et al., Learning protein fitness models from evolutionary and assay -labeled data.   Nature Biotechnology, 2022. 40(7): p. 1114 -1122.   34. Aakre, Christopher  D., et al., Evolving New Protein -Protein Interaction Specificit y through  Promiscuous Intermediates.  Cell, 2015. 163(3): p. 594 -606.  35. Sinai, S., et al., Generative AAV capsid diversification by latent interpolation.  bioRxiv, 2021: p.  2021.04.16.440236.   36. Van der Maaten, L. and G. Hinton, Visualizing data using t -SNE. Journal of machine learning  research, 2008. 9(11).   37. Rao, R.M., et al. Msa transformer . in International Conference on Machine Learning . 2021.  PMLR.   38. Frazer, J., et al., Disease variant prediction with deep generative models of evolutionary data.   Nature, 2021. 599(7883): p. 91 -95.  39. Luo, Y ., et al., ECNet is an evolutionary context -integrated deep learning framework for protein  engineering.  Nature communications, 2021. 12(1): p. 1 -14.  40. Steinegger, M., et al., HH-suite3 for fast remote homology  detection and deep protein  annotation.  BMC bioinformatics, 2019. 20(1): p. 1 -15.  41. Hopf, T.A., et al., Sequence co -evolution gives 3D contacts and structures of protein complexes.   elife, 2014. 3: p. e03430.   42. Seemayer, S., M. Gruber, and J. S√∂ding, CCMpred ‚Äîfast and precise prediction of protein            residue ‚Äìresidue contacts from correlated mutations.  Bioinformatics, 2014. 30(21): p. 3128 - 3130.   43. Rao, R., et al., Evaluating protein transfer learning with TAPE.  Advances in neural information  processing sy stems, 2019. 32.  44. Meier, J., et al., Language models enable zero -shot prediction of the effects of mutations on  protein function.  Advances in Neural Information Processing Systems, 2021. 34: p. 29287 - 29303.   45. Devlin, J., et al., Bert: Pre -training of deep bidirectional transformers for language  understanding.  arXiv preprint arXiv:1810.04805, 2018.   46. Ba, J.L., J.R. Kiros, and G.E. Hinton, Layer normalization.  arXiv preprint arXiv:1607.06450,  2016.   47. Fukushima, K., Cognitron: A self -organizing multil ayered neural network.  Biological  cybernetics, 1975. 20(3): p. 121 -136.  48. Rollins, N.J., et al., Inferring protein 3D structure from deep mutation scans.  Nature genetics,  2019. 51(7): p. 1170 -1176.   49. Bolognesi, B., et al., The mutational landscape of a  prion -like domain.  Nature communications,  2019. 10(1): p. 1 -12.     "
5,https://arxiv.org,2301.00005,10.48550/arXiv.2301.00005,"Stas Tiomkin, Ilya Nemenman, Daniel Polani, Naftali Tishby",Intrinsic Motivation in Dynamical Control Systems,"Biological systems often choose actions without an explicit reward signal, a phenomenon known as intrinsic motivation. The computational principles underlying this behavior remain poorly understood. In this study, we investigate an information-theoretic approach to intrinsic motivation, based on maximizing an agent's empowerment (the mutual information between its past actions and future states). We show that this approach generalizes previous attempts to formalize intrinsic motivation, and we provide a computationally efficient algorithm for computing the necessary quantities. We test our approach on several benchmark control problems, and we explain its success in guiding intrinsically motivated behaviors by relating our information-theoretic control function to fundamental properties of the dynamical system representing the combined agent-environment system. This opens the door for designing practical artificial, intrinsically motivated controllers and for linking animal behaviors to their dynamical properties.","Intrinsic Motivation in Dynamical Control Systems Stas Tiomkina, Ilya Nemenmanb,c,d, Daniel Polanie, and Naftali Tishby‚àóf,g aComputer Engineering Department, Charles W. Davidson College of Engineering San Jose State University, CA, 95192, bDepartment of Physics,cDepartment of Biology,dInitiative in Theory and Modeling of Living Systems, Emory University, Atlanta, GA 30322, USA ,eAdaptive Systems Research Group, University of Hertfordshire, Hateld, UK,fThe Rachel and Selim Benin School of Computer Science and Engineering,gEdmond and Lilly Safra Center for Brain Sciences (ELSC), Hebrew University of Jerusalem, 96906 Israel Abstract Biological systems often choose actions without an ex- plicit reward signal, a phenomenon known as intrinsic motivation. The computational principles underlying this behavior remain poorly understood. In this study, we investigate an information-theoretic approach to in- trinsic motivation, based on maximizing an agent's em- powerment (the mutual information between its past ac- tions and future states). We show that this approach generalizes previous attempts to formalize intrinsic moti- vation, and we provide a computationally ecient algo- rithm for computing the necessary quantities. We test our approach on several benchmark control problems, and we explain its success in guiding intrinsically mo- tivated behaviors by relating our information-theoretic control function to fundamental properties of the dynam- ical system representing the combined agent-environment system. This opens the door for designing practical ar- ticial, intrinsically motivated controllers and for linking animal behaviors to their dynamical properties. Keywords| information capacity jsensitivity gainjsta- bilizationjpredictive information Introduction Living organisms are able to generate behaviors that solve novel challenges without prior experience. Can this ability be explained by a single, generic mechanism? One proposal is that novel, useful behaviors can be generated through in- trinsic motivation [1], which is dened informally as a set of computational algorithms that are derived directly from the intrinsic properties of the organism-environment dynam- ics and not specically learned. Increasingly, there is a move away from reinforcement learn- ing and its extrinsically specied reward structure [2,3] in the theory and practice of articial agents, robots, and machine learning more generally [4{20]. A specic class of such intrin- sic motivation algorithms for articial systems is known as ‚àóProf. Naftali Tishby passed away when this work was in de- velopment. This project began under his leadership when Stas Tiomkin was a PhD student in his group. The rest of the au- thors agree that he should be a senior author on this manuscript, but his consent for this was not obtained.empowerment maximization . It proposes that agents should maximize the mutual information [21] between their poten- tial actions and a subsequent future state of the world [22]. This corresponds to maximizing the diversity of future world states achievable as a result of the chosen actions, potentiating a broader set of behavior options in the future. Intrinsically motivated synthetic agents develop behaviors that are atypi- cal for inanimate engineered systems and often resemble those of simple living systems. Interestingly, potentiating future ac- tions is also a key part of the success of modern reward-based training algorithms [8,23,24]. Despite the successes of empowerment maximization, it re- mains unclear how well it can be used as a general intrinsic motivation principle. There are many dierent versions of in- trinsic motivation related to empowerment, and their relation to each other is unknown [20,23,25]. Additionally, most work on empowerment maximization has relied on simulational case studies and ad hoc approximations, and analytical results are scarce. In order to gain insight, it is important to link em- powerment to other, better-understood characterizations of the systems in question. Finally, calculating the mutual in- formation between two interlinked processes in the general case is a challenging task [26,27], which has so far limited the use of empowerment maximization to simple cases. In this work, we unify dierent versions of intrinsic moti- vation related to the empowerment maximization paradigm. Here our main contribution is in showing analytically that empowerment-like quantities are linked to the sensitivity of the agent-environment dynamics to the agent's actions. This connects empowerment maximization to well-understood properties of dynamical systems. Since highly sensitive re- gions of the dynamics potentiate many diverse future behav- iors, the connection to dynamical systems also explains why empowerment-based intrinsic motivations succeed in generat- ing behaviors that resemble those of living systems. The analytical results allow us to develop a practical com- putational algorithm for calculating empowerment for com- plex scenarios in the continuous time limit, which is the sec- ond major contribution of the paper. We apply the algo- rithm to standard benchmarks used in intrinsic motivation research [14, 28, 29]. Specically, a controller based on the ecient calculation of empowerment manages to balance an inverted pendula without extrinsic rewards. This opens the door for designing complex robotic intrinsically motivated agents with systematically computed | rather than heuristi- cally estimated | empowerment. 1arXiv:2301.00005v1  [cs.AI]  29 Dec 2022CTe;Ta;T  x0x(t) = max p(~ ajx0)I[fXTe; XTe 1; : : : ; X Ta+Tg|{z} ~X future states;fATa 1; ATa 2; : : : ; A 0g|{z} ~A possible actionsjx0x(t)] Empowerment Ta=Te;T= 0controlled Lyapunov expt. Ta= 1;T=Te 1kicked CEF Ta= 1;T= 0 Figure 1: Unied view on information theoretical intrinsic motivation, for a discretized process sequence. Starting at time x0(i.e.x(t)), potential actions are applied for Tatimes, following that, after waiting for  Ttime steps, the future system trajectory is considered until Te. A controlled Lyapunov exponent is a Lyapunov exponent, but only in directions controlled by the agent, cf., (11). \Kicked CEF"" refers to a variant of Causal Entropic Forcing [30], with the addition that an action kicks the system at the beginning of a trajectory. For more details see Generalized Empowerment . Results A Preliminaries Notation We consider an agent that takes on states x(t)2 X:=Rdx, evolving in time under the dynamics fwith (small) stochastic perturbations (t)2Rdx. Via its (small) actions, a(t)2A:=Rdaltered through the control gain g, the agent can aect the dynamics of the system: dx(t) =f(x(t))dt+g(x(t))da(t) +d: (1) Hereddenotes the system noise, modeled as a Wiener pro- cess. The agent's actions a(t) are modeled by a stochastic control process with variance 2 tcontrolled by the agent and with a mean of zero. This models potential eect of actions centered around the null action. To compute various quantities of interest, we will consider a discretized versions of this system, for which we adopt a modied notation. To distinguish it from the continuous version, we replace the continuous time in parentheses by an integer index, xk:=x(t+kt). Here  tdenotes the physical time step, and we adopted the convention that x0=x(t), so that the index corresponding to the current physical time, t, is chosen as 0. We will consider trajectories of a xed duration, and the agent will apply actions over a part of that trajectory. We denote by Tethe time index of the very last state of the trajectory, which we also refer to as the time horizon . We further use Tato denote the (dis- cretized) duration of the action sequence. Then state, con- trol and perturbation trajectories at nite equidistant times, ft+ktgT k=0, are denoted by xTe 0fxkgTe k=0,aTa 0fakgTa k=0, andTe 0fkgTe k=0, respectively. For consistency with the control theory literature, we write a trajectory in the reverse order, e.g., xTe 0= (xTe;:::x 0). When we wish to emphasize the continuous nature of the underlying process, we will write tet+Tetandtat+Tatfor explicitly continuous times. Reinforcement Learning vs. Intrinsic Motivation To elicit a desired behavior in an agent, one typically uses reinforce- ment learning (RL). RL is task-specic, and an agent needs an extrinsic feedback about its performance from a reward func- tion to learn the behavior. The precise construction of this reward function is critical to achieve a desired performance in a short training time [2]. Some of the complications include a signicant degree of arbitrariness when choosing amongst reward functions with equivalent performance [31] and the diculty of translating an often vague desired behavior intoa concrete reward function. Furthermore, complex behaviors consist of combinations of shorter sequences. Designing a re- ward function capable of partitioning the solution into such parts and hence learning it in a realistic time is hard [32]. In contrast to this, in living systems, acquisition of skills often starts with task-unspecic learning. This endows or- ganisms with potentiating skills, which are not rewarding on their own. This is then followed by task-oriented specializa- tion, which combines task-unspecic behaviors into complex and explicitly rewarding tasks [1,33]. While specic tasks are often rened with the help of an extrinsic reinforcement, the potentiating tasks usually are intrinsically motivated [9]. Empowerment The type of intrinsic motivation we focus on is empowerment . Empowerment is based on information- theoretic quantities [4,23,30,34{40]. It denes a pseudo-utility function on the state space, based on the system dynamics only, without resorting to a reward. Formally, we express the dynamics of the system by the conditional probability dis- tributionp(xTejaTe 1 0;x0) of the resulting state when one starts in a state x0and subsequently carries out an action se- quenceaTe 1 0. Then the empowerment C(x0) is a function of the starting state, x0. It is given by the maximally achievable mutual information (the channel capacity [21]) between the control action sequence of length Teand the nal state when starting in the state x0: C(x0) := max p(aTe 1 0jx0)I(XTe;ATe 1 0jx0): (2) Herep() denotes a probability density or a probability distri- bution function, and Iis the mutual information [21] I(XTe;ATe 1 0jx0) =H(XTejx0) H(XTejATe 1 0x0):(3) His the entropy, and conditioning an entropy on a random variable means the entropy of the conditional distribution, averaged over the conditioning variable. The empowerment C(x0) depends on both the state, x0, and the time horizon, Te. However, for notational convenience, we omit all parameters from the notation except for the dependency on x0. Locally maximizing empowerment (e.g., by following its gradient over x0) guides an agent to perform actions atypical within the natural dynamics of the system. Indeed, since em- powerment measures the diversity of achievable future states, maximizing it increases this diversity (\empowers"" the agent { hence the name). Thus it is expected to be particularly useful for learning potentiating tasks [9]. Crucially, empowerment quanties the relation between the nal state and the inten- tional control, rather than the diversity of states due to the stochasticity of the system. In particular, it is not just the en- 2tropy of a passive diusion process in the state variables, but of the subprocess that the agent can actively generate. Fur- thermore, it quanties diversity due to potential future action sequences, which are not then necessarily carried out. Empowerment is typically used in the form of the empow- erment maximization principle [17], treatsC(x0) as a pseudo- utility function. At each time step, an agent chooses an action to greedily optimize its empowerment at the next time step. That is, the agent climbs up in its empowerment landscape, eventually achieving a local maximum of C: a  x(t) = argmax a2AE C  f(x(t)) +g(x(t))at0+d :(4) HereAis the set of permitted actions,  t0is a small time step used to simulate the actual behavior of the system (and which is selected independently from the time step  tused to discretize (1)). An empowerment-maximizing agent generates its behavior by repeating this action selection procedure for each decision step it takes. Crucially, no general analytical solutions or ecient algo- rithms for numerical estimation of empowerment for arbitrary dynamical systems are known, limiting adoption of the em- powerment maximization principle. Our goal is to provide a method to calculate it under specic approximations. B Empowerment in dynamical systems The linear response approximation To relate empowerment to traditional quantities used to describe dynamical systems, we assume that the control signal ain (1) is small. This is true in some of the most interesting cases, where the chal- lenge is to solve a problem with only weak controls that can- not easily \force"" a solution. Under this assumption, (1) is approximated by a linear time-variant dynamics around the trajectories of the autonomous dynamics (i.e., for a= 0). To proceed, we now introduce the following notation. We de- ne xsas thes-th step of the trajectory in the discretized deterministic approximation of the dynamics (1), given by xs=f(xs 1) +g(xs 1)as 1 (5) with x0=x0x(t). For example,  x3=f(f(f(x0) + g(x0)a0) +g(x1)a1) +g(x2)a2. We denote this recur- sive mapping from  x0to xsbyF, xs=F(x0; as 1 0). Then the sensitivity of the state at the time step sto the action at the time step rcan be calculated via the iterated dierentia- tion chain rule applied to the state derivative of the dynamics F: @xs @ar=sY =r+2rxf(x 1)g(xr); (6) whererxf(x) is thedxdxJacobian matrix, which approx- imatesfup to the linear order in the state and the control. Specically, the ( i;j)-th entry ofrxf(x) is@fi(x) @x;j, where indicesi;jstand for components of the vectors xandf. For s=r+ 1, the expression in (6) evaluates to@xr+1 @ar=g(xr). Now we dene the linear response of the sequence of the system's states xs2s1to the sequence of the agent's actions  ar2r1Fs1;s2r1;r2(x0) =@xs2 @ar2@xs2 @ar2 1:::@xs2 @ar1 @xs2 1 @ar2@xs2 1 @ar2 1:::@xs2 1 @ar1 ......:::... @xs1 @ar2@xs1 @ar2 1:::@xs1 @ar12 6666666666643 777777777775 dxsdar; (7) wheres=s2 s1+ 1,r=r2 r1+ 1,s+ T+r 1 =Te, and the entries are computed via (6). Usually we consider situations where the agent applies its controls for rtime steps, and then after a gap observes the state for ssteps. That is, s1=r2+ 1 + T, where T0 is the gap between the end of the control sequence and the start of the observations. Notice that traditional denitions of sensitivity of a dy- namical system to its controls are blocks Fs0 1;s0 2 r0 1;r0 2in this over- all sensitivity matrix, Fs1;s2r1;r2. For example, if r0 1=r0 2= 0, T0=Te 1, ands0 1=Te, thens0 2=Te, and the sensitivity matrix collapses to just the entries that measure the sensitiv- ity of the current state to the controls during the immediately preceding time step, FTe;Te 0;0(x0) =@xTe @a0. This is also the blue block of the overall sensitivity matrix, (7). With the denitions above, in the linear response regime, the eect of a sequence of (small) actions on a sequence of states, (1), becomes xs2s1=Fs1;s2r1;r2(x0)ar2r1+ ~; (8) where aand xare the reverse-time-ordered vectors of small actions and the induced deviations of states (which themselves can be vectors).Here ~ models both the total noise resulting from the integration of the process noise dfrom (1) and the noise of the subsequent observation of the state per- turbation  xs2s1, which we assume as Gaussian. Generalized Empowerment Since the entire dynamics is now linear, cf. (8), we can consider formally eects of arbitrary length sequences of actions on arbitrary length sequences of future states. In other words, we can dene the generalized empowerment , CTe;Ta;T(x0) := max p(~ ajx0)I(XTe Ta+T;ATa 1 0jx0):(9) Here,Tadenotes the number of time steps at which actions are performed,  Tis the time gap between the action se- quence and the beginning of the observation of the resulting states, and Teis the last step in that observed sequence. That is,CTe;Ta;Tmeasures the maximum mutual information be- tween a sequence of actions and a later sequence of states, rather than just one nal state, like empowerment does. Plugging in (8) into (4), we observe that computing the gen- eralized empowerment in discretized time with an arbitrary discretization step and an arbitrary time horizon Tereduces to a traditional calculation of the channel capacity of a linear Gaussian channel, though with a large number of dimensions reecting both the duration of the signal and the duration of the response. Specically, CTe;Ta;T(x0) = max i0P ii=P1 2dxX i=1ln(1 +i(x0)i): (10) 3Herei(x0) are the singular values of the appropriate sub- matrixFs0 1;s0 2 r0 1;r0 2(x0); for example, the traditional empowerment corresponds to the red-dashed submatrix in (7). Further, P is the power of the control signal  aover the whole control period, and i0 is that part of the overall power of the control signal which is associated with the i-th singular value (called channel power ). The channel power can be computed by the usual water-lling procedure [21]. Note that here we denotePas power, as per control-theoretic convention, but since we x the time interval over which it is applied, the units ofPare those of energy. As per our weak control assumption, we assume Pto be suitably small. With (10), calculation of any generalized empowerment be- comes tractable, at least in principle. This also shows explic- itly that the (generalized) empowerment is a function of the sensitivity matrix F, and with it of quantities used to char- acterize dynamics, such as the Lyapunov exponents. To computeCTe;Ta;T(x0) eciently for an arbitrary dy- namical system (1) and arbitrary long time horizons and ar- bitrary small discretization steps, we start by discretizing the time and calculating the linear response matrix F. While in this paper we do this by analytical dierentiation, numerical dierentiation can be used whenever fis unknown. We then calculate the singular values of F; this is straightforward on modern computers for dimensionalities of up to a few hundred. Finally, we apply the \water lling"" procedure to nd the set of channel powers ito match the available total power Pin (10), and from there we calculate the (generalized) empower- ment value. We will employ this approach for all examples in this paper. Connecting Generalized Empowerment to Related Quan- tities Generalized empowerment with dierent durations of action and observation sequences is related to various quan- tities describing dynamical systems, including those dening intrinsic motivation [8, 20, 23, 41]. For example, Causal En- tropic Forcing (CEF) [20] is dened as actions that maximize the entropy of future trajectories of a system. With Ta= 1 and T= 0,CTe;Ta;Tin (9) measures the immediate con- sequences of a single action on a trajectory with a xed time horizonTe. MaximizingCTe;Ta;Tis then equivalent to choos- ing actions that maximize susceptibility , and not the entropy of trajectories with a given time horizon. In other words, one can interpretCTe;1;0as a \kicked"", or agent-controllable, ver- sion of CEF, where just the rst action can be selected by the agent at any time, and uncontrolled future variability is discarded in action planning (see Fig. 1 for an illustration). Such kicked CEF corresponds to the green submatrix in (7). Now consider the top right corner (blue) of (7) with Te= Ta= 1, or, equivalently, s0 2=s2ands0 1=s0 2 1. In the limit of a very long horizon, s2!1 , the appropriate submatrix of Fis lim s2!1 @xs2 @ar1@xs2 @ar1y!1 s2 ; (11) whereyis the transpose, and@xs2 @ar1is given by (6). In the special case that the control gain is the identity, g(x) =x, the logarithm of the eigenvalues of  reduces to the usual characteristic Lyapunov exponents of the dynamical system [42]. However, once a more general control gain is applied, the action-controlled perturbation, ar1may be able to aect only a part of the state space. This means that  not onlyis a generalized empowerment with specic indices, but it is also a specialization of the concept of Lyapunov exponents to the controllable subspace. Thus we refer to the log-spectrum of  as the control Lyapunov exponents , cf. Fig. 1. In summary, (9) and the linearization, (7), provide a unied view of various sensitivties of the dynamics to the controls, and hence on various versions of intrinsic motivation. C Intrinsic motivation in power-constrained agents An agent controlling a system with unconstrained actions can trivially reach any state in a controllable dynamical sys- tem [43] by simply forcing their desired outcome without so- phisticated control. Thus to render the setup interesting, we consider only power-constrained, or weak agents. To show that empowerment maximization, in the linearized regime, is an ecient control principle, we use it to stabilize a family of inverted pendula (single pole, double pole, and cart-pole), which are simple, paradigmatic models of important phenom- ena, such as human walking [44]. Solutions for the stabilization problem are known. They require to accumulate energy by swinging the pendulum back and forth into resonance without overshooting and then to keep the pendulum upright. When details of the system are not specied a priori , this solution needs to be learned by the agent. Finding such an indirect control policy by tra- ditional reinforcement learning is nontrivial [3], since the in- creasing oscillations require a long time for the balancing to take place, and the acquisition of informative rewards indi- cating success is signicantly delayed. As we will show, it is precisely in such situations that intrinsic motivation based on empowerment is especially useful, since it is determined from only comparatively local properties of the dynamics along the present trajectory and its potential future variations. Inverted pendulum We start with a relatively simple task of swinging up and stabilizing an inverted pendulum without an external reward. With an angle of (in radians) from the upright vertical, the equations of motion of the pendulum are d(t) d_(t) = _(t)dt g lsin((t))dt+da(t) ml2+dW(t) ml2! ; (12) where _is the angular velocity of the pendulum, mis its mass, lis the length, ais the torque applied by the agent, gis the free fall acceleration, and dW(t) is a Wiener process. We apply a (stochastically chosen) control signal a(t) for the duration Teand observe the nal state ~=+ ~obs, where ~obsis the standard Gaussian observation noise at the nal state. Empowerment is then given by the maximally achievable mutual information between a(t) and ~at a given power level for a(t), i.e., the channel capacity between the two. The observation noise eectively determines the resolution, at which the end state is considered. Note that in our linear approximation the process noise dW(t) undergoes the same gain sequence as the control signal, and thus it rescales the empowerment landscape and changes the behavior of the sys- tem. Thus to compare empowerment values in dierent states, it is essential to include the observation noise. We now apply our empowerment-based control protocol, (4), to the inverted pendulum. We calculate the empowerment landscape by using the time-discretized version of Eqs. (1, 12). For this, we map the deterministic part of the dynamics ( f;g in (1)) onto discrete time as per (5). We then compute the 4Figure 2: Intrinsic motivation based control in the power-constrained regime. Top row: generalized empowerment landscapes in the linear approximation for empowerment (left), controlled Lyapunov exponent (middle), and kicked CEF (right) versions of the problem, plotted against (horizontal axis) and _(vertical axis), measured in rad and rad =s, respectively. Black dots in each panel are the nal state, and white lines are the trajectories of the pendulum, starting at the bottom denoted by the red dots. Bottom row: the control signals chosen from the generalized empowerment maximization as a function of time. Here the time horizon is te= 0:5s. channel capacity by applying (10) using the singular values from (8), where states are given by ( ;_)2Rdx, and actions consist of applying a torque a. The landscapes for the orig- inal empowerment, the controlled Lyapunov exponent, and the kicked CEF versions of the problem, all with the time horizons of te= 0:5 s and the discretization  t= 10 3are shown in Fig. 2. Then, from each state, we choose the con- trol action to greedily optimize the generalized empowerment. The panels in the upper row in this Figure also show trajec- tories obtained this way. The lower row shows time traces of the control signal derived from the generalized empower- ment maximization. In all cases, initially, the agent drives the pendulum at the maximum allowable torque, which we set to be power-constrained to 1 N m. Around 13, 10, and 10 seconds after the start (for the three versions of the em- powerment, respectively), the pendulum accumulates enough energy to reach the vertical, and the agents reduce the torques to very small values, a1 N m, which are now sucient to keep the pendulum in the upright position and prevent it from falling. It is striking that the generalized empowerment land- Figure 3: Convergence of the method for  t!0 and te= 0:5s. As time resolution is rened fourfold at every stage, one arrives at a well-dened value for the empowerment es- timation as  t!0. The numerical stability of this limit approximation is consistent throughout the landscape.scapes and their induced trajectories are qualitatively similar to those that would be generated by an optimal value func- tion, derived by standard optimal control techniques based on a reward specically designed to achieve the top position [3]. In our analysis, we chose a particular discretization  t= 10 3s, and we need to show that our results depend only weekly on this choice. For this, we repeat our analysis at dif- ferent t. Figure 3 shows the dependence of the maximum value of the original empowerment (black dot in left panel of Fig. 2) on  t. To the extent that the estimate converges to a well-dened number linearly as  t!0, the discrete time dy- namics provides a consistent approximation to the continuous time dynamics. Double Pendulum Now we show that the empowerment maximization formalism is capable of dealing with more chal- lenging problems, such as a power-constrained control of a (potentially chaotic) double pendulum [16], Fig. 4, with equa- tions of motion: d1(t) = 1 d1(t) d2(t)2(t) +1(t) ; (13) d2(t) =1 m2`2c2+I2 d2 2(t) d1(t) da(t) +dW(t) +d2 2(t) d1(t)1(t)  m2`1`c2_1(t)2sin2(t) 2(t) ; with d1(t) =m1`2 c1+m2(`2 1+`2 c2+ 2`1`c2cos2(t)) +I1+I2; d2(t) =m2(`2 c2+`1`c2cos2(t)) +I2; 1(t) = m2`1`c2_(t)2sin2(t) 2m2`1`c2_2(t)_1(t) sin2(t) + (m1`c1+m2`1)gcos1(t) +2(t); 2(t) =m2`c2gcos(1(t) +2(t)): We add Wiener noise, dW(t), and permit the controller to apply a scalar control signal ja(t)j1, at the joint between the two links. In the equations of motion, mi,`i,`ci, andIi stand for the mass, the length, the length to center of mass, and the moment of inertia of the i-th link,i2[1;2], respec- 5Figure 4: Top left: Double pendulum with control torque on the joint between the links with dynamics given by (13) Top right: Slices through the empowerment landscape of a double pendulum. Each subplot shows a particular slice in the 4D landscape, when two other coordinates are zero. For example, the plot with axes _2,_1is shown for 2= 0 rad and 1= 0 rad. Bottom: Traversing the state space of the double pendulum according to (4). The rst and the second 15s are shown with dierent scale for the instantaneous empowerment. The initial and the nal positions are both links down and both links up, respectively. Torque is applied to the middle joint only. tively. Figure 4 shows the landscape for the original empow- erment for selected slices of the phase space. This landscape is more complex than for the single-pendulum. Nonetheless it retains the property that, following the local gradient in the state space directly, one ultimately reaches the state of the maximum empowerment, which is precisely where both links of the pendulum are balanced upright. The vertical position, however, is a priori not sucient to guarantee the balanc- ing since the control only applies torque at the joint linking the pendulum halves. That is, the controller cannot move the pendulum in arbitrary directions through the state space. Surprisingly, this concern notwithstanding, the algorithm still balances the pendulum, cf. Fig. 4. Cart-Pole We have additionally veried that the empower- ment maximization also balances an inverted pendulum on a moving cart, cf. Fig.5. Here the control signal (force) is ap- plied to the cart. Thus the pendulum is now aected only indirectly. The dynamics of this system is: dx(t) =msin(t)(`_2(t) +gcos(t)) +da(t) +dW(t) M+msin2(t);(14) d(t) = da(t) cos(t) m`_2(t) cos(t) sin(t)  (M+m)gsin(t); wherex(t),(t),m,M,`,g,ja(t)j1 are thexcoordinate of the center of mass of the cart, the angle of the pole, the polemass, the cart mass, the pole length, the free fall acceleration, and the force applied to the cart. Discussion In this study, we focused on a class of intrinsic motivation models that mimic decision-making abilities of biological or- ganisms in various situations without explicit reward signals. We used an information-theoretic formulation in which the controller starts with knowledge of the (stochastic) dynami- cal equations describing the agent and the environment, and then selects actions that \empower"" the agent. That is, the controller improves its ability to aect the system in the fu- ture, as measured by the mutual information between the ac- tion sequence and the subsequent responses. This leads the system to the most sensitive points in the state space, which we showed solves a problem known to be dicult for simple reinforcement learning algorithms: balancing inverted pen- dula. Depending on which subsets of the past actions and future responses are used to drive the intrinsic motivation, our approach interpolates between the original formulation of empowerment maximization, maximization of the \kicked"" version of Causal Entropic Forcing, and maximization of the \controlled"" subset of the Lyapunov exponents of the agent- environment pair. This provides insight into which properties of the dynamical system are responsible for the behaviors pro- 6Figure 5: Left: Cart-Pole system with control force, ~ a(t), applied to the cart only, which moves on the rail (or on the edge of a table), allowing the pole to rotate in the x-y plane. Its dynamics is given by (14). Right: Traversing the state space of the pendulum on a cart according to empowerment maximization. The initial and the nal state of the pole are down and up, respectively. The horizontal axis is time in seconds t2[0;20]s. duced by these dierent motivation functions. One big challenge in using information-theoretic quantities is computing them, which can be dicult to do either analyt- ically or from data. Our paper makes a signicant contribu- tion to solving this problem in the context of empowerment by providing an explicit algorithm for computing various versions of empowerment, for arbitrary lengths of pasts and futures, using the small noise/small control approximation to the dy- namics, while still treating the dynamics as nonlinear. This is often the most interesting regime, modeling weak, power- constrained controllers. Crucially, our algorithm is local, so that climbing up the empowerment gradient only requires es- timation of the dynamics in the vicinity of the current state of the system. This should be possible in real control appli- cations by using the data directly, possibly with the help of deep neural networks to approximate the relevant dynamical landscapes [45{47]. Therefore, knowing the exact form of the dynamical system, which could be a potential limitation of our approach, is not strictly required. This opens up oppor- tunities for scaling our method to more complex scenarios. Our work suggests that, in addition to the Lyapunov spec- trum, dened via the trajectory divergence in time due to a small arbitrary perturbation, one may want to consider the optimal Lyapunov spectrum, where the initial perturbation is optimally aligned with the controllable directions in the dy- namics. We defer a systematic study of optimal Lyapunov spectra to future work. A potential extension of our analysis relates to social in- teractions. Interacting agents have their own intrinsic moti- vations and aect each other's ability to achieve their goals. Understanding how multiple agents interact, each trying to empower itself in the presence of others, and whether and when this leads to cooperation or conict is a promising area for future research. Crucially, the ability to aect someone else's empowerment may provide insight into what distin- guishes social interactions from purely physical interactions among nearby individuals. Acknowledgements ST was supported in part by Califor- nia State University, and the College of Engineering at SJSU. IN was supported in part by the Simons Foundation Investi- gator award, the Simons-Emory Consortium on Motor Con- trol, and NIH grant 2R01NS084844. DP acknowledges partial support by the EC H2020-641321 socSMCs FET Proactive project and the Pazy Foundation.References [1] P.-Y. Oudeyer and F. Kaplan. What is intrinsic motiva- tion? a typology of computational approaches. Frontiers in neurorobotics , 1:6, 2009. [2] R. S. Sutton and A. G. Barto. Reinforcement learning: An introduction . MIT press, 2018. [3] K. Doya. Reinforcement learning in continuous time and space. Neural computation , 12(1):219{245, 2000. [4] S. Mohamed and D. J. Rezende. Variational informa- tion maximisation for intrinsically motivated reinforce- ment learning. In Advances in neural information pro- cessing systems , pages 2125{2133, 2015. [5] K. Gregor, D J. Rezende, and D. Wierstra. Variational intrinsic control. arXiv preprint arXiv:1611.07507 , 2016. [6] K. Baumli, D. Warde-Farley, S. Hansen, and V. Mnih. Relative variational intrinsic control. Proceedings of the AAAI Conference on Articial Intelligence , 35(8):6732{ 6740, 2021. [7] T. Kwon. Variational intrinsic control revisited. Interna- tional Conference on Learning Representations, (ICLR), 2021, 2021. [8] A. Sharma, S. Gu, S Levine, V. Kumar, and K. Hausman. Dynamics-aware unsupervised discovery of skills. arXiv preprint arXiv:1907.01657 , 2019. [9] A. Sharma, M Ahn, S. Levine, V. Kumar, K. Haus- man, and S. Gu. Emergent real-world robotic skills via unsupervised o-policy reinforcement learning. RSS, Robotics: Science and Systems 2020 , 2020. [10] B. Eysenbach, A Gupta, J. Ibarz, and S. Levine. Di- versity is all you need: Learning skills without a reward function. International Conference on Learning Repre- sentations, (ICLR), 2019 , 2018. [11] R. Houthooft, X. Chen, Y. Duan, J. Schulman, F. De Turck, and P. Abbeel. Vime: Variational infor- mation maximizing exploration. Advances in neural in- formation processing systems , 29, 2016. [12] J. Achiam, H. Edwards, D. Amodei, and P. Abbeel. Variational option discovery algorithms. arXiv preprint arXiv:1807.10299 , 2018. [13] J. Choi, A. Sharma, H. Lee, S. Levine, and S. S. Gu. Vari- ational empowerment as representation learning for goal- conditioned reinforcement learning. International Con- ference on Machine Learning , pages 1953{1963, 2021. [14] C. Salge, C. Glackin, and D. Polani. Empowerment{ an introduction. In Guided Self-Organization: Inception , pages 67{114. Springer, 2014. [15] A. S. Klyubin, D Polani, and C. L. Nehaniv. Em- powerment: A universal agent-centric measure of con- trol. IEEE Congress on Evolutionary Computation, 2005, 1:128{135, 2005. [16] T. Jung, D. Polani, and P. Stone. Empowerment for con- tinuous agent|environment systems. Adaptive Behavior , 19(1):16{39, 2011. 7[17] A. S. Klyubin, D. Polani, and C. L. Nehaniv. Keep your options open: An information-based driving prin- ciple for sensorimotor systems. PLoS ONE , 3(12):e4018, Dec 2008. [18] R. Zhao, P. Abbeel, and S. Tiomkin. Ecient empower- ment estimation for unsupervised stabilization. Interna- tional Conference Learning Representations , 2020. [19] R. Zhao, S. Tiomkin, and P. Abbeel. Dynamical system embedding for ecient intrinsically motivated articial agents. Advances in Neural Information Processing Sys- tems, (NeurIPS), DeepRL , 2019. [20] A. D. Wissner-Gross and C. E. Freer. Causal entropic forces. Physical review letters , 110(16):168702, 2013. [21] T. M. Cover and J. A. Thomas. Elements of information theory . John Wiley & Sons, 2012. [22] A. S. Klyubin, D. Polani, and C. L. Nehaniv. Empow- erment: A universal agent-centric measure of control. In 2005 IEEE Congress on Evolutionary Computation , vol- ume 1, pages 128{135. IEEE, 2005. [23] B. Eysenbach, A. Gupta, J. Ibarz, and S. Levine. Di- versity is all you need: Learning skills without a reward function. In ICLR , 2019. [24] Y. Du, S. Tiomkin, E. Kiciman, D. Polani, P. Abbeel, and A. D. Dragan. Ave: Assistance via empowerment. Neural Information Processing Systems, NeurIPS 2020 , 2020. [25] C. Salge and D. Polani. Empowerment as replacement for the three laws of robotics. Frontiers in Robotics and AI, 4:25, 2017. [26] W. Bialek, I. Nemenman, and N. Tishby. Predictabil- ity, complexity, and learning. Neural computation , 13(11):2409{2463, 2001. [27] C. Holmes and I. Nemenman. Estimation of mutual infor- mation for real-valued data with error bars and controlled bias. Phys Rev E , 100:022404, 2019. [28] T. Jung, D. Polani, and P. Stone. Empowerment for con- tinuous agent|environment systems. Adaptive Behavior , 19(1):16{39, 2011. [29] R. Zhao, K. Lu, P. Abbeel, and S. Tiomkin. Ecient em- powerment estimation for unsupervised stabilization. In International Conference on Learning Representations, ICLR , 2021. [30] A. D. Wissner-Gross and C. E. Freer. Causal entropic forces. Physical review letters , 110(16):168702, 2013. [31] A. Y. Ng, D. Harada, and S. Russell. Policy invariance under reward transformations: Theory and application to reward shaping. [32] P. Dayan and G. E. Hinton. Feudal reinforcement learn- ing. In Advances in neural information processing sys- tems, pages 271{278, 1993. [33] T. D. Kulkarni, K. Narasimhan, A. Saeedi, and J. Tenen- baum. Hierarchical deep reinforcement learning: Inte- grating temporal abstraction and intrinsic motivation. InAdvances in Neural Information Processing Systems, NeurIPS , 2016. [34] Y. Burda, H. Edwards, D. Pathak, A. Storkey, T. Darrell, and A. A. Efros. Large-scale study of curiosity-driven learning. In ICLR , 2019. [35] D. Pathak, P. Agrawal, A. A. Efros, and T. Darrell. Curiosity-driven exploration by self-supervised predic- tion. In ICML , 2017. [36] M. Karl, M. Soelch, P. Becker-Ehmck, D. Benbouzid, P. van der Smagt, and J. Bayer. Unsupervised real-time control through variational empowerment. 2018. [37] C. Salge, C. Glackin, and D. Polani. Approximation of empowerment in the continuous domain. Advances in Complex Systems , 16(02n03):1250079, 2013. [38] C. Salge, C. Glackin, and D. Polani. Empowerment{ an introduction. In Guided Self-Organization: Inception , pages 67{114. Springer, 2014. [39] T. Anthony, D. Polani, and C. L. Nehaniv. General self-motivation and strategy identication: Case studies based on sokoban and pac-man. IEEE Transactions on Computational Intelligence and AI in Games , 6(1):1{17, 2014.[40] H. J. Charlesworth and M. S. Turner. Intrinsically mo- tivated collective motion. Proceedings of the National Academy of Sciences , 116(31):15362{15367, 2019. [41] J. Schmidhuber. Formal theory of creativity, fun, and intrinsic motivation (1990{2010). IEEE Transactions on Autonomous Mental Development , 2(3):230{247, 2010. [42] H. D. Abarbanel, R. Brown, and M. B. Kennel. Local lya- punov exponents computed from observed data. Journal of Nonlinear Science , 2(3):343{365, 1992. [43] L. C. Evans. An introduction to mathematical optimal control theory version 0.2. 1983. [44] A. D. Kuo. The six determinants of gait and the inverted pendulum analogy: A dynamic walking perspective. Hu- man movement science , 26(4):617{656, 2007. [45] B. C. Daniels and I. Nemenman. Automated adaptive inference of phenomenological dynamical models. Nature communications , 6(1):1{8, 2015. [46] S. L. Brunton, J. L. Proctor, and J. N. Kutz. Discovering governing equations from data by sparse identication of nonlinear dynamical systems. Proceedings of the national academy of sciences , 113(15):3932{3937, 2016. [47] B. Chen, K. Huang, S. Raghupathi, I. Chandratreya, Q. Du, and H. Lipson. Automated discovery of funda- mental variables hidden in experimental data. Nature Computational Science , 2(7):433{442, 2022. 8"
6,https://arxiv.org,2301.00006,10.48550/arXiv.2301.00006,"Hyeonsu Jeong, Hye Won Chung",Recovering Top-Two Answers and Confusion Probability in Multi-Choice Crowdsourcing,"Crowdsourcing has emerged as an effective platform for labeling large amounts of data in a cost- and time-efficient manner. Most previous work has focused on designing an efficient algorithm to recover only the ground-truth labels of the data. In this paper, we consider multi-choice crowdsourcing tasks with the goal of recovering not only the ground truth, but also the most confusing answer and the confusion probability. The most confusing answer provides useful information about the task by revealing the most plausible answer other than the ground truth and how plausible it is. To theoretically analyze such scenarios, we propose a model in which there are the top two plausible answers for each task, distinguished from the rest of the choices. Task difficulty is quantified by the probability of confusion between the top two, and worker reliability is quantified by the probability of giving an answer among the top two. Under this model, we propose a two-stage inference algorithm to infer both the top two answers and the confusion probability. We show that our algorithm achieves the minimax optimal convergence rate. We conduct both synthetic and real data experiments and demonstrate that our algorithm outperforms other recent algorithms. We also show the applicability of our algorithms in inferring the difficulty of tasks and in training neural networks with top-two soft labels.","Recovering Top-Two Answers and Confusion Probability in Multi-Choice Crowdsourcing Hyeonsu Jeong1Hye Won Chung1 Abstract Crowdsourcing has emerged as an effective plat- form for labeling large amounts of data in a cost- and time-efficient manner. Most previous work has focused on designing an efficient algorithm to recover only the ground-truth labels of the data. In this paper, we consider multi-choice crowdsourc- ing tasks with the goal of recovering not only the ground truth, but also the most confusing answer and the confusion probability. The most confus- ing answer provides useful information about the task by revealing the most plausible answer other than the ground truth and how plausible it is. To theoretically analyze such scenarios, we propose a model in which there are the top two plausi- ble answers for each task, distinguished from the rest of the choices. Task difficulty is quantified by the probability of confusion between the top two, and worker reliability is quantified by the probability of giving an answer among the top two. Under this model, we propose a two-stage inference algorithm to infer both the top two an- swers and the confusion probability. We show that our algorithm achieves the minimax optimal convergence rate. We conduct both synthetic and real data experiments and demonstrate that our algorithm outperforms other recent algorithms. We also show the applicability of our algorithms in inferring the difficulty of tasks and in training neural networks with top-two soft labels. 1. Introduction Crowdsourcing has been widely adopted to solve a large number of tasks in a time- and cost-efficient manner with the help of human workers. In this paper, we consider 1School of Electrical Engineering, KAIST, Daejeon, Korea. Correspondence to: Hyeonsu Jeong <hsjeong1121@kaist.ac.kr >, Hye Won Chung <hwchung@kaist.ac.kr >. Proceedings of the 40thInternational Conference on Machine Learning , Honolulu, Hawaii, USA. PMLR 202, 2023. Copyright 2023 by the author(s).multiple-choice tasks, where a worker is asked to provide a single answer among multiple choices. Some examples are as follows: 1) Using crowdsourcing platforms such as MTurk, we solve object counting or classification tasks on a large collection of images. Answers can be noisy either due to the difficulty of the scene or due to unreliable workers making random guesses. 2) Scores are collected from referees for papers submitted to a conference. For certain papers, scores can vary widely among reviewers, either due to the inherent nature of the paper (clear pros and cons) or due to the reviewer‚Äôs subjective interpretation of the scoring scale (Stelmakh et al., 2019; Liu et al., 2022). In the above scenarios, the answers provided by human workers may not be consistent among themselves, not only due to the presence of unreliable workers, but also due to the inherent difficulty of the tasks. In particular, for multiple choice tasks, there can exist plausible answers other than the ground truth, which we call confusing answers .1For tasks with confusing answers, even reliable workers may provide wrong answers due to confusion. Thus, we need to decompose the two different causes of wrong answers: low reliability of workers and confusion due to task difficulty. However, most previous models of multi-choice crowdsourc- ing do not adequately model the errors from confusion. For example, the single-coin Dawid-Skene model (Dawid & Skene, 1979), which is the most widely studied model in the literature, assumes that a worker is associated with a single skill parameter that is fixed across all tasks, which models the probability of giving a correct answer for every task. Under this model, any algorithm that infers the worker‚Äôs skill would count a confused labeling as the worker‚Äôs error and lower its accuracy estimate for the worker, resulting in a wrong estimate of the worker‚Äôs true skill level. To model the effect of confusion in multi-choice crowd- sourcing problems, we propose a new model in which each task can have a confusing answer other than the ground truth, with a different confusion probability across tasks. Task dif- ficulty is quantified by the confusion probability between 1This phenomenon is evident on public datasets: for ‚ÄòWeb‚Äô dataset (Zhou et al., 2012), which consists of five-choice tasks, the most dominant top-two answers of each task account for 80% of the total answers, and the ratio between the top two is 2.4:1. 1arXiv:2301.00006v2  [cs.HC]  31 May 2023Recovering Top-Two Answers and Confusion Probability in Multi-Choice Crowdsourcing the top two plausible answers, and worker skill is modeled by the probability of giving an answer among the top two, to distinguish reliable workers from pure spammers who just give random guesses among possible choices. We justify the proposed top-two model with public datasets. Under this new model, we aim to recover both the ground truth and the most confusing answer with the confusion probability, which indicates how plausible the recovered ground truth is compared to the most confusing answer. We provide an efficient two-stage inference algorithm to recover the top-two plausible answers and the confusion probability. The first stage of our algorithm uses the spectral method to obtain an initial estimate for the top-two answers and the confusion probability, and the second stage uses this initial estimate to estimate the workers‚Äô reliabilities and to refine the estimates for the top-two answers. Our algorithm achieves the minimax optimal convergence rate. We then perform experiments comparing our method to recent crowd- sourcing algorithms on both synthetic and real datasets, and show that our method outperforms other methods in recov- ering top-two answers. This result demonstrates that our model better explains the real-world datasets, including er- rors due to confusion. Our code is available at https:// github.com/Hyeonsu-Jeong/TopTwo . Our main contributions can be summarized as follows. ‚Ä¢Top-two model: We propose a new model for multi- choice crowdsourcing tasks, where each task has top- two answers, and the difficulty of the task is quantified by the confusion probability between the top two plau- sible answers. We justify the proposed model by ana- lyzing six public datasets and showing that the top-two structure explains the real datasets well. ‚Ä¢Inference algorithm and its application: We propose a two-stage algorithm that recovers the top-two an- swers and the confusion probability of each task at the minimax optimal convergence rate. We demonstrate the potential applications of our algorithm not only in crowdsourced labeling, but also in two more applica- tions: (i) quantifying task difficulty, and (ii) training neural networks for classification with soft labels that include the top-two information and the task difficulty. 2. Related works Dawid-Skene(D&S) model. In crowdsourcing (Welinder et al., 2010; Liu & Wang, 2012; Demartini et al., 2012; Ay- din et al., 2014; Demartini et al., 2012), one of the most widely studied models is the Dawid-Skene (D&S) model (Dawid & Skene, 1979). In this model, each worker is as- sociated with a single confusion matrix, fixed across tasks, that models the probability of giving a label b‚àà[K]for the true label a‚àà[K]for a K-ary classification task. Inthe single-coin D&S model, the model is further simplified such that each worker has a fixed skill level regardless of the true label or task. Under the D&S model, various methods have been proposed to estimate the confusion matrix or skill of workers by spectral methods (Zhang et al., 2014; Dalvi et al., 2013; Ghosh et al., 2011; Karger et al., 2013), iterative algorithms (Karger et al., 2014; 2011; Li & Yu, 2014; Liu et al., 2012; Ok et al., 2016), or rank-1 matrix completion (Ma et al., 2018; Ma & Olshevsky, 2020; Ibrahim et al., 2019). The estimated skill can be used to infer the ground truth answer by approximating maximum likelihood (ML)- type estimators (Gao & Zhou, 2013; Gao et al., 2016; Zhang et al., 2014; Karger et al., 2013; Li & Yu, 2014; Raykar et al., 2010; Smyth et al., 1994; Ipeirotis et al., 2010; Berend & Kontorovich, 2014). Unlike the D&S models, our model allows each worker to make errors over tasks with different probabilities due to confusion. Thus, our algorithm needs to estimate not only the worker‚Äôs skill, but also the task dif- ficulty. Since the number of tasks is often much larger than the number of workers in practice, estimating task difficulty is much more challenging than estimating worker skill. We provide a statistically-efficient algorithm to estimate the task difficulty and use this estimate to infer the top-two answers. Task Dificculty. We also remark that there have been some recent attempts to model task difficulty in crowdsourcing (Khetan & Oh, 2016; Shah et al., 2020; Krivosheev et al., 2020; Shah & Lee, 2018; Bachrach et al., 2012; Li et al., 2019; Tian & Zhu, 2015). However, these works are either restricted to binary tasks (Khetan & Oh, 2016; Shah et al., 2020; Shah & Lee, 2018) or focus on grouping confusable classes (Krivosheev et al., 2020; Li et al., 2019; Tian & Zhu, 2015). Our result, on the other hand, applies to any set of multi-choice tasks of which the choices are not necessarily restricted to a fixed set of classes/labels. Modeling confusion. There is a growing interest in the machine learning community in utilizing soft labels to train deep neural networks. Various methods have been proposed to generate soft labels of training data, e.g., by using mixup (Zhang et al., 2017; Sohn et al., 2022) or by using the output of trained models (Sabetpour et al., 2021). Also, CIFAR- 10H (Peterson et al., 2019) dataset was generated by using all the human annotations from the data collection step as soft labels on images. Our algorithm estimates the task difficulty and the top two answers, which can produce a new form of soft label that can be used in this line of work, as will be discussed in Sec. 6.3. Notation. For a vector x,xirepresents the i-th component ofx. For a matrix M,Mijrefers to the (i, j)th entry of M. For any vector x, its‚Ñì2and‚Ñì‚àû-norm are denoted by ‚à•x‚à•2 and‚à•x‚à•‚àû, respectively. We follow the standard definitions of asymptotic notations, Œò(¬∑),O(¬∑),o(¬∑), and ‚Ñ¶(¬∑). 2Recovering Top-Two Answers and Confusion Probability in Multi-Choice Crowdsourcing 3. Model and Problem Setup We consider a crowdsourcing model to infer the top two most plausible answers among Kchoices for each task. There are nworkers and mtasks. For each task j‚àà[m] := {1, . . . , m }, we denote the correct answer by gj‚àà[K]and the next plausible or the most confusing answer by hj‚àà[K]. We call the pair (gj, hj)the top two answers for the task j‚àà[m]. Letp‚àà[0,1]nandq‚àà(1/2,1]mbe parameters modeling the reliability of the workers and the difficulty of the tasks, respectively. For each pair of (i, j), thej-th task is assigned to the i-th worker independently with probability s. We use a matrix A‚ààRn√ómto represent the responses of the workers, where Aij= 0if the j-th task is not assigned to the i-th worker, and if it is assigned, Aijis equal to the received label. The distribution of Aijis determined by the worker reliability piand the task difficulty qjas follows: Aij=Ô£± Ô£¥Ô£¥Ô£¥Ô£≤ Ô£¥Ô£¥Ô£¥Ô£≥gj, w.p.s  piqj+1‚àípi K , hj, w.p.s  pi(1‚àíqj) +1‚àípi K , b‚àà[K]\{gj, hj},w.p.s 1‚àípi K , 0, w.p.1‚àís. (1) Here piis the reliability of the i-th worker in giving the answer from the most plausible top two (gj, hj). Ifpi= 0, the worker is considered a spammer, giving random answers among Kchoices, and a larger value of piindicates a higher reliability of the worker. The parameter qjrepresents the inherent difficulty of the task jin discriminating between the top two answers: for an easy task, qjis closer to 1, and for a hard task, qjis closer to 1/2. We call qjthe confusion probability. Our goal is to recover the top two answers (gj, hj)for all j‚àà[m]with high probability at the minimum possible sampling probability s. We assume that the model parameters (p,q)are unknown. We propose the top-two model to reflect common character- istics of public crowdsourcing datasets, as outlined in Appx. ¬ßA. The most important observation is that the top-two an- swers dominate the overall answers, and only the second dominant answer has an incidence rate comparable to the ground truth. In other words, the incidence rate of the sec- ond answer falls within the one-sigma range of the ground truth, indicating a significant overlap. However, such an overlap is not observed with the third or fourth answers. This suggests that the assumption of a unique ‚Äúconfusing an- swer‚Äù is adequate to model confusion due to task difficulty. More details can be found in Appx. ¬ßA. Binary conversion. We provide the main observation on the structure of A, which will be used to design algorithms for estimating the top two plausible answers (gj, hj)and the confusion probability qjforj‚àà[m]. The K-ary task can be decomposed into (K‚àí1)-binary tasks as follows (Karger et al., 2013): define A(k)for1‚â§k < K such that the(i, j)-th entry A(k) ijindicates whether the original answer Aijis greater than kor not, i.e., A(k) ij=‚àí1if1‚â§Aij‚â§k; A(k) ij= 1ifk < A ij‚â§K; and A(k) ij= 0ifAij= 0. We show that E[A(k)]is a rank-1 matrix and the singular value decomposition (SVD) of E[A(k)]can reveal the top-two answers {(gj, hj)}m j=1and the confusion probability vector q. Proposition 1. For each 1‚â§k < K , the bi- nary mapped matrix A(k)‚àà {‚àí 1,0,1}n√ómsatisfies E[A(k)]‚àís(K‚àí2k) K1n√óm= 2sp(r(k))‚ä§,where r(k)= [r(k) 1¬∑¬∑¬∑r(k) m]‚ä§is defined as Case I: gj> hj r(k) j:=Ô£± Ô£¥Ô£≤ Ô£¥Ô£≥k Kwhere k < h j; k K‚àí(1‚àíqj)where hj‚â§k < g j; k K‚àí1 where gj‚â§k, Case II: gj< hj r(k) j:=Ô£± Ô£¥Ô£≤ Ô£¥Ô£≥k Kwhere k < g j; k K‚àíqjwhere gj‚â§k < h j; k K‚àí1 where hj‚â§k. The proof of Proposition 1 is available in Appendix ¬ßF. By defining ‚àÜr(k) j:=r(k) j‚àír(k‚àí1) j fork‚àà[K]withr(0) j:= 0 andr(K) j:= 0 for all j, we have ‚àÜr(k) j=Ô£± Ô£¥Ô£≤ Ô£¥Ô£≥1 K‚àíqj where k=gj, 1 K‚àí(1‚àíqj)where k=hj, 1 Kotherwise .(2) Note that ‚àÜr(k) jhas its minimum at k=gjand its second smallest value at k=hjforqj‚àà(1/2,1]. If one can specify gj, the task difficulty qjcan also be found from 1 K‚àí‚àÜr(gj) j. In the next section, we will use this structure ofr(k)fork‚àà[K]to infer the top two answers and the confusion probability.2 Remark 1. Our top-two model can be generalized to top- T (T‚â•2) model, where it is assumed that for each task there areT(‚â§K)plausible answers with the associated confu- sion probabilities with respect to the ground truth. Even for this generalized model, we can define binary-converted observation matrices A(k),k‚àà[K], which enjoy the rank- 1 structure, and prove the results similar to Proposition 1, showing that the top Tplausible answers and the associ- ated confusion probabilities can be inferred using the rank-1 structure. More details can be found in the Appendix A.2. 2We assume that Œ∑‚àön‚â§ ‚à•p‚à•2‚â§‚àönfor some Œ∑ >0, which ensures that there are only o(n)spammers ( pi= 0). We also assume that ‚à•r(k)‚à•2= Œò(‚àöm)for every k‚àà[K], which can be easily satisfied except for exceptional cases from (2). 3Recovering Top-Two Answers and Confusion Probability in Multi-Choice Crowdsourcing 4. Proposed Algorithm In this section, we present an algorithm to estimate the top two answers {(gj, hj)}m j=1and the confusion probability vector q. Our algorithm consists of two stages. In Stage 1, we compute an initial estimate of the top two answers and the confusion probability q. In Stage 2, we estimate the worker reliability vector pby using the result of the first stage, and use the estimated pandqto refine our estimates for the top two answers. We randomly split the entires of the original response matrix A‚ààRn√ómintoA1‚ààRn√ómand A2‚ààRn√ómwith probability s1and1‚àís1, respectively, and use only A1for stage 1 and (A1,A2)for stage 2. 4.1. Stage 1: Initial estimates using SVD The first stage of our algorithm is presented in Algorithm 1. In this stage, we use the data matrix A1‚ààRn√ómto estimate the left singular vector p‚àó:=p/‚à•p‚à•2and the scaled right singular vector ‚à•p‚à•2r(k)ofE[A(k)]for all k‚àà[K], which are then used to infer both the top two answers and the confusion probability by using (2). The first stage begins with randomly splitting the entries ofA1‚ààRn√ómagain into two independent matrices B‚ààRn√ómandC‚ààRn√ómwith equal probabilities. We then convert BandCinto(K‚àí1)-binary matrices B(k)andC(k)for1‚â§k < K , defined as B(k) ij=‚àí1 if1‚â§Bij‚â§k;B(k) ij= 1 ifk < B ij‚â§K; and B(k) ij= 0 ifBij= 0, and similarly for C(k). Define X(k)andY(k)asX(k):=B(k)‚àís‚Ä≤(K‚àí2k) K1n√ómand Y(k):=C(k)‚àís‚Ä≤(K‚àí2k) K1n√ómfors‚Ä≤=s¬∑s1/2. We have E[X(k)] =E[Y(k)] =s‚Ä≤p(r(k))‚ä§from Proposition 1. We use X(k)andY(k)to estimate p‚àó:=p/‚à•p‚à•2and ‚à•p‚à•2r(k), respectively. The estimators are denoted by u(k) andv(k), respectively. We define u(k)as the left singular vector of X(k)with the largest singular value. Sign ambi- guity of the singular vector is resolved by defining u(k)as the one between {u(k),‚àíu(k)}in which at least half of the entries are positive. After trimming abnormally large com- ponents of u(k)and defining the trimmed vector as Àúu(k), we calculate v(k):=1 s‚Ä≤(Y(k))‚ä§Àúu(k), which is an estimate for ‚à•p‚à•2r(k). By using v(k)for1‚â§k < K , we get estimates for top-two answers (ÀÜgj,ÀÜhj)as in (3)by using the relation in(2). Lastly, we estimate ‚à•p‚à•2and use v(k)/‚à•p‚à•2‚âàr(k) to estimate the confusion probability vector qas in (4). 4.2. Stage 2: Plug-in Maximum Likelihood Estimator (MLE) The second stage uses the result of Stage 1 to estimate the worker reliability vector p. Remind that we randomly split the original response matrix AintoA1andA2with proba-Algorithm 1 Spectral Method for Initial Estimation (TopTwo1 Algorithm) Input: data matrix A1‚àà {0,1, . . . , K }n√ómand parame- terŒ∑ >0where Œ∑‚àön‚â§ ‚à•p‚à•2‚â§‚àön. Output: estimated top-two answers {(ÀÜgj,ÀÜhj)}m j=1and con- fusion probability vector ÀÜq. 1:Randomly split (with equal probabilities) A1into BandC, and convert the two matrices into bi- nary matrices X(k)‚àà {‚àí 1,0,1}n√ómandY(k)‚àà {‚àí1,0,1}n√ómfor1‚â§k < K , respectively, as de- scribed in Sec. 4.1. 2:Letu(k)be the leading normalized left singular vector ofX(k). Trim the abnormally large components of u(k) by setting them to zero if u(k) i>2 Œ∑‚àönand denote the resulting vector as Àúu(k). 3:Calculate the estimate of ‚à•p‚à•r(k)by defining v(k):= 1 s‚Ä≤(Y(k))‚ä§Àúu(k).Assume v(0):=0andv(K):=0. 4:Fork‚àà[K], calculate ‚àÜv(k) j:=v(k) j‚àív(k‚àí1) j.Esti- mate the top-two answers for j‚àà[m]by ÀÜgj:= arg min k‚àà[K]‚àÜv(k) j;ÀÜhj:= arg min kÃ∏=ÀÜgj,k‚àà[K]‚àÜv(k) j.(3) 5:Estimate ‚à•p‚à•2bylj:=K K‚àí2P kÃ∏=ÀÜgj,kÃ∏=ÀÜhj‚àÜv(k) jand l:=1 mPm j=1lj. 6:Estimate qjforj‚àà[m]by defining ÀÜqj:= 1/K‚àí‚àÜv(ÀÜgj) j/l. (4) bility s1and1‚àís1, respectively, and use A1only for Alg. 1. Thus, the estimated top-two answers {(ÀÜgj,ÀÜhj)}m j=1from Alg. 1 depend only on A1. We then define the estimator ÀÜp for worker reliability by comparing the unused data matrix A2with the estimated top two answers {(ÀÜgj,ÀÜhj)}m j=1as ÀÜpi=K (K‚àí2)Ô£´ Ô£≠1 ms(1‚àís1)mX j=11(A2 ij= ÀÜgjorÀÜhj)‚àí2 KÔ£∂ Ô£∏. (5) The final step refines the estimates for the top two answers by using the plug-in MLE where the estimated (ÀÜp,ÀÜq)are placed in (p,q)at the oracle MLE, which finds (ÀÜgj,ÀÜhj)‚àà [K]2\{(1,1),(1,2), . . . , (K, K )}such that (ÀÜgj,ÀÜhj) := arg max(a,b)‚àà[K]2,aÃ∏=bPn i=1logP(Aij|p, qj,(a, b))as in (6). Our complete algorithm is presented in Algorithm 2. The time complexity of Alg. 2 is O(m2logm+nmK2), since the SVD in Alg. 1 can be computed via power iter- ations within O(m2logm)steps (Boutsidis et al., 2015), and the step for finding the pair of answers maximizing (6) requires O(nmK2)steps. 4Recovering Top-Two Answers and Confusion Probability in Multi-Choice Crowdsourcing Algorithm 2 Plug-in MLE (TopTwo2 Algorithm) Input: data matrix A‚àà {0,1, . . . , K }n√ómand the sample splitting rate s1>0. Output: estimated top two answers {(ÀÜgMLE j,ÀÜhMLE j)}m j=1 and confusion probability vector ÀÜq. 1:Randomly split AintoA1andA2by defining A1:= A‚ó¶SandA2=A‚ó¶( 1n√óm‚àíS)where Sis ann√óm matrix whose entries are i.i.d. with Bernoulli( s1) and‚ó¶ is an entrywise product. 2:Apply Algorithm 1 to A1to yield estimates for top-two answers {(ÀÜgj,ÀÜhj)}m j=1and confusion probability vector ÀÜq. 3:By using {(ÀÜgj,ÀÜhj)}m j=1andA2, calculate the estimate for worker reliability vector ÀÜpas in (5). 4:By using the whole Aand(ÀÜp,ÀÜq), find the plug-in MLE estimates (ÀÜgMLE j,ÀÜhMLE j)by arg max a,b‚àà[K]2,aÃ∏=bnX i=1logKÀÜpiÀÜqj 1‚àíÀÜpi+ 1 1(Aij=a) + logKÀÜpi(1‚àíÀÜqj) 1‚àíÀÜpi+ 1 1(Aij=b). (6) 5. Performance Analysis To state our main theoretical results, we first need to intro- duce some notation and assumptions. Let ¬µ(i,j) (a,b),kdenote the probability that a worker i‚àà[n]gives the label k‚àà[K] for the assigned task j‚àà[m], whose top two answers are (gj, hj) = (a, b). Note that ¬µ(i,j) (a,b),kcan be written in terms of(pi, qj)from the distribution in (1)for every a, b, k ‚àà [K]3. Let ¬µ(i,j) (a,b)= [¬µ(i,j) (a,b),1¬µ(i,j) (a,b),2¬∑¬∑¬∑ ¬µ(i,j) (a,b),K]‚ä§. We introduce a quantity that measures the average abil- ity of workers in distinguishing the ground-truth pair of top-two answers (gj, hj)from any other pair (a, b)‚àà [K]2/{(gj, hj)}for the task j‚àà[m]. We define D(j):= min (gj,hj)Ã∏=(a,b)1 nnX i=1DKL ¬µ(i,j) (gj,hj),¬µ(i,j) (a,b) ;(7) D:= min j‚àà[m]D(j), (8) where DKL(P, Q) :=P iP(i) log( P(i)/Q(i))is the KL- divergence between PandQ. Note that D(j)is strictly positive if qj‚àà(1/2,1)and there exists at least one worker iwithpi>0, so that (gj, hj)can be distinguished from any other (a, b)‚àà[K]2/{(gj, hj)}statistically in (1). We de- fineDas the minimum of D(j)overj‚àà[m], indicating the average ability of workers in distinguishing (gj, hj)from any other (a, b)for the most difficult task in the set of tasks.Theorem 1 states the performance guarantees for Algo- rithm 1 by providing sufficient conditions for achieving an arbitrarily accurate estimation of the top-two answers and the confusion probability. Theorem 1 (Performance Guarantees for Algorithm 1) . For any œµ, Œ¥1>0, if the sampling probability s¬∑s1= ‚Ñ¶ 1 Œ¥2 1‚à•p‚à•2 2logK œµ , Algorithm 1 guarantees the recovery of the ordered top-two answers (gj, hj)with probability at least1‚àíœµfor any task j‚àà[m]having qj‚àà(1/2,1), i.e., P (ÀÜgj,ÀÜhj) = (gj, hj) ‚â•1‚àíœµfor all j‚àà[m],(9) and also guarantees the recovery of the confusion probabil- ityqjwith P(|ÀÜqj‚àíqj|< Œ¥1)‚â•1‚àíœµfor all j‚àà[m], (10) where the number mof tasks is sufficiently large and the number of workers scales as n=O(m/logm). For a task jwithqj= 1, it is impossible to recover hj, since hjcannot be distinguished from the rest of wrong labels c‚àà[K]\{gj}statistically from (1). For such tasks, we can still guarantee the recovery of gjwith accuracy P(ÀÜgj=gj)‚â•1‚àíœµunder the conditions in Theorem 1. By using Theorem 1, we can also find the sufficient conditions to guarantee the recovery of paired top-two answers for all tasks and qwith an arbitrarily small ‚Ñì‚àû-norm error with probability at least 1‚àíœµ. Corollary 1. For any œµ, Œ¥1>0, if the sampling probability s¬∑s1= ‚Ñ¶ 1 Œ¥2 1‚à•p‚à•2 2logmK œµ , Algorithm 1 guarantees the recovery of {(gj, hj)}m j=1andqwith probability at least 1‚àíœµasm‚Üí ‚àû such that P (ÀÜgj,ÀÜhj) = (gj, hj),‚àÄj‚àà[m] ‚â•1‚àíœµ,and (11) P(‚à•q‚àíÀÜq‚à•‚àû< Œ¥1)‚â•1‚àíœµ. (12) Proofs of Thm. 1 and Cor. 1 are available in Appendix ¬ßG. We next analyze the performance of Algorithm 2, which uses Algorithm 1 as the first stage. Before providing the main theorem for Algorithm 2, we state a lemma character- ing a sufficient condition for estimating the worker reliabil- ity vector pfrom (5) with an arbitrarily small ‚Ñì‚àûerror. Lemma 1. Conditioned on (ÀÜgj,ÀÜhj) = (gj, hj)for all j‚àà [m], ifs(1‚àís1) = ‚Ñ¶ 1 Œ¥2 2mlogn œµ , the estimator ÀÜpidefined in(5)of Algorithm 2 guarantees P(‚à•p‚àíÀÜp‚à•‚àû< Œ¥2)‚â• 1‚àíœµfor any œµ >0. Combining Corollary 1 and Lemma 1, we can obtain the estimators (ÀÜp,ÀÜq)of the worker reliability vector pand the confusion probability vector q, respectively, with ‚Ñì‚àû-norm 5Recovering Top-Two Answers and Confusion Probability in Multi-Choice Crowdsourcing error bounded by any arbitrarily small Œ¥ >0with probabil- ity at least 1‚àí2œµif s=s¬∑s1+s(1‚àís1) = ‚Ñ¶log(mK/œµ ) Œ¥2‚à•p‚à•2 2+log(n/œµ) Œ¥2m = ‚Ñ¶log(mK/œµ ) Œ¥2‚à•p‚à•2 2 (13) where the last equality is from the assumption that ‚à•p‚à•2= Œò(‚àön)andn=O(m/logm). In this regime, the sample complexity for estimating the task difficulty qis greater than that required for estimating worker reliability p. To make the sampling probability s <1, we need n= ‚Ñ¶(log m). Our second theorem characterizes the sufficient condition on the sampling probability sto guarantee the recovery of the pair of top-two answers for all tasks by (6)of Alg. 2, when a sufficiently accurate estimation of (p,q)is provided. Theorem 2. Assume that there is a positive scalar œÅsuch that¬µ(i,j) (gj,hj),c‚â•œÅfor all (i, j, g j, hj, c)‚àà[n]√ó[m]√ó[K]3. For any œµ >0, if(ÀÜp,ÀÜq)are given with max{‚à•p‚àíÀÜp‚à•‚àû,‚à•q‚àíÀÜq‚à•‚àû} ‚â§Œ¥:= minœÅ 4,œÅD 4(6 + D) , (14) and the sampling probability satisfies s= ‚Ñ¶log(1/œÅ) log( mK2/œµ) + log( m/œµ) nD , then for any œµ >0the estimates of {(gj, hj)}m j=1from (6) of Algorithm 2 guarantees P (ÀÜgMLE j,ÀÜhMLE j) = (gj, hj),‚àÄj‚àà[m] ‚â•1‚àíœµ.(15) Proofs of Lemma 1 and Theorem 2 are available in Ap- pendix ¬ßH. The assumption in Theorem 2 that ¬µ(i,j) (gj,hj),c‚â•œÅ for some œÅ >0holds if pi<1for all i‚àà[n], i.e., there is no perfectly reliable worker. This assumption can be easily satisfied by adding an arbitrary small random noise to the worker answers as well. By combining the statements in Corollary 1, Lemma 1, and Theorem 2 with Œ¥1=Œ¥2=Œ¥for Œ¥defined in (14), we get the overall performance guarantee for Algorithm 2. Corollary 2 (Performance Guarantees for Algorithm 2) .Al- gorithm 2 guarantees the recovery of top-two answers for all tasks with P (ÀÜgMLE j,ÀÜhMLE j) = (gj, hj),‚àÄj‚àà[m] ‚â•1‚àíœµ for any œµ >0ifssatisfies s=‚Ñ¶log(m/œµ) Œ¥2‚à•p‚à•2 2+log(m/œµ) nD . (16) In(16), the first term is for guaranteeing the accurate esti- mate of (p,q)with‚Ñì‚àû-norm error bounded by Œ¥, and thesecond term is for the recovery of top-two answers from MLE with high probability. Since ‚à•p‚à•2 2= Œò( n), the two terms have the same order but with different constant scal- ing, depending on model-specific parameters (p,q). Lastly, we show the optimality of the convergence rates of Algorithm 1 and Algorithm 2 with respect to two types of minimax errors, respectively. The proof of Theorem 3 is available in Appendix ¬ßI. Theorem 3. (a) Let Fpbe a set of p‚àà[0,1]nsuch that the collective quality of workers, measured by ‚à•p‚à•2, is pa- rameterized by pasF¬Øp:={p:1 n‚à•p‚à•2 2=p}. Assume that p‚â§2/3. If the average number (ns)of samples (queries) per task is less than1 2plog K‚àí1 Kœµ , then min ÀÜgmax p‚ààFp,g‚àà[K]m1 mX j‚àà[m]P(ÀÜgjÃ∏=gj)‚â•œµ. (17) (b) There is a universal constant c >0such that for any p‚àà[0,1]nandq‚àà(1/2,1]m, if the sampling probability s‚â§1/(4nD), then min (ÀÜg,ÀÜh)max (g,h)‚àà[K]m√ó[K]m gjÃ∏=hj,‚àÄj[m]1 mX j‚àà[m]P((ÀÜgj,ÀÜhj)Ã∏= (gj, hj))‚â•c. (18) From part (a) of Theorem 3, it is necessary to have s= ‚Ñ¶  (1/‚à•p‚à•2 2) log(1 /œµ) to make the minimax error in (17) less than œµ. Since Thm. 1 shows that Alg. 1 recovers (ÀÜgj,ÀÜhj) with probability at least 1‚àíœµifs= ‚Ñ¶  (1/‚à•p‚à•2 2) log(1 /œµ) when s1= 1, we can conclude that Alg. 1 achieves the minimax optimal rate for a fixed collective intelligence of workers, measured by ‚à•p‚à•2. From part (b) of Theorem 3, for any (p,q), unless we have s >1/(4nD)there exists a constant fraction of tasks for which the recovered top-two answers are incorrect. This bound matches with our suffi- cient condition on sin(16) from Alg. 2 upto logarithmic factors, as long as Œ¥2‚à•p‚à•2‚â≥nD, showing the minimax optimality of Alg. 2. More discussions on the theoretical results are available at Appendix ¬ßE. It is also worth comparing our algorithm with the simple majority voting (MV) scheme where we infer the top-two answers by counting the majority of the received answers. Simple analysis shows that the MV scheme requires the sampling probability sto be ns= Œò  (1 nP ipi)‚àí2log1 œµ to recover (gj, hj)with probability 1‚àíœµ. Since1 n‚à•p‚à•2 2= 1 nP ip2 i‚â• 1 nP ipi2, Algorithm 1 achieves strictly better trade-offs unless piis the same for all workers i‚àà[n]. For a spammer-hammer model (Karger et al., 2014) where Œ±‚àà(0,1)fraction of workers are hammers with pi= 1and the rest are spammers with pi= 0, Algorithm 1 requires ns= Œò 1 Œ±log1 œµ samples per task, while MV requires ns= Œò 1 Œ±2log1 œµ samples per task to recover top-two answers with probability 1‚àíœµ. 6Recovering Top-Two Answers and Confusion Probability in Multi-Choice Crowdsourcing Figure 1. Prediction error in recovering the ordered top-two answers (g, h)for four different scenarios, summarized in Table 1, as the avg. number of queries per task changes. Our TopTwo2 algorithm achieves the best performance, near the oracle MLE for all the scenarios. 6. Experiments We evaluate the proposed algorithm under diverse scenarios of synthetic datasets in Sec. 6.1, and for two applications‚Äìin identifying difficult tasks in real datasets in Sec. 6.2, and in training neural network models with soft labels defined from the top-two plausible labels in Sec. 6.3. 6.1. Experiments on synthetic dataset We compare the empirical performance of Algorithm 1 and Algorithm 2 (referred as TopTwo1 and TopTwo2) with other baselines: majority voting(MV), MV-D&S and OPT- D&S (Zhang et al., 2014), PGD (Ma et al., 2018), M-MSR (Ma & Olshevsky, 2020), MultiSPA-KL and MultiSPA-EM (Ibrahim et al., 2019), EBCC (Li et al., 2019) and oracle- MLE. OTP-D&S and MV-D&S assume the D&S model and use the EM algorithm, initialized with worker confusion matrices estimated by spectral method or MV , respectively. PGD, on the other hand, assumes a simpler single-coin D&S model, which is equivalent to our model (1)with a fixed qj= 1 for all tasks, and estimates piof each worker and uses this estimate to compute the MLE. We choose these baselines because they have the strongest established guaran- tees in the literature, and they are all MLE-based approaches, from which the top-two answers can be inferred. Obviously, oracle-MLE, which uses the ground-truth model parameters, Table 1. Parameters for synthetic data experiments under diverse scenarios. Worker Task Easy pi‚àà[0,1] qj‚àà[0.9,1] Hard pi‚àà[0,1] qj‚àà(0.5,0.6] Few-smart90%pi‚àà[0,0.1]qj‚àà(0.5,1]10%pi‚àà[0.9,1] High-variance pi‚àà[0,0.1]50%qj‚àà(0.5,0.6] 50%qj‚àà[0.9,1.0]provides the best possible performance since oracle MLE uses the ground-truth (p,q)from which the synthetic data is generated. See Appendix ¬ßC for more details of these baselines. We devise four scenarios described in Table 1 to verify the robustness of our model for different (p,q)ranges, at(n, m) = (50 ,500) withs‚àà(0,0.2]. The number of choices for each task is fixed as 5. Fig. 1 reports the empir- ical error probability1 mPm j=1P((ÀÜgj,ÀÜhj)Ã∏= (gj, hj))aver- aged over 30 runs, with 95% confidence intervals (shaded region). Four columns correspond to the four scenarios, respectively. The prediction errors for gjandhjare plotted in Fig. 6 of Appendix. ¬ßD.1. We can observe that for all considered scenarios, TopTwo2 achieves the best performance, close to the oracle MLE, in recovering (gj, hj). Depending on the scenario, the reason for TopTwo2‚Äôs outperformance can be explained differently. For the Easy scenario, since qjis close to 1, it is easy to distinguish gjfromhj, but hard to distinguish hjfrom other labels. Our algorithm achieves the best performance in esti- mating hjby a large margin (Fig. 6), which also leads to a better performance in estimating (gj, hj)compared to other baselines. For the Hard scenario, it is difficult to distinguish gjfrom hj, but our algorithm using an accurate ÀÜqjcan bet- ter distinguish gjfrom hj. For Few-smart , our algorithm achieves the largest gain compared to other methods, since our algorithm can effectively distinguish few smart workers from spammers. High-variance shows the effect of having diverse qjin a dataset. We remark that our algorithm (TopTwo2) achieves the best performance, close to that of the oracle MLE, for all sce- narios, while the next performer changes depending on the scenario. For example, the OPT D&S is the second best per- former in the Hard scenario, while it is the worst performer in the Few-smart scenario. We also show the robustness of our algorithm to changes in model parameters in Appendix ¬ßD. 7Recovering Top-Two Answers and Confusion Probability in Multi-Choice Crowdsourcing (a) The average prediction error on color comparison tasks  (b) Histogram of dist. gap Figure 2. (a) Prediction error for (gj, hj),gjandhj(from left to right) for color comparison task using real data collected by MTurk. TopTwo2 algorithm achieves the best performance. (b) Histogram of color distance gap for two task groups, the easy group with the highest qj(red) and the difficult group with the lowest qj(blue). The difficult group tends to have a smaller color distance gap. 6.2. Experiments on real-world dataset: inferring task difficulties We provide experimental results using real-world data col- lected by MTurk and show that our algorithm can be used to infer task difficulty. Since publicly available datasets do not provide information about confusing answers or task difficulty, we designed a new set of multiple-choice tasks for which we can identify both. We designed a color com- parison task in which we asked the crowd to choose, from six given choices, the color that looks the most like a ref- erence color of each task. See Fig. 4 in Appx. ¬ßA.1 for example tasks. After randomly generating a reference color and the six choices, we identified the ground truth and the most confusing answer for each task by measuring the dis- tance between colors using the CIEDE2000 color difference formula (Sharma et al., 2005). If the distance from the ref- erence color to the ground truth is much shorter than the distance to the most confusing answer, then the task is con- sidered easy. We designed 1,000 tasks and distributed them to 200 workers, collecting 19.5 responses for each task. Af- ter collecting the data, we subsampled it to simulate how the prediction error decreases as the number of responses per task increases. Fig. 2a shows the performance in detecting (gj, hj),gjandhj, averaged over 10 random sampling, with a 95% confidence interval (shaded region). First, we can verify that the ground truth and the most con- fusing answer we identified by the measured color distance are valid with the collected data, since the prediction error actually decreases as the number of queries per task in- creases. As shown in Fig. 2a, TopTwo2 algorithm achieves the best performance in detecting (gj, hj),gjandhjin all ranges. We further investigate the correlation between the task difficulty - quantified by the distance gap between the ground truth and the most confusing answer from the ref- erence color - and the estimated confusion probability qj across tasks. We select the top 50 most difficult/easiest tasks according to the estimated confusion probability qjand plotthe histograms of the distance gap for the two groups in Fig 2b. We can see that the difficult group (blue, with the lowest qj) tends to have a smaller distance gap than the easy group (red). This result shows that our algorithm can identify difficult tasks in real datasets. 6.3. Training neural networks with soft labels having top-two information An appealing example where we can use the knowledge of the second best answer is in training deep neural net- works for classification tasks. Traditionally, a hard label (one ground-truth label per image) has been used to train a classifier. Recent work has shown that using a soft label (a full label distribution that reflects human perceptual un- certainty) is sometimes advantageous in obtaining a model with better generalization ability (Peterson et al., 2019). However, obtaining an accurate full label distribution re- quires much higher sample complexity than just recovering only the ground-truth. For example, Peterson et al. (2019) provided a CIFAR10H dataset with full human label distri- butions for 10,000 instances of CIFAR10 test examples by collecting an average of 50 judgments per image, which is about 5-10 times larger than the usual datasets (Table 4 in Appendix A.1). Our top-two model, on the other hand, can effectively re- duce the required sample complexity while still providing the benefit of the soft-label training. To demonstrate this idea, we train two deep neural network models, VGG-19 and ResNet18, with the soft label vectors having the top- two (top2) structure extracted from the CIFAR10H dataset3. 3As in (Peterson et al., 2019), we used the original 10,000 test examples from CIFAR10 for training and 50,000 training examples for testing. Thus, the final accuracy is lower than usual. Since CIFAR10H is collected from selected ‚Äòreliable‚Äô workers who answered a set of test examples with an accuracy higher than 75%, we directly used the top two dominant answers and the fraction between them to obtain the soft label vector (top2). 8Recovering Top-Two Answers and Confusion Probability in Multi-Choice Crowdsourcing Table 2. Comparison of performances for CIFAR10H dataset with hard/soft label training Network Train accuracy Test accuracy VGG-19 (hard) 97.46¬±0.59% 77.64¬±1.54% VGG-19 (top2) 97.00 ¬±0.51% 79.20¬±1.04% VGG-19 (full) 96.69 ¬±0.48% 78.66 ¬±0.97% ResNet18 (hard) 98.47 ¬±0.320% 76.49% ¬±1.80% ResNet18 (top2) 98.67 ¬±0.491% 80.58% ¬±2.36% ResNet18 (full) 99.19¬±0.125% 80.93% ¬±2.66% We then compare the training and testing results of our method with those of the hard label (hard) and full label (full) training. Experimental details are given in Appendix ¬ßB. Compared to the original training with hard labels, train- ing with the top-two soft labels achieves 1.56% and 4.09% higher test accuracy in VGG-19 and ResNet18, respectively (averaged over three runs, 150 epochs), as shown in Table 2. This result is also comparable to that of the full label distribution. It shows that training with the top-two soft labels can achieve better generalization (test accuracy) than training with hard labels, because the top-two soft label contains simple but helpful side information, the most con- fusable class, and the confusion probability. In Sec. B.4, we also report an additional experiment showing that training with the top-two labels is more robust to the label noise than training with the full label distribution. 7. Discussion We proposed a new model for multiple-choice crowdsourc- ing with top-two confusable answers and varying confusion probabilities across tasks. We provided an algorithm to infer the top-two answers and the confusion probability. This work can benefit various query-based data collection systems, such as MTurk or review systems, by providing additional information about the task, such as the most plau- sible answer other than the ground truth and how plausible it is. This information can be used to quantify the accuracy of the ground truth or to classify the tasks based on difficulty. We also demonstrated possible applications of our top-two model in designing soft labels for training neural networks. 8. Acknowledgements This research was supported by the National Research Foun- dation of Korea under grant 2021R1C1C11008539, and by the MSIT(Ministry of Science and ICT), Korea, under the ITRC(Information Technology Research Center) sup- port program(IITP-2023-2018-0-01402) supervised by the IITP(Institute for Information & Communications Technol- ogy Planning & Evaluation).References Aydin, B. I., Yilmaz, Y . S., Li, Y ., Li, Q., Gao, J., and Demir- bas, M. Crowdsourcing for multiple-choice question an- swering. In Proceedings of the Twenty-Eighth AAAI Con- ference on Artificial Intelligence , pp. 2946‚Äì2953, 2014. Bachrach, Y ., Graepel, T., Minka, T., and Guiver, J. How to grade a test without knowing the answers‚Äîa bayesian graphical model for adaptive crowdsourcing and aptitude testing. arXiv preprint arXiv:1206.6386 , 2012. Bandeira, A. S. and Van Handel, R. Sharp nonasymptotic bounds on the norm of random matrices with independent entries. The Annals of Probability , 44(4):2479‚Äì2506, 2016. Berend, D. and Kontorovich, A. Consistency of weighted majority votes. Advances in Neural Information Process- ing Systems , 27, 2014. Boutsidis, C., Kambadur, P., and Gittens, A. Spectral cluster- ing via the power method-provably. In International con- ference on machine learning , pp. 40‚Äì48. PMLR, 2015. Dalvi, N., Dasgupta, A., Kumar, R., and Rastogi, V . Aggre- gating crowdsourced binary ratings. In Proceedings of the 22nd international conference on World Wide Web , pp. 285‚Äì294, 2013. Dawid, A. P. and Skene, A. M. Maximum likelihood es- timation of observer error-rates using the em algorithm. Journal of the Royal Statistical Society: Series C (Applied Statistics) , 28(1):20‚Äì28, 1979. Demartini, G., Difallah, D. E., and Cudr ¬¥e-Mauroux, P. Zen- crowd: leveraging probabilistic reasoning and crowd- sourcing techniques for large-scale entity linking. In Pro- ceedings of the 21st international conference on World Wide Web , pp. 469‚Äì478, 2012. Gao, C. and Zhou, D. Minimax optimal convergence rates for estimating ground truth from crowdsourced labels. arXiv preprint arXiv:1310.5764 , 2013. Gao, C., Lu, Y ., and Zhou, D. Exact exponent in optimal rates for crowdsourcing. In International Conference on Machine Learning , pp. 603‚Äì611. PMLR, 2016. Ghosh, A., Kale, S., and McAfee, P. Who moderates the moderators? crowdsourcing abuse detection in user- generated content. In Proceedings of the 12th ACM con- ference on Electronic commerce , pp. 167‚Äì176, 2011. Ibrahim, S., Fu, X., Kargas, N., and Huang, K. Crowd- sourcing via pairwise co-occurrences: Identifiability and algorithms. Advances in neural information processing systems , 32, 2019. 9Recovering Top-Two Answers and Confusion Probability in Multi-Choice Crowdsourcing Ipeirotis, P. G., Provost, F., and Wang, J. Quality man- agement on amazon mechanical turk. In Proceedings of the ACM SIGKDD workshop on human computation , pp. 64‚Äì67, 2010. Karger, D., Oh, S., and Shah, D. Iterative learning for reliable crowdsourcing systems. Advances in neural in- formation processing systems , 24, 2011. Karger, D. R., Oh, S., and Shah, D. Efficient crowdsourcing for multi-class labeling. In Proceedings of the ACM SIGMETRICS/international conference on Measurement and modeling of computer systems , pp. 81‚Äì92, 2013. Karger, D. R., Oh, S., and Shah, D. Budget-optimal task allocation for reliable crowdsourcing systems. Operations Research , 62(1):1‚Äì24, 2014. Khetan, A. and Oh, S. Achieving budget-optimality with adaptive schemes in crowdsourcing. Advances in Neural Information Processing Systems , 29:4844‚Äì4852, 2016. Krivosheev, E., Bykau, S., Casati, F., and Prabhakar, S. De- tecting and preventing confused labels in crowdsourced data. Proceedings of the VLDB Endowment , 13(12):2522‚Äì 2535, 2020. Li, H. and Yu, B. Error rate bounds and iterative weighted majority voting for crowdsourcing. arXiv preprint arXiv:1411.4086 , 2014. Li, Y ., Rubinstein, B., and Cohn, T. Exploiting worker correlation for label aggregation in crowdsourcing. In International conference on machine learning , pp. 3886‚Äì 3895. PMLR, 2019. Liu, C. and Wang, Y . Truelabel + confusions: A spectrum of probabilistic models in analyzing multiple ratings. In Proceedings of the 29th International Conference on Ma- chine Learning, ICML , 2012. Liu, Q., Peng, J., and Ihler, A. T. Variational inference for crowdsourcing. Advances in neural information process- ing systems , 25, 2012. Liu, Y ., Xu, Y ., Shah, N. B., and Singh, A. Integrating rankings into quantized scores in peer review. arXiv preprint arXiv:2204.03505 , 2022. Ma, Q. and Olshevsky, A. Adversarial crowdsourcing through robust rank-one matrix completion. Advances in Neural Information Processing Systems , 33:21841‚Äì 21852, 2020. Ma, Y ., Olshevsky, A., Szepesvari, C., and Saligrama, V . Gradient descent for sparse rank-one matrix completion for crowd-sourced aggregation of sparsely interacting workers. In International Conference on Machine Learn- ing, pp. 3335‚Äì3344. PMLR, 2018.Ok, J., Oh, S., Shin, J., and Yi, Y . Optimality of belief prop- agation for crowdsourced classification. In International Conference on Machine Learning , pp. 535‚Äì544. PMLR, 2016. Peterson, J. C., Battleday, R. M., Griffiths, T. L., and Rus- sakovsky, O. Human uncertainty makes classification more robust. In Proceedings of the IEEE/CVF Interna- tional Conference on Computer Vision , pp. 9617‚Äì9626, 2019. Raykar, V . C., Yu, S., Zhao, L. H., Valadez, G. H., Florin, C., Bogoni, L., and Moy, L. Learning from crowds. Journal of machine learning research , 11(4), 2010. Sabetpour, N., Kulkarni, A., Xie, S., and Li, Q. Truth discovery in sequence labels from crowds. In 2021 IEEE International Conference on Data Mining (ICDM) , pp. 539‚Äì548. IEEE, 2021. Shah, D. and Lee, C. Reducing crowdsourcing to graphon es- timation, statistically. In International Conference on Ar- tificial Intelligence and Statistics , pp. 1741‚Äì1750, 2018. Shah, N. B., Balakrishnan, S., and Wainwright, M. J. A permutation-based model for crowd labeling: Optimal estimation and robustness. IEEE Transactions on Infor- mation Theory , 67(6):4162‚Äì4184, 2020. Sharma, G., Wu, W., and Dalal, E. N. The ciede2000 color- difference formula: Implementation notes, supplementary test data, and mathematical observations. Color Research & Application: Endorsed by Inter-Society Color Council, The Colour Group (Great Britain), Canadian Society for Color, Color Science Association of Japan, Dutch Society for the Study of Color, The Swedish Colour Centre Foundation, Colour Society of Australia, Centre Fran c ¬∏ais de la Couleur , 30(1):21‚Äì30, 2005. Smyth, P., Fayyad, U., Burl, M., Perona, P., and Baldi, P. Inferring ground truth from subjective labelling of venus images. Advances in neural information processing systems , 7, 1994. Sohn, J.-Y ., Shang, L., Chen, H., Moon, J., Papailiopoulos, D., and Lee, K. Genlabel: Mixup relabeling using gen- erative models. In International Conference on Machine Learning , pp. 20278‚Äì20313. PMLR, 2022. Stelmakh, I., Shah, N. B., and Singh, A. Peerreview4all: Fair and accurate reviewer assignment in peer review. In Algorithmic Learning Theory , pp. 828‚Äì856. PMLR, 2019. Tian, T. and Zhu, J. Max-margin majority voting for learning from crowds. Advances in neural information processing systems , 28, 2015. 10Recovering Top-Two Answers and Confusion Probability in Multi-Choice Crowdsourcing Welinder, P., Branson, S., Perona, P., and Belongie, S. The multidimensional wisdom of crowds. Advances in neural information processing systems , 23, 2010. Zhang, H., Cisse, M., Dauphin, Y . N., and Lopez-Paz, D. mixup: Beyond empirical risk minimization. arXiv preprint arXiv:1710.09412 , 2017. Zhang, Y ., Chen, X., Zhou, D., and Jordan, M. I. Spec- tral methods meet em: A provably optimal algorithm for crowdsourcing. Advances in neural information process- ing systems , 27, 2014. Zhou, D., Basu, S., Mao, Y ., and Platt, J. Learning from the wisdom of crowds by minimax entropy. Advances in neural information processing systems , 25, 2012. 11Recovering Top-Two Answers and Confusion Probability in Multi-Choice Crowdsourcing A. Verification for the Proposed Top-Two Model We proposed the top-two model to reflect the key attributes of seven datasets including Adult2, Dog, Web, Flag, Food, Plot, and Color, of which the details are summarized in Appendix A.1. Table 3 shows empirical distributions of the mean incidence of responses for the top-three dominating answers, sorted by the dominance proportions, for the six public datasets and the Color dataset that we collected, with the standard deviation over the tasks in the dataset. In Fig. 3, we also plot empirical distributions of the mean incidence of responses sorted by the dominant proportion with error bars indicating the standard deviation. The i-th data point represents the average incidence of the i-th highest response in each task. For example, in Adult2 dataset, the most dominating answer takes 0.8 portion of the total answers, and the next dominating answer takes 0.14 portion of the total answers on average. Table 3. Proportions of top-three dominating answers in public datasets Dataset Ground truth 2nd dominating answer 3rd dominating answer Adult2 0.80 ¬±0.19 0.14 ¬±0.13 0.04 ¬±0.07 Dog 0.76 ¬±0.15 0.22 ¬±0.14 0.01 ¬±0.04 Web 0.59 ¬±0.20 0.25 ¬±0.12 0.12 ¬±0.09 Flag 0.90 ¬±0.16 0.09 ¬±0.13 0.01 ¬±0.03 Food 0.80 ¬±0.18 0.17 ¬±0.15 0.02 ¬±0.05 Plot 0.62 ¬±0.21 0.30 ¬±0.16 0.06 ¬±0.07 Color 0.43 ¬±0.1 0.23 ¬±0.06 0.15 ¬±0.05 (a)  (b)  (c)  (d) (e)  (f)  (g) Figure 3. Empirical distribution of the mean incidence of responses sorted by the dominant proportion, averaged over all tasks in each dataset. The i-th data point represents the average incidence of the i-th highest response in each task. The error bars indicate the standard deviation of the mean incidence of the i-th dominating answer over the tasks in the dataset. From the table and figure, we can observe that for all the considered public datasets the top-two answers dominate the overall answers, i.e., about 65-90% of the total answers belong to the top two. Furthermore, the average ratio from the most 12Recovering Top-Two Answers and Confusion Probability in Multi-Choice Crowdsourcing (a)gj= 6andhj= 5  (b)gj= 4andhj= 3 (c)gj= 5andhj= 3  (d)gj= 6andhj= 2 Figure 4. Example tasks for ‚ÄòColor‚Äô dataset where the ground truth gand the most confusing answer hare determined by the color distance from the reference color (top). dominating answer to the second one is 4:1, while that between the second and the third is 7.5:1. There often exist overlaps in the error bars between the ground truth and the second dominating answer, e.g., for Web, Plot, and Color datasets, but no such overlap is found between the ground truth and the third dominating answer. What we can call a ‚Äòconfusing answer‚Äô is an answer that has an incidence rate comparable to that of the ground truth. In all the considered datasets, only the second dominating answer shows such a tendency, and thus, we can conclude that the third dominating answer cannot be called a ‚Äòconfusing answer‚Äô, and the top-two model in (1) well describes the errors in answers caused by confusion. Moreover, from the public datasets, we also observe that the task difficulty can be quantified by the confusion probability between the top-two answers. As an example, for the Web dataset, when we select the easiest 500 tasks and hardest 500 tasks by ordering tasks with the ratio of correct answers, the ratio between the ground-truth to the 2nd best answer was 10.7:1 for the easiest group, while it was 1.5:1 for the hardest group. This observation shows that the ratio between the top-two answers indeed captures task difficulty as does our model parameter for task difficulty qjin (1). A.1. Datasets We collect six publicly available multi-class datasets: Adult2, Dog, Web, Flag, Food and Plot. Since these datasets do not provide information about the most confusing answer or the task difficulty, we additionally create a new dataset called ‚ÄòColor‚Äô, for which we can identify the most confusing answer and also quantify the task difficulty for all the included tasks. ‚Ä¢Color is a dataset where the task is to find the most similar color to the reference color among six different choices. For each task, we randomly create a reference color and then choose six choices of colors. The distance from the reference color to the ground truth color is in between 4.5 and 5.5, the distance to the most confusing answer is in between 5.5 and 6.5, and the distance to the rest of the choices is between 11 and 12, where the distance between the pairs of colors is measured by CIEDE2000 (Sharma et al., 2005) color difference formulation. The tasks are ordered in terms of their difficulty levels by measuring the gap between: the distance from the reference color to the ground truth; and that to the most confusing answer. If the distance from the reference color to the ground truth is much shorter than that to the most confusing answer, then the task is considered easy. Using MTurk, we collected 19600 labels from 196 workers for 1000 tasks. Each Human Intelligence Task (HIT) is composed of randomly selected 100 tasks, and we pay $1 to each worker who completed a HIT. Fig. 4 shows an example task for the Color dataset. ‚Ä¢Adult2 (Ipeirotis et al., 2010) is a 4-class dataset where the task is to classify the web pages into four categories (G, PG, R, X) depending on the adult level of the websites. This dataset contains 3317 labels for 333 websites which are offered by 269 workers. ‚Ä¢Dog (Zhang et al., 2014) is a 4-class dataset where the task is to discriminate a breed (out of Norfolk Terrire, Norwich Terrier, Irish Wolfhound, and Scottich Deerhound) for a given dog. This dataset contains 7354 labels collected from 52 workers for 807 tasks. 13Recovering Top-Two Answers and Confusion Probability in Multi-Choice Crowdsourcing ‚Ä¢Web (Zhou et al., 2012) is a 5-class dataset where the task is to determine the relevance of query-URL pairs with a 5-level rating (from 1 to 5). The dataset contains 15567 labels for the 2665 query-URL pairs offered by 177 workers. ‚Ä¢Flag (Krivosheev et al., 2020) is a dataset for multiple-choice tasks where each task is to identify the country for a given flag from 10 given choices. A total of 1600 votes are collected from 220 workers for the 100 tasks. ‚Ä¢Food (Krivosheev et al., 2020) is a dataset for multiple-choice tasks where each task asks to identify a picture of a given food or dish from 5 given choices. This dataset contains 1220 labels for 76 tasks collected from 177 workers. ‚Ä¢Plot (Krivosheev et al., 2020) is a dataset for multiple-choice tasks where the task is to identify a movie from a description of its plot from 10 given choices. Only workers who correctly solved the first 10 test questions can answer the rest of the tasks. A total of 1937 labels are collected from 122 workers for 100 tasks. Table 4 shows a summarized information for the introduced datasets. Table 4. Dataset information Dataset # workers # tasks # labels or choices sparsity dtask dworker Adult2 269 333 4 0.037 10.0 12.4 Dog 109 807 4 0.092 10.0 74.0 Web 176 2653 5 0.033 5.9 88.3 Flag 220 100 10 0.073 16.0 7.3 Food 177 54 5 0.125 22.1 6.7 Plot 122 56 10 0.293 35.7 16.4 Color 196 1000 6 0.1 19.5 99.4 A.2. Top- Tmodel: extension of the Top-Two model In this section, we also show that our top-two model (1)can be generalized to have T‚â•2plausible answers. The distribution of the response Aijcan be defined as follows: Aij=Ô£± Ô£¥Ô£¥Ô£≤ Ô£¥Ô£¥Ô£≥gjtfort‚àà[T] w.p.s  piqjt+1‚àípi K ; eachb‚àà[K]\{gj1, . . . , g jT}w.p.s pi(1‚àíPT t=1qjt) +1‚àípi K ; 0, w.p.1‚àís,(19) where gj1, . . . , g jTrepresent the Tplausible answers, and qj1, . . . , q jTare the associated confusion probabilities with respect to the ground truth. Without loss of generality, let the ground truth answer of the task jbegj1, where we assume qj1‚â•qj2¬∑¬∑¬∑ ‚â• qjT>1‚àíPT t=1qjt. Similar to the top-two model, we can define a binary converted observation matrices A(k)for1‚â§k < K , which enjoy the rank-1 structure. The analysis of the binary converted observation matrices reveals that‚àÜr(k) jin (2) can be represented as below ‚àÜr(k) j=( 1 K‚àíqjt fork=gjt, t‚àà[T], 1 Kotherwise .(20) Thus, we can estimate the top- Tplausible answers for each task by finding the lowest- Tvalues of ‚àÜr(k) j,k‚àà[K]. We can also obtain qjtfrom1 K‚àí‚àÜr(t) j. Based on this observation, we can generalize Algorithm 1 of the top-two model to Algorithm 3 of the top- Tmodel. 14Recovering Top-Two Answers and Confusion Probability in Multi-Choice Crowdsourcing Algorithm 3 Spectral Method for Initial Estimation (Top- T1 Algorithm) 1:Input: data matrix A1‚àà {0,1, . . . , K }n√ómand parameter Œ∑ >0where Œ∑‚àön‚â§ ‚à•p‚à•2‚â§‚àön. 2:Randomly split (with equal probabilities) and convert A1into binary matrices X(k)‚àà {‚àí 1,0,1}n√ómandY(k)‚àà {‚àí1,0,1}n√ómfor1‚â§k < K as described in Sec. 4.1. 3:Letu(k)be the leading normalized left singular vector of X(k). Trim the abnormally large components of u(k)by setting them to zero if u(k) i>2 Œ∑‚àönand denote the resulting vector as Àúu(k). 4:Calculate the estimate of ‚à•p‚à•r(k)by defining v(k):=1 s‚Ä≤(Y(k))‚ä§Àúu(k).Assume v(0):=0andv(K):=0. 5:Fork‚àà[K], calculate ‚àÜv(k) j:=v(k) j‚àív(k‚àí1) j.Estimate the top- Tanswers for j‚àà[m]by ÀÜgjt:= arg min k‚àà[K],kÃ∏=ÀÜgj1,...,ÀÜgj(t‚àí1)‚àÜv(k) j, t‚àà[T].(21) 6:Estimate ‚à•p‚à•2bylj:=K K‚àíTP k/‚àà{gj1,...,g jT}‚àÜv(k) jandl:=1 mPm j=1lj. 7:Estimate qjtforj‚àà[m]andt‚àà[T]by defining ÀÜqjt:= 1/K‚àí‚àÜv(ÀÜgjt) j/l. (22) 8:Output: estimated top-T answers {ÀÜgj1, . . . , ÀÜgjT}m j=1and confusion probability matrix ÀÜq. To proceed to the second stage, we also generalize Algorithm 2 of the top-two model to Algorithm 4 of the top- Tmodel by defining the estimate of the worker reliability in a similar way as (5), but for the case of the top- Tmodel: ÀÜpi=K (K‚àíT)Ô£´ Ô£≠1 ms(1‚àís1)mX j=11(A2 ij‚àà {ÀÜgj1, . . . , ÀÜgjT})‚àíT KÔ£∂ Ô£∏. (23) We then apply the Maximum Likelihood Estimator (MLE) using ÀÜpandÀÜq. See Algorithm 4 for details. Algorithm 4 Plug-in MLE (Top- T2 Algorithm) 1:Input: data matrix A‚àà {0,1, . . . , K }n√ómand the sample splitting rate s1>0. 2:Randomly split AintoA1andA2by defining A1:=A‚ó¶SandA2=A‚ó¶( 1n√óm‚àíS)where Sis ann√ómmatrix whose entries are i.i.d. with Bern( s1) and‚ó¶is an entrywise product. 3:Apply Algorithm 1 to A1to yield estimates for top-T answers {ÀÜgj1, . . . , ÀÜgjT}m j=1and confusion probability vector ÀÜq. 4:By using {ÀÜgj1, . . . , ÀÜgjT}m j=1andA2, calculate the estimate ÀÜpas in (23). 5:By using the whole Aand(ÀÜp,ÀÜq), find the plug-in MLE estimates {ÀÜgMLE j1, . . . , ÀÜgMLE jT}m j=1by arg max a1,...,a T‚àà[K]TnX i=1TX t=1logKÀÜpiÀÜqjt 1‚àíÀÜpi+ 1 1(Aij=at) (24) 6:Output: estimated top-two answers {ÀÜgMLE j1, . . . , ÀÜgMLE jT}m j=1 Although theoretical analysis needs to be changed accordingly, the model and algorithms can be easily extended to the general case of T‚â•2plausible answers as above, since the binary-converted observation matrices still enjoy the rank-1 structure. Generalizing the theoretical analysis will be an interesting open problem. 15Recovering Top-Two Answers and Confusion Probability in Multi-Choice Crowdsourcing (a) Images with lowest q(considered to be hard)  (b) Images with highest q(considered to be easy) Figure 5. Training images with (a) lowest and (b) highest confusion probabilities. B. Experimental Details for Neural Network Training We show the details of the experiments presented in Sec. 6.3. B.1. Datasets The CIFAR10H dataset (Peterson et al., 2019) consists of 511,400 human classifications by 2,571 participants which were collected via Amazon Mechanical Turk. Each participant classified 200 images, 20 from each category. Every 20 tasks, a trivial question is presented to prevent random guessing, and participants who scored below 75% were excluded from the dataset. We present the images with the lowest/highest qfrom the training samples in Fig 5. The image with a lower q means that the first answer and the second answer are hard to distinguish. B.2. Model We train two simple CNN architectures, VGG-19 and ResNet-18, to show the usefulness of the second answer and the confusion probability. For each model, our loss function is defined as the cross-entropy between the softmax output and the two-hot vector (in which the values are qand1‚àíqforgandh, respectively). We compare the results of our top-two label training with those of full-distribution training and hard label (one-hot vector) training. B.3. Training We train each model using 10-fold cross validation (using 90% of images for training and 10% images for validation) and average the results across 5 runs. We run a grid search over learning rates, with the base learning rate chosen from {0.1, 0.01, 0.001}. We find 0.1 to be optimal in all cases. We train each model for a maximum of 150 epochs using SGD optimizer with a momentum of 0.9 and a weight decay of 0.0001. Our neural networks are trained using NVIDIA GeForce 3090 GPUs. B.4. Training neural networks with corrupted CIFAR10H datasets The CIFAR10H dataset is collected from workers whose reliability is above 75%, so that the full label distribution is in fact almost the same as the top-two distribution. To analyze the robustness against the label noise, we conduct an additional experiment by adding different portions of random responses to the original CIFAR10H dataset. In the experiment, we add the responses from spammers, who provide random labels on each image, to the original dataset, with the varying ratio of [0.1,0.2,0.3,0.4,0.5]. For example, if the ratio of spammers is 0.5, it means that we add the same number of responses from spammers as the original dataset. The exact number of the added responses is (# of added responses) =(spammer ratio) 1‚àí(spammer ratio)√ó(# of total responses) . (25) As in the experiments of Sec.6.3, we train two neural networks, ResNet18 and VGG-19, with the top-two label distribution and the full label distribution as the spammer ratio increases. Table 5 shows the test accuracy of the trained neural networks. As shown in the table, the top-two label training outperforms the full label training in the high spammer ratio regime. This is because the training with the full label distribution tries to fit the model to all the collected answers, which include the responses from spammers. On the other hand, training with the top-two labels is more robust against the label noise, since it focuses on the simple yet meaningful side information, the ground-truth label and the most confusing label with the ratio between the two in the collected answers. 16Recovering Top-Two Answers and Confusion Probability in Multi-Choice Crowdsourcing Table 5. Comparison of performances for the corrupted CIFAR10H dataset with top2/full label training spammer ratioResNet18 VGG-19 top-two full top-two full 0.1 80.18¬±1.30% 80.73¬±0.79% 78.90¬±0.72% 78.67¬±1.45% 0.2 80.30¬±1.81% 79.79¬±0.59% 79.10¬±0.64% 78.65¬±0.91% 0.3 79.80¬±0.44% 79.23¬±0.79% 79.08¬±1.22% 77.80¬±1.08% 0.4 79.05¬±0.78% 76.82¬±0.75% 79.15¬±1.46% 77.40¬±1.09% 0.5 78.40¬±0.96% 75.88¬±0.93% 78.22¬±0.69% 76.11¬±1.53% C. Baseline Methods In this section, we explain the baseline methods with which we compare the performance of our algorithms. To analyze the performance in recovering the top-two answers, we considered the ML-based algorithms, including the Spectral-EM algorithm (MV-D&S and OPT-D&S) (Zhang et al., 2014), Projected Gradient Descent (PGD) (Ma et al., 2018), M-MSR (Ma & Olshevsky, 2020), MultiSPA (Ma & Olshevsky, 2020), and EBCC (Li et al., 2019), which provide a ‚Äúscore‚Äù for each label so that we can recover the top-two answers. ‚Ä¢Spectral-EM algorithm (MV-D&S and OPT-D&S) (Zhang et al., 2014) is a two-stage algorithm for multi-class crowd labeling problems. These algorithms are built for the D&S model where each worker has his/her own confusion matrix. In the first stage of the algorithm, the confusion matrix of each worker is estimated via spectral method (OPT-D&S) or majority voting (MV-D&S), respectively, and in the second stage, the estimates for the confusion matrices are refined by optimizing the objective function of the D&S estimator via the Expectation Maximization (EM) algorithm. ‚Ä¢Projected Gradient Descent (PGD) (Ma et al., 2018) is an approach to estimate the skills of each worker in the single-coin D&S model. The authors formulate the skill estimation problem as a rank-one correlation-matrix completion problem. They propose a projected gradient descent method to solve the correlation-matrix completion problem. ‚Ä¢M-MSR (Ma & Olshevsky, 2020) algorithm is an approach to estimate the reliability of each worker in the multi-class D&S model. M-MSR algorithm utilizes that the rank of the response matrix is one. To estimate the reliability of the workers, they use update rules to find the left singular vector and right singular vector of the response matrix. In this process, the extreme values are filtered out to guarantee the stable convergence of the algorithm. ‚Ä¢MultiSPA-EM (Ibrahim et al., 2019) is an approach to estimate each worker‚Äôs confusion matrix using pairwise co- occurrence matrix. To estimate the confusion matrices, three SPA (successive projection algorithm)-based algorithms are proposed; MultiSPA, MultiSPA-KL and MultiSPA-EM. MultiSPA utilizes the second order statistics to obtain the confusion matrices and the ground truth. MultiSPA-KL is an iterative optimization method to minimize the KL-divergence between the expectation of the co-occurrences and the empirical co-occurrences, where the initial estimates are obtained from MultiSPA. MultiSPA-EM is an EM based algorithm where the initial estimates are obtained from MultiSPA. Since the MultiSPA-EM outperforms MultiSPA and MultiSPA-KL, we only include these two in our baselines. ‚Ä¢EBCC (Li et al., 2019) algorithm is an enhanced version of the Baysian classifier combination model. The authors assume that each label has its own subtypes. Each subtype has different probability distribution even if the label is the same. EBCC algorithm utilizes the Expectation-Maximization (EM) algorithms to recover the hidden variables and estimates the true labels. D. Synthetic Experiments D.1. Additional plots for synthetic data experiments in Sec. 6.1 In Section 6.1, we devised four scenarios described in Table 1 to verify the robustness of our model for various (p,q)ranges, with(n, m, s ) = (50 ,500,0.2). The performance of algorithms is measured by the empirical average error probabilities 17Recovering Top-Two Answers and Confusion Probability in Multi-Choice Crowdsourcing Figure 6. Prediction error for (gj, hj)(top row), gj(middle) and hj(bottom) for four scenarios. Our algorithm (TopTwo2) achieves the best performance, near the oracle MLE for all the scenarios. in recovering gj,hjand(gj, hj), i.e.,1 mPm j=1P(ÀÜgjÃ∏=gj),1 mPm j=1P(ÀÜhjÃ∏=hj)and1 mPm j=1P((ÀÜgj,ÀÜhj)Ã∏= (gj, hj))and plotted in Fig. 6. We can observe that for all the considered scenarios TopTwo2 achieves the best performance, near the oracle MLE, in recovering (gj, hj). Depending on scenarios though, the reason TopTwo2 outperforms can be explained differently. For Easy scenario, since qjis close to 1, it becomes easy to distinguish gjfrom hjbut hard to distinguish hj from other labels. Our algorithm achieves the best performance in estimating hjby a large margin. For Hard scenario, it becomes hard to distinguish gjandhj, but our algorithm, which uses an accurate ÀÜqj, can better distinguish gjandhj. ForFew-smart , our algorithm achieves the largest gain compared to other methods, since our algorithm can effectively distinguish few smart workers from spammers. High-variance show the effect of having diverse qjin a dataset. D.2. Robustness of our methods In this section, we present a set of four additional synthetic experiments to demonstrate the robustness of our methods, Alg. 1 and Alg. 2 (referred to as TopTwo1 and TopTwo2). In each experiment, we change a parameter of our synthetic error model and compare the prediction error of our algorithms to the baselines: majority voting(MV), MV-D&S (Zhang et al., 2014), PGD (Ma et al., 2018), MultiSPA-KL and MultiSPA-EM(Ibrahim et al., 2019), EBCC(Li et al., 2019) and Oracle-MLE. We measure the performance of each algorithm by the empirical average error probabilties in recovering the ground truth gj, the most confusing answer hjand the pair of top two (gj, hj), i.e.,1 mPm j=1P(ÀÜgjÃ∏=gj),1 mPm j=1P(ÀÜhjÃ∏=hj)and 1 mPm j=1P((ÀÜgj,ÀÜhj)Ã∏= (gj, hj)). Obviously, Oracle-MLE provides a lower bound for the performance. Changing the dimension of observed matrix : We first check the robustness of our methods against the change of 18Recovering Top-Two Answers and Confusion Probability in Multi-Choice Crowdsourcing (a) Effect of the number of workers on the performance (b) Effect of the number of tasks on the performance (c) Effect of the variance of worker reliability on the performance (d) Effect of the variance of task difficulty on the performance (e) Effect of the portion of spammers on the performance Figure 7. Prediction error for (gj, hj)(first column), gj(second column), and hj(third column) for five different setups. The solid lines represent the mean prediction errors of each algorithm averaged over 10 runs, and the shaded regions represent the standard deviations. 19Recovering Top-Two Answers and Confusion Probability in Multi-Choice Crowdsourcing dimensions of the observation matrix A‚àà {0,1. . . , K }n√ómwithn‚â§m. We vary the number of workers ( n) or the number of tasks ( m) while fixing the other dimension. The default values of nandmare 50 and 500, respectively, and the sampling probability sis fixed as 0.1throughout the experiments. The worker reliability piand the task difficulty qjis sampled uniformly at random from [0,1]and(1/2,1], respectively, for all i‚àà[n]andj‚àà[m]. In Fig. 7a and 7b, we report the results when we change nfor a fixed mands, or when we change mfor a fixed nand s, respectively. From Fig. 7a, we can see that as the number of workers increases, the performance of every algorithm improves since the number of samples per task scales as nsfor a fixed s. Our algorithm achieves the performance close to the Oracle-MLE for all the considered range, which implies that the worker reliabilities {pi}are well estimated with our methods. From Fig. 7b, we can see that our algorithm achieves a robust performance against the change in the number of tasks, although the performance gets closer to that of Oracle-MLE as the number of tasks increases. Since our method uses SVD in the first stage, the larger dimension is beneficial for the concentration of the random perturbation matrix with respect to the expectation of the observation matrix. This phenomenon is observed for other baseline methods as well, which are based on the spectral method. Changing the variance of worker reliability : In this experiment, we change the range of pi, the parameter for worker skill/reliability, for i‚àà[n], with a fixed mean in order to observe the impact of the variance of the worker reliability on the prediction error. We randomly sample pifrom the window [0.5‚àíx,0.5 +x]withxvarying from 0.05to0.25. The mean of the worker reliability is fixed as 0.5. As shown in Fig. 7c, when the variance of the worker reliability increases, the baseline methods estimating worker reliabilities perform better than the majority voting. Our TopTwo2 algorithm achieves the best performance close to Oracle-MLE, as the standard deviation increases, i.e., as the workers become more heterogeneous. Changing the variance of task difficulty : We also design an experiment to observe the impact of the variance of qj,j‚àà[m], the parameter for task difficulty, on the prediction error. We randomly sample qjfrom the window [0.75‚àíx,0.75 +x]with xvarying from 0.05to0.25. The mean of the worker reliability is fixed as 0.75. If the variance of the task difficulty is small, it could be sufficient to only estimate the worker reliability since all the tasks have almost the similar task difficulties. As shown in Fig. 7d, when the variance of the task difficulty increases, our TopTwo2 algorithm performs better than the other baselines. This is the evidence for the validity of our method in estimating the task difficulty. Changing the portion of spammers : Spammers who provide random answers always exist in crowdsourcing systems. To improve the inference performance, it is important to distinguish spammers from reliable workers. In our experimental setup, we define a spammer as a worker whose reliability parameter piis in the range [0,0.1]. We change the portion of spammers among the workers from 0.1to0.9and compare the prediction error of our methods to those of other baseline methods. In Fig. 7e, we can see that our algorithm achieves the best performance among all the considered baselines except Oracle- MLE, which can exactly distinguish spammers from reliable workers. This result demonstrates the superiority of our methods in detecting spammers compared to other methods. D.3. Estimating the worker reliability vector and the task difficulty vector In this section, we examine the accuracy of our estimates for the worker reliability vector pand the task difficulty vector q. The worker reliability is estimated by ÀÜpdefined in (5)of Algorithm 2 and the task difficulty is estimated by ÀÜqdefined in (4) of Algorithm 1. To analyze the accuracy of these estimators, we compute the mean squared error (MSE),1 n‚à•ÀÜp‚àíp‚à•2 2and 1 m‚à•ÀÜq‚àíq‚à•2 2, respectively. To analyze the estimation accuracy for the worker reliability, we first sample piuniformly at random from [0,1]for all i‚àà[n]and fix the worker reliability vector p. Then, we randomly sample the task difficulty vector q‚àà(1/2,1]mfifty times and then sample the observation matrices from the distribution (1)for each (p,q)pair with a fixed p. For each observation matrix, we subsample the data with varying probabilities and apply Algorithm 2 to get the estimate ÀÜp, which is then used to calculate the MSE of p. We report the MSE averaged over these fifty cases. Similarly, to analyze the estimation accuracy for the task difficulty, we randomly sample and fix a task difficulty vector q‚àà(1/2,1]mand generate fifty different observation matrices while varying the worker reliability vector p. We again report the MSE averaged over these fifty cases. The number of workers and that of tasks is set to be (50,500) for the worker reliability estimation, and to be (100,1000) for the task difficulty estimation. In Fig. 8a and 8b, we plot the MSE for pandq, respectively, as the average number of queries per task increases. We can 20Recovering Top-Two Answers and Confusion Probability in Multi-Choice Crowdsourcing see that both for pandq, the MSEs converge to near zero as the average number of queries per task increases. However, estimating the task difficulty requires more number of samples as our theory (13) suggests. (a) Mean squared error1 n‚à•ÀÜp‚àíp‚à•2 2  (b) Mean squared error1 m‚à•ÀÜq‚àíq‚à•2 2 Figure 8. Mean squared errors in estimating the worker reliability vector p(left) and the task difficulty vector q(right), respectively. E. Discussion of theoretical results In this section, we present a discussion of the main theoretical results. ‚Ä¢Theorem 1 asserts that the sampling probability of ‚Ñ¶ 1 Œ¥2 1‚à•p‚à•2 2logK œµ is sufficient to recover the top-two answers (gj, hj)for any task j‚àà[m]and to estimate the confusion probability qjwith accuracy of |ÀÜqj‚àíqj|< Œ¥1by Algorithm 1 with probability at least 1‚àíœµ. Combined with Theorem 3 part (a), we can see that this sample complexity is the minimax optimal rate for a fixed collective quality of workers, measured by ‚à•p‚à•2 2. ‚Ä¢Theorem 2 shows that when we have an entrywise bound on the estimated worker reliability vector pand the task difficulty vector q, the plug-in MLE estimator, used in Algorithm 2, guarantees the recovery of top-two answers if the sampling probability s= ‚Ñ¶(log(m/œµ) n¬ØD)where ¬ØD, which depend on (p,q), indicates the average reliability of workers in distinguishing the top-two answers from any other pairs for the most difficult task. Combined with Theorem 3 part (b), we can see that this sample complexity is the minimax optimal rate for any (p,q), ignoring the logarithmic terms. ‚Ä¢ Combining the conditions for the accurate estimation of model parameters in (13) and the convergence of the plug-in MLE (Theorem 2), Corollary 2 shows the condition on the sample complexity to guarantee the performance of Algorithm 2. F. Proof of Proposition 1 For each task jand label k, define four indicator functions: Œ†a(j, k) := 1(gj> k, h j> k), Œ†b(j, k) := 1(gj‚â§k, hj> k), Œ†c(j, k) := 1(gj> k, h j‚â§k), Œ†d(j, k) := 1(gj‚â§k, hj‚â§k),(26) which satisfy Œ†a(j, k) + Œ† b(j, k) + Œ† c(j, k) + Œ† d(j, k) = 1 . For notational simplicity, we will often drop (j, k)fronŒ†‚àó. The pmf of A(k)is given by A(k) ij=Ô£± Ô£¥Ô£≤ Ô£¥Ô£≥‚àí1with probability s(1‚àíœÅ(k) ij), 1 with probability sœÅ(k) ij, 0 with probability 1‚àís,(27) 21Recovering Top-Two Answers and Confusion Probability in Multi-Choice Crowdsourcing where œÅ(k) ij= Œ† a(j, k)pi+ Œ† b(j, k)pi(1‚àíqj) + Œ† c(j, k)piqj+(K‚àík)(1‚àípi) K, and its expectation is E[A(k) ij] = s(2œÅ(k) ij‚àí1).Note that by using Œ†a= 1‚àíŒ†b‚àíŒ†c‚àíŒ†d, the probability œÅ(k) ijcan be written as œÅ(k) ij= pi  qj(Œ†c‚àíŒ†b)‚àí(Œ†c+ Œ†d) +k K +K‚àík K.Thus, by defining r(k) j:=qj(Œ†c‚àíŒ†b)‚àí(Œ†c+ Œ†d) +k K, (28) the expectation of A(k) ijcan be written as E[A(k) ij] =s(2œÅ(k) ij‚àí1) = s 2pir(k) j+K‚àí2k K , (29) and E[A(k)]‚àís(K‚àí2k) K1n√óm= 2sp(r(k))‚ä§. (30) Note that Case I: gj> hj Œ†a(j, k) = 1 where k < h j, Œ†c(j, k) = 1 where hj‚â§k < g j, Œ†d(j, k) = 1 where gj‚â§k;Case II: gj< hj Œ†a(j, k) = 1 where k < g j, Œ†b(j, k) = 1 where gj‚â§k < h j, Œ†d(j, k) = 1 where hj‚â§k.(31) Thus, r(k) jin (28) is equal to Case I: gj> hj r(k) j=Ô£± Ô£¥Ô£≤ Ô£¥Ô£≥k Kwhere k < h j; k K‚àí(1‚àíqj)where hj‚â§k < g j; k K‚àí1 where gj‚â§k,Case II: gj< hj r(k) j=Ô£± Ô£¥Ô£≤ Ô£¥Ô£≥k Kwhere k < g j; k K‚àíqjwhere gj‚â§k < h j; k K‚àí1 where hj‚â§k. G. Performance Analysis of Algorithm 1 G.1. Proofs of Theorem 1 and Corollary 1 In Algorithm 1, we use the data matrix A1, which is obtained by randomly splitting the original data matrix AintoA1and A2with probability s1and(1‚àís1), respectively. Then, the first stage of Algorithm 1 begins with randomly splitting A1 again into two independent matrices BandCwith equal probabilities, and then converting BandCinto(K‚àí1)-binary matrices B(k)andC(k)as explained in Sec. 3. We define X(k)andY(k)asX(k):=B(k)‚àís‚Ä≤(K‚àí2k) K1n√ómand Y(k):=C(k)‚àís‚Ä≤(K‚àí2k) K1n√ómwhere s‚Ä≤=s¬∑s1/2. We have E[X(k)] =E[Y(k)] =s‚Ä≤p(r(k))‚ä§from Prop. 1. For notational simplicity, we will ignore this random splitting for a moment and just pretend that X(k)andY(k)are sampled independently with s‚Ä≤=sthroughout this section. We first outline the proof. Based on the observation that E[X(k)] =sp(r(k))‚ä§, ifE[X(k)]is available we can recover p‚àó=p ‚à•p‚à•2by SVD, and by using p‚àóit is possible to recover ‚à•p‚à•2r(k), which then reveals {(gj, hj)}m j=1as well as qfrom the relation in (2). To estimate p‚àófromX(k), we first bound the spectral norm of the perturbation, ‚à•X(k)‚àíE[X(k)]‚à•2. We then use this bound and Wedin Sin Œòtheorem to bound sinŒ∏(u(k),p‚àó)where u(k)is the left singular vector of X(k)with the largest singular value. We trim the abnormally large components of u(k)and denote the resulting vector by Àúu(k). After trimming, it is still possible to show that sinŒ∏(Àúu(k),p‚àó)can be bounded in the same order as that of sinŒ∏(u(k),p‚àó). Finally, we provide an entrywise bound between v(k)=2 s(Y(k))‚ä§Àúu(k)and‚à•p‚à•2r(k)in Lemma 5, which is the main lemma to prove Theorem 1. We state our main technical lemmas first and then prove Theorem 1. Let us define the perturbation matrix E:=X(k)‚àíE[X(k)] =B(k)‚àís(K‚àí2k) K1n√óm‚àísp(r(k))‚ä§=B(k)‚àíE[B(k)] (32) 22Recovering Top-Two Answers and Confusion Probability in Multi-Choice Crowdsourcing where B(k) ij=Ô£± Ô£¥Ô£≤ Ô£¥Ô£≥‚àí1w.p.s(1‚àíœÅ(k) ij), 1 w.p.sœÅ(k) ij, 0 w.p.1‚àís,(33) andœÅ(k) ij= Œ† a(j, k)pi+ Œ†b(j, k)pi(1‚àíqj) + Œ† c(j, k)piqj+(K‚àík)(1‚àípi) Kfor(Œ†a,Œ†b,Œ†c,Œ†d)defined in (26). For the perturbation matrix Ein (32), we have E[Ei,j] = 0,and|Ei,j| ‚â§2,1‚â§i‚â§n,1‚â§j‚â§m, (34) and also var(Eij) = var( B(k) ij) =E[(B(k) ij)2]‚àí(E[B(k) ij])2 =s‚àí(s(œÅ(k) ij‚àí1/2))2‚â§s.(35) Note that {Eij}are independent across all i, j. Define ŒΩ:= maxÔ£± Ô£≤ Ô£≥max iX jE[E2 i,j],max jX iE[E2 i,j]Ô£º Ô£Ω Ô£æ‚â§max{m, n}s. (36) By applying the spectral norm bound to random matrices with independent entires, appeared in (Bandeira & Van Handel, 2016) and summarized in Theorem 4, we can bound the spectral norm of Eas below. Lemma 2 (Spectral norm bound of E).With probability 1‚àí(n+m)‚àí8, we have ‚à•E‚à• ‚â§4p smax ( m, n) + Àúcp log(n+m) (37) for some constant Àúc >0when m‚â•n. For some sufficiently large m, assuming n=o(m)ands= ‚Ñ¶(log( n+m)/m), the spectral norm of Ecan be further bounded by ‚à•E‚à• ‚â§5‚àösm. (38) Using the bounded spectral norm of Ein(38) and applying the Wedin Sin Œòtheorem, summarized in Theorem 5, we can bound the angle between u(k)andp‚àó. Lemma 3. For some sufficiently large m, assuming n=o(m)ands= ‚Ñ¶(log( n+m)/m), we have sinŒ∏(u(k),p‚àó)‚â§Œò(1/‚àösn) (39) with probability at least 1‚àí(n+m)‚àí8. Proof. By applying the Wedin Sin ŒòTheorem (Theorem 5), we have sinŒ∏(u(k),p‚àó)‚â§‚àö 2‚à•E‚à• s‚à•p‚à•2¬∑ ‚à•r(k)‚à•2‚àí ‚à•E‚à•. (40) We have ‚à•p‚à•2= Œò(‚àön)and‚à•r(k)‚à•2= Œò(‚àöm)by assumptions on model parameters. By Lemma 2, for some sufficiently largem, assuming n=o(m)ands= ‚Ñ¶(log( n+m)/m), we have ‚à•E‚à• ‚â§5‚àösmwith probability at least 1‚àí(n+m)‚àí8. Combining these bounds, we get sinŒ∏(u(k),p‚àó)‚â§Œò(‚àösm) Œò(s‚àömn)‚àíŒò(‚àösm)=1 Œò (‚àösn). (41) We trim the abnormally large components of u(k)by letting it zero if u(k) i>2/(Œ∑‚àön)and denote the resulting vector as Àúu(k). This process is required to control the maximum entry size of Àúu(k), which is used later in the proof. For the next lemma, we show that after the trimming process, the norm of Àúu(k)is still close to 1 and the angle between Àúu(k)andp‚àóhas the same order as that of sinŒ∏(u(k),p‚àó). 23Recovering Top-Two Answers and Confusion Probability in Multi-Choice Crowdsourcing Lemma 4. Given‚à•p‚àó‚à•2‚â•Œ∑‚àön, we have ‚à•Àúu(k)‚à•2‚â•q 1‚àí50 sin2Œ∏(u(k),p‚àó), (42) sinŒ∏(Àúu(k),p‚àó)‚â§6‚àö 2 sinŒ∏(u(k),p‚àó). (43) The proof of Lemma 4 is provided in Section G.2. Finally, we provide our main lemma giving the entrywise bound on the difference between v(k)=1 s(Y(k))‚ä§Àúu(k)and ‚à•p‚à•2r(k). Lemma 5 (Entrywise Bound) .For any Œ¥1, œµ > 0, and any task j‚àà[m]and label index k‚àà[K], if the sampling probability s‚â•Œò 1 Œ¥2 1‚à•p‚à•2 2log1 œµ ,then we can guarantee P1 sD Y(k) ‚àój,Àúu(k)E ‚àí ‚à•p‚à•2r(k) j< Œ¥1‚à•p‚à•2 >1‚àíœµ (44) asm‚Üí ‚àû when n=O(m/logm). Proof. For notional simplicity, denote Œ∏(Àúu(k),p‚àó)byŒ∏. To prove (44), we show bounds on two probabilities, P1 sD Y(k) ‚àój,Àúu(k)E ‚àí ‚à•Àúu(k)‚à•2‚à•p‚à•2r(k) jcosŒ∏>Œ¥1‚à•p‚à•2 2 < œµ/2, (45) P‚à•Àúu(k)‚à•2‚à•p‚à•2r(k) jcosŒ∏‚àí ‚à•p‚à•2r(k) j>Œ¥1‚à•p‚à•2 2 < œµ/2. (46) Then, the triangle inequality implies (44). We first prove (45). Remind that we do the random splitting of the input matrix Aand define the two independent binary- converted matrices as X(k)andY(k), for1‚â§k < K , which are used to estimate Àúu(k)andv(k), respectively. Thus, Àúu(k)is independent from Y(k)and this independence is used when we bound the first and second moments of v(k) j=1 s‚ü®Y(k) ‚àój,Àúu(k)‚ü©. For any 1‚â§j‚â§m, the first and second moments of v(k) j=1 s‚ü®Y(k) ‚àój,Àúu(k)‚ü©satisfy E1 sD Y(k) ‚àój,Àúu(k)E =‚ü®p,Àúu(k)‚ü©r(k) j=‚à•p‚à•2‚à•Àúu(k)‚à•2(cosŒ∏)r(k) j= Œò(‚àön) (47) ifr(k) jÃ∏= 0by Lemma 3 and 4, and var1 sD Y(k) ‚àój,Àúu(k)E ‚â§1 s2nX i=1(Àúu(k) i)2E[(Y(k) ij)2] = Œò1 s (48) sinceE[(Y(k) ij)2] = Œò( s)andPn i=1(Àúu(k) i)2= Œò(1) by Lemma 3 and 4. Furthermore, we have max 1‚â§i‚â§m|Y(k) ijÀúu(k) i| ‚â§ Œò 1‚àön since Àúu(k) i‚â§2 Œ∑‚àön. By applying the Bernstein‚Äôs inequality, we can show that P1 sD Y(k) ‚àój,Àúu(k)E ‚àí ‚à•Àúu(k)‚à•2‚à•p‚à•2r(k) jcosŒ∏>Œ¥1‚à•p‚à•2 2 ‚â§2 exp  ‚àíŒò(Œ¥2 1‚à•p‚à•2 2) Œò 1 s + Œò ( Œ¥1‚à•p‚à•2/‚àön)! ‚â§exp  ‚àíŒò(sŒ¥2 1‚à•p‚à•2 2)(49) where the second inequality is due to the assumption ‚à•p‚à•2= Œò(‚àön). To make this probability less thanœµ 2, it is sufficient to haves‚â•‚Ñ¶ 1 Œ¥2 1‚à•p‚à•2 2log1 œµ . We next prove (46) by bounding‚à•Àúu(k)‚à•2‚à•p‚à•2r(k) jcosŒ∏‚àí ‚à•p‚à•2r(k) j. By the triangle inequality, we have ‚à•Àúu(k)‚à•2‚à•p‚à•2r(k) jcosŒ∏‚àí ‚à•p‚à•2r(k) j‚â§‚à•Àúu(k)‚à•2‚à•p‚à•2r(k) jcosŒ∏‚àí ‚à•p‚à•2r(k) jcosŒ∏ +‚à•p‚à•2r(k) jcosŒ∏‚àí ‚à•p‚à•2r(k) j.(50) 24Recovering Top-Two Answers and Confusion Probability in Multi-Choice Crowdsourcing Note that 1 ‚à•p‚à•2¬∑‚à•Àúu(k)‚à•2‚à•p‚à•2r(k) jcosŒ∏‚àí ‚à•p‚à•2r(k) jcosŒ∏=r(k) jcosŒ∏‚à•Àúu(k)‚à•2‚àí1 ‚â§Œò(sin2Œ∏(u(k),p‚àó)) =1 Œò (ns),(51) with probability 1‚àí(n+m)‚àí8by Lemma 3 and 4, and also note that 1 ‚à•p‚à•2¬∑‚à•p‚à•2r(k) jcosŒ∏‚àí ‚à•p‚à•2r(k) j=r(k) j(1‚àícosŒ∏) ‚â§Œò(sin2Œ∏(u(k),p‚àó)) =1 Œò (ns),(52) with probability 1‚àí(n+m)‚àí8by Lemma 3 and 4. To make these errors of order 1/Œò (ns)less thanŒ¥1 2, it is sufficient to haves‚â•‚Ñ¶ 1 Œ¥1n . By combining the above results, it can be guaranteed that1 2sD Y(k) ‚àój,Àúu(k)E ‚àí ‚à•p‚à•2r(k) j< Œ¥‚à•p‚à•2with probability at least 1‚àíœµ, if the sampling probability s‚â•max ‚Ñ¶1 Œ¥2 1‚à•p‚à•2 2log1 œµ ,‚Ñ¶1 Œ¥1n = ‚Ñ¶1 Œ¥2 1‚à•p‚à•2 2log1 œµ (53) where the last equality is due to ‚à•p‚à•2= Œò(‚àön). The condition s= ‚Ñ¶(log( n+m)/m)in Lemma 3 is immediately satisfied by (53) when n=O(m/logm). Proof of Theorem 1. By using Lemma 5, we next prove Theorem 1. By applying the union bound over k‚àà[K], if s‚â•Œò 1 Œ¥2 1‚à•p‚à•2 2logK œµ then we have ‚à•p‚à•2(r(k) j‚àíŒ¥1)‚â§v(k) j=1 sD Y(k) ‚àój,Àúu(k)E ‚â§ ‚à•p‚à•2(r(k) j+Œ¥1),‚àÄk‚àà[K] (54) for any Œ¥1>0andj‚àà[m]with probability at least 1‚àíœµ. Under the condition (54), for any qj‚àà(1/2,1)and Œ¥ <minn 2qj‚àí1 2,1‚àíqj 2o , we can guarantee that 1 K‚àíqj+Œ¥ <1 K‚àí(1‚àíqj)‚àíŒ¥and1 K‚àí(1‚àíqj) +Œ¥ <1 K‚àíŒ¥, (55) which implies (ÀÜgj,ÀÜhj) = (gj, hj)for(ÀÜgj,ÀÜhj)defined in (3). This proves (9) of Theorem 1. We next prove (10), the accuracy guarantee in estimating the task difficulty vector q. After estimating ‚à•p‚à•2r(k)by v(k)=1 s(Y(k))‚ä§Àúu(k), we estimate ‚à•p‚à•2by calculating lwhere lj:=K K‚àí2P kÃ∏=ÀÜgj,kÃ∏=ÀÜhj‚àÜv(k) jandl:=1 mPm j=1lj. Assume that |‚à•p‚à•2‚àíl| ‚â§ ‚à•p‚à•2Œ¥‚Ä≤.We will specify the required order of Œ¥‚Ä≤later. Remind that the estimate for qjis defined asÀÜqj:=1 K‚àí‚àÜv(ÀÜgj) j l.Under the condition that ÀÜgj=gjand|vj‚àí ‚à•p‚à•2r(k) j| ‚â§ ‚à•p‚à•2Œ¥1, both of which are satisfied under the conditions of Lemma 5, we have  1 K‚àíqj‚àí2Œ¥1 1 +Œ¥‚Ä≤‚â§‚àÜv(ÀÜgj) j l‚â§ 1 K‚àíqj+ 2Œ¥1 1‚àíŒ¥‚Ä≤. (56) By the Taylor expansion for1 1‚àíx= 1 + x+ Œò(x2)asx‚Üí0, we have |ÀÜqj‚àíqj| ‚â§2Œ¥1+Œ¥‚Ä≤1 K‚àíqj+ 2Œ¥1 + Œò(Œ¥‚Ä≤2) = Œò( Œ¥1+Œ¥‚Ä≤). (57) 25Recovering Top-Two Answers and Confusion Probability in Multi-Choice Crowdsourcing Thus, both the order of Œ¥‚Ä≤, which is the estimation error of ‚à•p‚à•2, and that of Œ¥, which is the estimation error of ‚à•p‚à•2r(k) j, govern the estimation accuracy of qj. We next show that we can have Œ¥‚Ä≤= Œò( Œ¥1). By Lemma 5, we have |vj‚àí ‚à•p‚à•2r(k) j| ‚â§ ‚à•p‚à•2Œ¥1, which implies ‚à•p‚à•2(‚àÜr(k) j‚àí2Œ¥1)‚â§‚àÜv(k) j‚â§ ‚à•p‚à•2(‚àÜr(k) j+ 2Œ¥1). (58) Under the condition (ÀÜgj,ÀÜhj) = (gj, hj), since ‚àÜr(k) j=1 KforkÃ∏= ÀÜgj,ÀÜhj, we have ‚à•p‚à•2‚àí ‚à•p‚à•22Œ¥1K K‚àí2‚â§lj=K K‚àí2X kÃ∏=ÀÜgj,kÃ∏=ÀÜhj‚àÜv(k) j‚â§ ‚à•p‚à•2+‚à•p‚à•22Œ¥1K K‚àí2, (59) and thus Œ¥‚Ä≤=2Œ¥1K K‚àí2= Œò( Œ¥1). Thus, it is enough to have s= ‚Ñ¶ 1 Œ¥2 1‚à•p‚à•2 2logK œµ to guarantee (10). Proof of Corollary 1. By using Lemma 5 and taking the union bound over all tasks j‚àà[m]as well as k‚àà[K], we can prove Corollary 1 in a similar way as that of Theorem 1. G.2. Proof of Lemma 4 We first prove (42), ‚à•Àúu(k)‚à•2‚â•q 1‚àí50 sin2Œ∏(u(k),p‚àó). LetIbe the set of indices 1‚â§i‚â§nsuch that u(k) i‚â•2 Œ∑‚àön. Then, we have u(k) i‚àíp‚àó i‚â•1 Œ∑‚àönfor all i‚ààIsince p‚àó i=pi/‚à•p‚à•2‚â§1 Œ∑‚àöndue to the assumption that ‚à•p‚à•2 2‚â•Œ∑2n. Thus, we have |I| Œ∑2n‚â§X i‚ààI(u(k) i‚àíp‚àó i)2‚â§ ‚à•u(k)‚àíp‚àó‚à•2 2. (60) By using the triangle inequality, we can show that sX i‚ààI u(k) i2 ‚â§vuutX i‚ààI u(k) i‚àí2 Œ∑‚àön2 +s 4|I| Œ∑2n ‚â§vuutX i‚ààI p‚àó i‚àí2 Œ∑‚àön2 +sX i‚ààI u(k) i‚àíp‚àó i2 +s 4|I| Œ∑2n ‚â§s 4|I| Œ∑2n+sX i‚ààI u(k) i‚àíp‚àó i2 +s 4|I| Œ∑2n ‚â§5‚à•u(k)‚àíp‚àó‚à•2.(61) Therefore, we get 1‚â• ‚à•Àúu(k)‚à•2 2= 1‚àíX i‚ààI(u(k) i)2‚â•1‚àí25‚à•u(k)‚àíp‚àó‚à•2 2. (62) By the law of cosine, we have ‚à•p‚àó‚àíu(k)‚à•2 2= sin2Œ∏(u(k),p‚àó) + (1 ‚àícosŒ∏(u(k),p‚àó))2= 2‚àí2 cosŒ∏(u(k),p‚àó) = 2 1‚àíq 1‚àísin2Œ∏(u(k),p‚àó) = 2sin2Œ∏(u(k),p‚àó) 1 +q 1‚àísin2Œ∏(u(k),p‚àó) ‚â§2 sin2Œ∏(u(k),p‚àó).(63) Combining (62) and (63) proves (42). 26Recovering Top-Two Answers and Confusion Probability in Multi-Choice Crowdsourcing We next prove (43), sinŒ∏(Àúu(k),p‚àó)‚â§6‚àö 2 sinŒ∏(u(k),p‚àó). First, note that ‚à•Àúu(k)‚àíu(k)‚à•2 2=P i‚ààI u(k) i2 . We have sinŒ∏(Àúu(k),p‚àó)‚â§ ‚à•Àúu(k)‚àíp‚à•2‚â§ ‚à•Àúu(k)‚àíu(k)‚à•2+‚à•u(k)‚àíp‚àó‚à•2‚â§6‚à•u(k)‚àíp‚àó‚à•2 (64) where the last inequality is from (61). Combined with (63), we get (43). H. Performance Analysis of Algorithm 2 H.1. Proof of Lemma 1 In this lemma, we show that conditioned on (ÀÜgj,ÀÜhj) = (gj, hj)for all j‚àà[m], ifs(1‚àís1) = ‚Ñ¶ 1 Œ¥2mlogn œµ , the estimator ÀÜpidefined in (5), ÀÜpi=K (K‚àí2)Ô£´ Ô£≠1 s(1‚àís1)Ô£´ Ô£≠1 mmX j=11(A2 ij= ÀÜgjorÀÜhj)Ô£∂ Ô£∏‚àí2 KÔ£∂ Ô£∏, guarantees P(‚à•p‚àíÀÜp‚à•‚àû< Œ¥2)‚â•1‚àíœµfor any œµ >0. Given (ÀÜgj,ÀÜhj) = (gj, hj)for all j‚àà[m], since A2is independent of (ÀÜgj,ÀÜhj), we have Eh 1(A2 ij= ÀÜgjorÀÜhj)i =P(A2 ij= ÀÜgjorÀÜhj) =s(1‚àís1)K‚àí2 Kpi+2 K , var 1(A2 ij= ÀÜgjorÀÜhj) ‚â§s(1‚àís1).(65) By applying the Bernstein‚Äôs inequality, we can show that PÔ£´ Ô£≠mX j=1 1(A2 ij= ÀÜgjorÀÜhj)‚àís(1‚àís1)K‚àí2 Kpi+2 K>(K‚àí2)ms(1‚àís1)Œ¥2 KÔ£∂ Ô£∏ ‚â§expÔ£´ Ô£¨Ô£≠‚àí1 2 (K‚àí2)ms(1‚àís1)Œ¥2 K2 ms(1‚àís1) +1 3(K‚àí2)ms(1‚àís1)Œ¥2 KÔ£∂ Ô£∑Ô£∏‚â§exp  ‚àíŒò  ms(1‚àís1)Œ¥2 2 .(66) Thus, if the sampling probability satisfies s(1‚àís1) = ‚Ñ¶1 mŒ¥2 2log1 œµ , (67) then we can guarantee that P(|ÀÜpi‚àípi|< Œ¥2)‚â•1‚àíœµ.By taking the union bound over i‚àà[n], if the sampling probability satisfies s(1‚àís1) = ‚Ñ¶1 mŒ¥2 2logn œµ , (68) then we can guarantee that P(‚à•ÀÜp‚àíp‚à•‚àû< Œ¥2)‚â•1‚àíœµ. H.2. Proof of Theorem 2 To prove this theorem, we use similar proof techniques from (Zhang et al., 2014). Since the work in (Zhang et al., 2014) focuses on the recovery of only the ground-truth label for each task, we generalize the techniques to recover not only the ground-truth label but also the most confusing answer. We first introduce some notations. Let ¬µ(i,j) (a,b),kdenote the probability that a worker i‚àà[n]gives label k‚àà[K]for the assigned task j‚àà[m]of which the top-two answers are (gj, hj) = (a, b). Let¬µ(i,j) (a,b)= [¬µ(i,j) (a,b),1¬µ(i,j) (a,b),2¬∑¬∑¬∑¬µ(i,j) (a,b),K]‚ä§. 27Recovering Top-Two Answers and Confusion Probability in Multi-Choice Crowdsourcing We introduce a quantity that measures the average ability of workers in distinguishing the ground-truth pair of top-two answers (gj, hj)from any other pair (a, b)‚àà[K]2/{(gj, hj)}for the task j‚àà[m]. We define D(j):= min (gj,hj)Ã∏=(a,b)1 nnX i=1DKL ¬µ(i,j) (gj,hj),¬µ(i,j) (a,b) ;D:= min j‚àà[m]D(j), (69) where DKL(P, Q) :=P iP(i) log( P(i)/Q(i))is the KL-divergence between PandQ. Note that D(j)is strictly positive if qj‚àà(1/2,1)and there exists at least one worker iwithpi>0for the distribution (1), so that (gj, hj)can be distinguished from any other (a, b)‚àà[K]2/{(gj, hj)}statistically. We define Das the minimum of D(j)overj‚àà[m], indicating the average ability of workers in distinguishing (gj, hj)from any other (a, b)for the most difficult task in the set. Let us define an event that will be shown holding with high probability, E:nX i=1KX k=11(Aij=k) logÔ£´ Ô£≠¬µ(i,j) (gj,hj),k ¬µ(i,j) (a,b),kÔ£∂ Ô£∏‚â•nsD/2for all j‚àà[m]and(a, b)‚àà[K]√ó[K]\(gj, hj). (70) Define li:=KX k=11(Aij=k) log ¬µ(i,j) (gj,hj),k/¬µ(i,j) (a,b),k . (71) We can see that l1, . . . , l nare mutually independent on any value of (gj, hj), and each libelongs to the interval [0,log(1/œÅ)] where ¬µ(i,j) (gj,hj),c‚â•œÅfor all (i, j, g j, hj, c)‚àà[n]√ó[m]√ó[K]3. We can easily show that E""nX i=1li(gj, hj)# =nX i=1sDKL ¬µ(i,j) (gj,hj),¬µ(i,j) (a,b) . (72) We define D:=nX i=1DKL ¬µ(i,j) (gj,hj),¬µ(i,j) (a,b) . (73) The following lemma shows that the second moment of liis bounded above by the KL-divergence between the label distribution under (gj, hj)pair and the label distribution under (a, b)pair. Lemma 6. Conditioning on any value of (gj, hj), we have E l2 i|(gj, hj) ‚â§2 log(1 /œÅ) 1‚àíœÅsDKL ¬µ(i,j) (gj,hj),¬µ(i,j) (a,b) . (74) The proof of this lemma can be obtained by following the proof of the similar result, Lemma 4 of (Zhang et al., 2014). According to Lemma 6, the aggregated second moment of liis bounded by E""nX i=1l2 i(gj, hj)# ‚â§2 log(1 /œÅ) 1‚àíœÅnX i=1sDKL ¬µ(i,j) (gj,hj),¬µ(i,j) (a,b) =2 log(1 /œÅ) 1‚àíœÅsD.(75) Thus, applying the Bernstein‚Äôs inequality, we have P""nX i=1li‚â•sD/2(gj, hj)# ‚â•1‚àíexp  ‚àí1 2(sD/2)2 2 log(1 /œÅ) 1‚àíœÅsD+1 3(2 log(1 /œÅ))(sD/2)! . (76) 28Recovering Top-Two Answers and Confusion Probability in Multi-Choice Crowdsourcing Since œÅ‚â§1/2andD‚â•nD(j)‚â•nD, combining the above inequality with union bound over j‚àà[m], we have P[E]‚â•1‚àímK2exp ‚àínsD 33 log(1 /œÅ) . (77) The maximum likelihood estimator finds a pair of (a, b)‚àà[K]2,aÃ∏=b, maximizing (ÀÜgj,ÀÜhj) = arg max (a,b)‚àà[K]2,aÃ∏=bnY i=1P(Aij|p, qj,(a, b)) = arg max (a,b)‚àà[K]2,aÃ∏=bnX i=1logP(Aij|p, qj,(a, b)) = arg max (a,b)‚àà[K]2,aÃ∏=bnX i=1KX k=11(Aij=k) log¬µ(i,j) (a,b),k. (78) The plug-in MLE in (6), on the other hand, finds a pair of (a, b)‚àà[K]2,aÃ∏=b, maximizing (ÀÜgj,ÀÜhj) = arg max (a,b)‚àà[K]2,aÃ∏=bnX i=1KX k=11(Aij=k) log ÀÜ¬µ(i,j) (a,b),k(79) where ÀÜ¬µ(i,j) (a,b),kis the estimated probability that a worker i‚àà[n]gives label k‚àà[K]for the assigned task j‚àà[m]of which the top two answers are (gj, hj) = (a, b)assuming pi= ÀÜpifrom (5)andqj= ÀÜqjfrom (4)in the distribution (1). Thus, for the plug-in MLE to correctly find the ground-truth top two answers (gj, hj), we need to satisfy the following event: nX i=1KX k=11(Aij=k) log ÀÜ¬µ(i,j) (gj,hj),k/ÀÜ¬µ(i,j) (a,b),k ‚â•0for all (a, b)‚àà[K]√ó[K]\(gj, hj). (80) For any arbitrary (a, b)Ã∏= (gj, hj), consider the quantity Q(a,b):=nX i=1KX k=11(Aij=k) log ÀÜ¬µ(i,j) (gj,hj),k/ÀÜ¬µ(i,j) (a,b),k , (81) which can be written as Q(a,b)=nX i=1KX k=11(Aij=k) log¬µ(i,j) (gj,hj),k ¬µ(i,j) (a,b),k+nX i=1KX k=11(Aij=k)Ô£Æ Ô£∞logÔ£´ Ô£≠ÀÜ¬µ(i,j) (gj,hj),k ¬µ(i,j) (gj,hj),kÔ£∂ Ô£∏‚àílogÔ£´ Ô£≠ÀÜ¬µ(i,j) (a,b),k ¬µ(i,j) (a,b),kÔ£∂ Ô£∏Ô£π Ô£ª. (82) Assuming that there exist œÅ > Œ¥ 3such that ¬µ(i,j) (a,b),k‚â•œÅand|ÀÜ¬µ(i,j) (a,b),k‚àí¬µ(i,j) (a,b),k| ‚â§Œ¥3for all i‚àà[n], j‚àà[m],(a, b)‚àà[K]2, (83) we have max i‚àà[n],k‚àà[K]Ô£Æ Ô£∞logÔ£´ Ô£≠ÀÜ¬µ(i,j) (gj,hj),k ¬µ(i,j) (gj,hj),kÔ£∂ Ô£∏‚àílogÔ£´ Ô£≠ÀÜ¬µ(i,j) (a,b),k ¬µ(i,j) (a,b),kÔ£∂ Ô£∏Ô£π Ô£ª‚â§2 logœÅ œÅ‚àíŒ¥3 . (84) By the Bernstein‚Äôs inequality, we also have P""nX i=1KX k=11(Aij=k)‚àíns> ns/ 2# ‚â§exp ‚àí1 2(ns/2)2 ns+1 3(ns/2) = exp ‚àí3ns 28 . (85) By taking the union bound over j‚àà[m], we have P""nX i=1KX k=11(Aij=k)‚àíns> ns/ 2for any j‚àà[m]# ‚â§mexp ‚àí3ns 28 . (86) 29Recovering Top-Two Answers and Confusion Probability in Multi-Choice Crowdsourcing Under the intersection of the eventPn i=1PK k=11(Aij=k)‚àíns‚â§ns/2for all j‚àà[m]and the event E, we can guarantee Q(a,b)=nX i=1KX k=11(Aij=k) log¬µ(i,j) (gj,hj),k ¬µ(i,j) (a,b),k+nX i=1KX k=11(Aij=k)Ô£Æ Ô£∞logÔ£´ Ô£≠ÀÜ¬µ(i,j) (gj,hj),k ¬µ(i,j) (gj,hj),kÔ£∂ Ô£∏‚àílogÔ£´ Ô£≠ÀÜ¬µ(i,j) (a,b),k ¬µ(i,j) (a,b),kÔ£∂ Ô£∏Ô£π Ô£ª ‚â•nsD 2‚àí3nslogœÅ œÅ‚àíŒ¥3 ‚â•nsD 2‚àí3Œ¥3 œÅ‚àíŒ¥3 >0(87) for every j‚àà[m]where the last inequality holds if Œ¥3< œÅD 6 +D. (88) In summary, under that the eventPn i=1PK k=11(Aij=k)‚àíns‚â§ns/2for all j‚àà[m]and the event Ehold, if we have Œ¥3such that |ÀÜ¬µ(i,j) (a,b),k‚àí¬µ(i,j) (a,b),k| ‚â§Œ¥3for all i‚àà[n], j‚àà[m],(a, b)‚àà[K]2(89) and Œ¥3< œÅand Œ¥3< œÅD 6 +D, (90) then we can guarantee that the plug-in MLE in (79) successfully recovers the pair of top two (gj, hj)for all the tasks j‚àà[m]. To make the right-hand side of (77) and (86) less than œµ/2, it is sufficient to have s= ‚Ñ¶log(1/œÅ) log( mK2/œµ) + log( m/œµ) nD . (91) Lastly, when we have max{‚à•p‚àíÀÜp‚à•‚àû,‚à•q‚àíÀÜq‚à•‚àû} ‚â§Œ¥, (92) we can guarantee that |ÀÜ¬µ(i,j) (a,b),k‚àí¬µ(i,j) (a,b),k| ‚â§4Œ¥:=Œ¥3. (93) Thus, it is sufficient to guarantee (92) with Œ¥ <minœÅ 4,œÅD 4(6 + D) . (94) I. Proof of Theorem 3 I.1. Proof of part (a) To prove this minimax bound, we use the similar arguments from (Karger et al., 2014). In particular, we consider a spammer-hammer model such that pi=( 0,for1‚â§i‚â§ ‚åä(1‚àíp)n‚åã 1,otherwise.(95) Assume that total ljworkers randomly sampled from [n]provide answers for the task j. Under the spammer-hammer model, the oracle estimator makes a mistake on task jwith probability (K‚àí1)/Kif it is only assigned to spammers. When ljis the number of assignments, we have P(ÀÜgjÃ∏=gj) =K‚àí1 K(1‚àíp)lj. (96) By convexity and using Jensen‚Äôs inequality, the average probability of error is lower bounded by 1 mX j‚àà[m]P(ÀÜgjÃ∏=gj)‚â•K‚àí1 K(1‚àíp)l(97) 30Recovering Top-Two Answers and Confusion Probability in Multi-Choice Crowdsourcing where1 mP i‚àà[m]li‚â§l. By assuming p‚â§2/3, we have (1‚àíp)‚â•e‚àí(p+p2). Thus, min ÀÜgmax p‚ààFp,g‚àà[K]m1 mX j‚àà[m]P(ÀÜgjÃ∏=gj)‚â•K‚àí1 Ke‚àí(p+p2)l‚â•K‚àí1 Ke‚àí2pl. (98) The inequality in (98) implies that if lis less than1 2plog K‚àí1 Kœµ , then no algorithm can make the minimax error in (98) less than œµ. Since the average number of queries per task in our model is ns, it implies that it is necessary to have s= ‚Ñ¶ 1 ‚à•p‚à•2 2log1 œµ . I.2. Proof of part (b) To prove the second part of the theorem, we use proof techniques from (Zhang et al., 2014), but generalizes the results for pair of top two answers. We assume that jc‚àà[m],(gc, hc)‚àà[K]2and(ac, bc)‚àà[K]2are the task index and the pairs of labels such that D=1 nnX i=1DKL ¬µ(i,jc) (gc,hc),¬µ(i,jc) (ac,bc) (99) forDdefined in (69). LetQbe a uniform distribution over the set {(gc, hc),(ac, bc)}m. For any (ÀÜg,ÀÜh), we have max (v,u)‚àà[K]m√ó[K]m vjÃ∏=uj,‚àÄj[m]EÔ£Æ Ô£∞mX j=11((ÀÜgj,ÀÜhj)Ã∏= (gj, hj))(g,h) = (v,u)Ô£π Ô£ª ‚â•mX j=1X (v,u)‚àà{(gc,hc),(ac,bc)}mQ((v,u))Eh 1((ÀÜgj,ÀÜhj)Ã∏= (gj, hj))(g,h) = (v,u)i(100) LetA:={Aij:i‚àà[n], j‚àà[m]}be the set of observations. Define two probability measures P0andP1, such that P0is the measure of Aconditioned on (gj, hj) = (gc, hc), while P1is that on (gj, hj) = (ac, bc). Then, we can have X (v,u)‚àà{(gc,hc),(ac,bc)}mQ((v,u))Eh 1((ÀÜgj,ÀÜhj)Ã∏= (gj, hj))(g,h) = (v,u)i =Q((gj, hj) = (gc, hc))P0((ÀÜgj,ÀÜhj)Ã∏= (gc, hc)) +Q((gj, hj) = (ac, bc))P1((ÀÜgj,ÀÜhj)Ã∏= (ac, bc)) ‚â•1 2‚àí1 2‚à•P0‚àíP1‚à•TV ‚â•1 2‚àí1 4p DKL(P0,P1).(101) where the second to the last inequality is by Le Cam‚Äôs method and the last inequality is by Pinsker‚Äôs inequality.4 Conditioned on (gj, hj), the set of random variables Aj:={Aij:i‚àà[n]}are independent of A\Ajfor both P0andP1, and thus DKL(P0,P1) =DKL(P0(Aj),P1(Aj)) +DKL(P0(A\Aj),P1(A\Aj)) =DKL(P0(Aj),P1(Aj)) (102) where P(X)denote the distribution of Xwith respect to the probability measure P. Given (gj, hj), since A1j, . . . , A njare independent, we can show that DKL(P0(Aj),P1(Aj)) =nX i=1DKL(P0(Aij),P1(Aij)) =nX i=1 (1‚àís) log1‚àís 1‚àís+sDKL ¬µ(i,j) (gc,hc),¬µ(i,j) (ac,bc) ‚â•snD.(103) 4The total variation distance between probability distributions PandQdefined on a set Xis defined as the maximum difference between probabilities they assign on subsets of X:‚à•P‚àíQ‚à•TV:= supA‚äÇX|P(A)‚àíQ(A)|. 31Recovering Top-Two Answers and Confusion Probability in Multi-Choice Crowdsourcing Combining (100)‚Äì (103), we have max (v,u)‚àà[K]m√ó[K]m vjÃ∏=uj,‚àÄj[m]EÔ£Æ Ô£∞1 mmX j=11((ÀÜgj,ÀÜhj)Ã∏= (gj, hj))(g,h) = (v,u)Ô£π Ô£ª ‚â•1 2‚àí1 4p snD.(104) Thus, if s‚â§1 4nD, then the above inequality is lower bounded by 3/8. This completes the proof. J. Useful Inequalities In this section, we summarize the useful inequalities used in the proof of the main results. The following inequality, which appeared in (Bandeira & Van Handel, 2016) provides a non-asymptotic spectral norm bound for random matrices with independent random entries. Theorem 4 (Spectral norm bound of a random matrice with independent entries) .Consider a random matrix X‚ààRn√óm, whose entries are independently generated and obey E[Xi,j] = 0,and|Xi,j| ‚â§B, 1‚â§i‚â§n,1‚â§j‚â§m. (105) Define ŒΩ:= maxÔ£± Ô£≤ Ô£≥max iX jE[X2 i,j],max jX iE[X2 i,j]Ô£º Ô£Ω Ô£æ. (106) Then there exists some universal constant c >0such that for any t >0, P ‚à•X‚à• ‚â•4‚àöŒΩ+t	 ‚â§(n+m) exp ‚àít2 cB2 . (107) We also present a useful corollary of Theorem 4, which can be shown from (107) by setting Àúc=‚àö 9candt= Bp 9clog(n+m). Corollary 3 (Corollary of Theorem 4) .IfE[X2 i,j]‚â§œÉ2for all i, jand satisfying conditions in Theorem 4, then we have ‚à•X‚à• ‚â§4œÉp max( m, n) + ÀúcBp log(n+m) (108) with probability 1‚àí(n+m)‚àí8for some constant Àúc >0. We next summarize the eigenspace perturbation theory for asymmetric matrices with singular value composition (SVD). Suppose X:= [X0,X1]andZ:= [Z0,Z1]are orthonormal matrices. When we define the distance between two subspaces X0andZ0by dist(X0,Z0) :=‚à•X0X‚ä§ 0‚àíZ0Z‚ä§ 0‚à•, (109) then we have dist(X0,Z0) =‚à•X‚ä§ 0Z1‚à•=‚à•Z‚ä§ 0X1‚à•. (110) Given‚à•X‚ä§ 0Z0‚à• ‚â§1, we write SVD of X‚ä§ 0Z0‚ààRr√órasX‚ä§ 0Z0:=Ucos ŒòV‚ä§where cos Œò = diag(cosŒ∏1, . . . , cosŒ∏r). We call {Œ∏1, . . . , Œ∏ r}principal angles between X0andZ0. Then, we have ‚à•X‚ä§ 0Z1‚à•=‚à•sin Œò‚à•= max {|sinŒ∏1|,¬∑¬∑¬∑,|sinŒ∏r|}. (111) LetM‚àóandM=M‚àó+Ebe two matrices in Rn√ómwithn‚â§m, whose SVD are represented by M‚àó=Pn i=1œÉ‚àó iu‚àó iv‚àó i‚ä§ andM=Pn i=1œÉiuivi‚ä§, where œÉ1‚â• ¬∑¬∑¬∑ ‚â• œÉn(resp. œÉ‚àó 1‚â• ¬∑¬∑¬∑ ‚â• œÉ‚àó n). Let us define U0:= [u1,¬∑¬∑¬∑,ur]‚ààRn√ór,V0:= [v1,¬∑¬∑¬∑,vr]‚ààRm√ór. (112) The matrices U‚àó 0andV‚àó 0are defined analogously. 32Recovering Top-Two Answers and Confusion Probability in Multi-Choice Crowdsourcing Theorem 5 (Wedin sin Œò Theorem) .If‚à•E‚à•< œÉ‚àó r‚àíœÉ‚àó r+1, then one has max{‚à•dist(U0,U‚àó 0)‚à•,‚à•dist(V0,V‚àó 0)‚à•} ‚â§‚àö 2‚à•E‚à• œÉ‚àór‚àíœÉ‚àó r+1‚àí ‚à•E‚à•, (113) where U‚àó 0(V‚àó 0) andU0(V0) are subspaces spanned by the largest rleft (right) singular vectors of M‚àóandM, respecively. Lastly, we also write down two useful concentration inequalities. Theorem 6 (Hoeffding) .LetX1, X2, . . . , X nbe independent random variables such that Xi‚àà[ai, bi]for1‚â§i‚â§n. Then, we have P""nX i=1(Xi‚àíE[Xi])> t# ‚â§2 exp ‚àí2t2 Pn i=1(bi‚àíai)2 . (114) Theorem 7 (Bernstein) .LetX1, X2, . . . , X nbe independent random variables such that Xi‚àà[ai, bi]for1‚â§i‚â§n. Let C:= max 1‚â§i‚â§n(bi‚àíai)andœÉ2=Pn i=1var(Xi). Then we have P""nX i=1(Xi‚àíE[Xi])> t# ‚â§2 exp ‚àít2/2 œÉ2+C¬∑t/3 . (115) 33"
7,https://arxiv.org,2301.00007,10.48550/arXiv.2301.00007,"Agnieszka Niemczynowicz, Rados≈Çaw A. Kycia, Maciej Jaworski, Artur Siemaszko, Jose M. Calabuig, Lluis M. Garc√≠a-Raffi, Baruch Schneider, Diana Berseghyan, Irina Perfiljeva, Vilem Novak, Piotr Artiemjew","Selected aspects of complex, hypercomplex and fuzzy neural networks","This short report reviews the current state of the research and methodology on theoretical and practical aspects of Artificial Neural Networks (ANN). It was prepared to gather state-of-the-art knowledge needed to construct complex, hypercomplex and fuzzy neural networks. The report reflects the individual interests of the authors and, by now means, cannot be treated as a comprehensive review of the ANN discipline. Considering the fast development of this field, it is currently impossible to do a detailed review of a considerable number of pages. The report is an outcome of the Project 'The Strategic Research Partnership for the mathematical aspects of complex, hypercomplex and fuzzy neural networks' meeting at the University of Warmia and Mazury in Olsztyn, Poland, organized in September 2022.","arXiv:2301.00007v2  [cs.LG]  20 Oct 2023Selected aspects of complex, hypercomplex and fuzzy neural networks edited by Agnieszka Niemczynowicz1and Rados/suppress law A. Kycia2,3 1Faculty of Mathematics and Computer Science, University of Warmia and Mazury in Olsztyn, Poland 2Faculty of Computer Science and Telecommunications, T. Ko¬¥ sciusz ko Cracow University of Technology, Krak¬¥ ow, Poland 3Faculty of Science, Masaryk University, Brno, Czechia January 3, 20232Contents 1 Introduction 5 2 Biological inspiration of artiÔ¨Åcial neural networks 7 R.A. Kycia, A. Niemczynowicz, M. Jaworski 2.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 2.2 Perceptron . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 2.3 Multilayer neural networks and backpropagation algorithm . . . . . . . . . . . . . . 8 2.4 Current state of development . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 2.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 3 Classical architecture of artiÔ¨Åcial neural networks 11 A. Niemczynowicz, R.A. Kycia, M. Jaworski 3.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 3.2 Encoding of data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 3.3 Multilayer ANN . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 3.4 Other architectures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 3.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 4 Dynamical systems approach to artiÔ¨Åcial neural networks 1 5 R.A. Kycia, A. Siemaszko 4.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 4.2 Biological Neural Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 4.3 Physics-motivated Neural Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 4.4 Modelling ArtiÔ¨Åcial Neural Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 4.4.1 Modelling ANN work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 4.4.2 Modelling of learning process . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 4.4.3 Neural ODEs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 4.5 Dynamical Models predicted by ANN . . . . . . . . . . . . . . . . . . . . . . . . . . 17 4.6 Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 5 Neural networks as universal approximators 21 J.M. Calabuig, Ll.M. Garc¬¥ ƒ±a-RaÔ¨É 34 CONTENTS 6 Complex and quaternionic neural networks 25 B. Schneider, D. Berseghyan 6.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 6.2 Elements of quaternionic analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26 6.3 Poincar¬¥ e-Bertrand formula for œà-hyperholomorphic singular integrals . . . . . . . . . 28 6.4 Poincar¬¥ e-Bertrand formula for the Cauchy-Cimmino singular int egrals . . . . . . . . 31 7 Fuzzy neural networks 39 I. PerÔ¨Åljeva, V. NovakChapter 1 Introduction This short report reviews the current state of the research and methodology on theoretical and practical aspects of ArtiÔ¨Åcial Neural Networks (ANN). It was pr epared to gather state-of-the-art knowledge needed to construct complex, hypercomplex and fuzzy neural networks. The report reÔ¨Çects the individual interests of the authors and, b y now means, cannot be treated as a comprehensive review of the ANN discipline. Considering the fast development of this Ô¨Åeld, it is currently impossible to do a detailed review of a considerable number of pages. The report is an outcome of the Project Meeting1at the University of Warmia and Mazury in Olsztyn, Poland, organized in September 2022. The contributors of the report are (in order of appearance): ‚Ä¢A. Niemczynowicz, UWM Olsztyn ‚Ä¢R.A. Kycia, CUT Cracow & MUNI Brno ‚Ä¢M. Jaworski, CUT Cracow ‚Ä¢A. Siemaszko, UWM Olsztyn ‚Ä¢J.M. Calabuig, UPV Valencia ‚Ä¢Ll.M. Garc¬¥ ƒ±a-RaÔ¨É, UPV Valencia ‚Ä¢B. Schneider, UO Ostrava ‚Ä¢D. Berseghyan, UO Ostrava ‚Ä¢I. PerÔ¨Åljeva, UO Ostrava ‚Ä¢V. Novak, UO Ostrava ‚Ä¢P. Artiemjew, UWM Olsztyn 1Project: ‚ÄôThe Strategic Research Partnership for the mathe matical aspects of complex, hypercomplex and fuzzy neural networks‚Äô 56 CHAPTER 1. INTRODUCTION Acknowledgement The report has been supported by the Polish National Agency for A cademic Exchange Strategic Partnership Programme under Grant No. BPI/PST/2021/1/0003 1.Chapter 2 Biological inspiration of artiÔ¨Åcial neural networks R.A. Kycia, A. Niemczynowicz, M. Jaworski 2.1 Introduction The abilities of the human brain always inspired scientists to mimic its abilit ies. There are plenty of functionalities oÔ¨Äered by this tissue. We can mention a few: ‚Ä¢Pattern recognition - recognition of objects, sounds, smells, tou ch. ‚Ä¢ClassiÔ¨Åcation - distinguish similar objects. ‚Ä¢Generation - digesting the input data and generating new output: m ovement, sound. The structure of the brain at microscopic size was not revealed unt il the invention of the micro- scope anddiscoveryofmicroorganismsbyVan Leeuwenhoekin 1676 . The distinction ofneuroncells as a basic building block of neurons has also a long history that evolves with the ideas of how the neural tissue works. However the neuron theory, so called Neuro n Doctrine, was proposed by San- tiago Ram¬¥ on y Cajal (1852-1934). Since then the detailed study o f biological and electrochemical properties of neurons were performed. There are many diÔ¨Äerent kinds of neurons depending on which part o f the neural system we are analyzing. The complexity of the problem arises due to the comple x structure of neurons and their mutual interactions. The mathematical description of the ne uron in terms of, e.g., dynamical systems has a long history [8] and grows into the discipline of computa tional neurobiology [16]. This also inspired computer scientists to use similar structures as ne urons and nervous systems for computational tasks. In this section we present the history o f these early attempts. The Ô¨Årst attempt was the model of a single neuron as discussed below. 78CHAPTER 2. BIOLOGICAL INSPIRATION OF ARTIFICIAL NEURAL NET WORKS 2.2 Perceptron The mathematical ideas behind connection of neurons was present ed in [9], and the Ô¨Årst implemen- tation of an artiÔ¨Åcial neuron, called perceptron, was designed at C ornell Aeronautical Laboratory by F. Rosenblatt as an electrical circuit and described in the report [14]. However, in the appendix of the report the mathematical equations for the perceptron ar e provided. Inthesimplestversiontheperceptronisabletodistinguishtwosets ofdatathatcanbeseparated by a linear hyperplane. In mathematical terms the input contains a n-dimensional vector of real numbersXi= [x1,...,xn], and the associated to this feature that can be coded into a numbe rfi. The perceptron is characterized as a vector of nweights (real numbers) w= [w1,...,wn] and an additional number w0called the bias unit. In order to make the computations more uniform the input vetor is extended to Xi= [1,x1,...,xn] and the weight vector to W= [w0,w1,...,wn]. The next element of the perceptron is a decision function, which can be t he Heaviside step function Œ∏(x) = 1 ifx >0,Œ∏(0) = 0 and Œ∏(x) =‚àí1 ifx <0. Then the data are classiÔ¨Åed with respect to the side of the hyperplane given by the equation xw= 0, byy=Œ∏(xw) function. The position of the plane (weights w) are taken such that to minimize the distance between the feature sfiand corresponding outputs yi=Œ∏(WXi). The implementation of the perceptron can be found in every standard machine learning books, e.g., [12]. In the very core of the perceptron idea is that the eÔ¨Äective classiÔ¨Å cation is possible if the data are linearly separable, i.e., separable by a hyperplane. This idea was pr oved in [11] and is related to the XOR problem. Since the XOR gate can be recognized as a funct ion that takes as an input two bits and returns the one bit, the input data for the function ca n be treated as input data for perceptron, and the features can be output of XOR function. It is obvious that these data are not linearly separable, and therefore, there is no possibility that perce ptrons separate features 0 from 1 be a hyperplane (in fact, a line on the plane). The ideas in the book we re inÔ¨Çuential enough to initiate, so called, (Ô¨Årst) AI winter , the period around 1980s where the perceptron idea was put on hold in favor of other AI architectures. 2.3 Multilayer neural networks and backpropagation algo- rithm The problem of not linearly separated data was proven to be solved b y showing that the connection of a few perceptrons in a network called neural network, and learn ing by back-propagatingthe error from the output through the network updating the weights is a solu tion of XOR problem. These ideas were described in [15] and started a renovation of the interes ts in artiÔ¨Åcial neurons. However, due to insuÔ¨Écient computing power needed for backpropagationalg orithm the practical works were stalled up to 2010s and this period is called second AI winter. The appearance of connected artiÔ¨Åcial neurons gave rise to the n ew paradigm of computing called connectionism that computing can be included in the topology (g raph ofconnections between neurons in a ANN), see [1]. 2.4 Current state of development Since the 2010s the increased interest in deep learning, i.e., use of ne ural networks for practical computational tasks is reviving. This situation is due to an increasing number of examples where2.5. SUMMARY 9 neural networks can handle data achieving better performance t han classical algorithms. This progress is also induced by the high interests in deep learning from th e biggest IT companies. The most common architectures used in applications are multilayer ne ural networks, where neurons are grouped in layers and each layer‚Äôs output is an input to a nother layer. The simpliÔ¨Åcation of construction of multilayer networks was provide d by the appearance of two powerful OpenSource libraries: ‚Ä¢TensorFlow (Google Brain Team [5]) - donated by Google; For common u se it is accessed by Keras frontend [2]. ‚Ä¢PyTorch (Meta AI [10]) - donated by Adam Paszke, Sam Gross, Soum ith Chintala, and Gregory Chanan. These two libraries have enormous infrastructure that was built ar ound them and are considered as an industrial standard of deep learning. As for the theoretical side of description of the neural networks , the insight in the idea of work of NN is expressed in, so called, Universal Approximation Theorems. The Ô¨Årst was proved by G. Cybenko in 1989 [3] for sigmoidal activation function. It was soon re alized that the approximation properties of neural networks rest in multilayer (feedforward) a rchitecture [7], [6]. Roughly stated, Universal Approximation Theorem says that for a speciÔ¨Åc topology (e.g., for arbitrary width and bounded depth) the output functions of feed-forward neural n etworks are dense in the space of continuousfunctionsonacompactspaceandwithsupremumnorm. Recentresearchinthisdirection focusesonestimatingtheoptimalwidthanddepthoflayerstoobta inbestapproximationproperties, see e.g., [13] for further references. There are many unresolved issues related to the work of ANN that a re related to the issue of which functions can be approximated by ANN or the properties of lea rning algorithms. They are summarized in the review article [4]. As it was pointed out, at the curre nt state of understanding there are still many unknowns however the big picture starts to em erge. 2.5 Summary Currently, the deep learning progress is motivated by numerous ap plications starting from com- puter vision, natural language processing, self-driving cars, and ending to generation of multimedia (pictures sounds), or design pharmaceutics. The development is p ushed by a big industry that has access to great computing powers needed to run learning algor ithms. However, the theoretical description, improvements of algorithms, or construction of nons tandard (e.g., not multilayers) ar- chitecture is still possible within the limiting computing resources. As o f 2022 it is still an active and promising area of research. Acknowledgements The article has been supported by the Polish National Agency for Ac ademic Exchange Strategic Partnership Programme under Grant No. BPI/PST/2021/1/0003 1.10 BIBLIOGRAPHY Bibliography [1] Buckner, Cameron, and James Garson. Connectionism (Stanford Encyclo- pedia of Philosophy). Stanford Encyclopedia of Philosophy, 18 May 1 997, https://plato.stanford.edu/entries/connectionism/ . Accessed 22 September 2022 [2] Chollet, F. Keraslibrary.Keras: the Pythondeep learningAPI, https://keras.io/ . Accessed 22 September 2022 [3] Cybenko, G. Approximation by superpositions of a sigmoidal function , Math. Control Sig- nal Systems, vol. 2, no. 1, 1989, pp. 303‚Äì314. https://doi.org/10.1007/BF02551274 , https://doi.org/10.1007/BF02551274 [4] Weinan, E., et al. Towards a Mathematical Understanding of Neural Network-Ba sed Ma- chine Learning: what we know and what we don‚Äôt , unpublished, vol. 1, no. 1, 2020, p. 56. https://arxiv.org/abs/2009.10713 [5] Google Brain Team. TensorFlow library. TensorFlow.org, 2015, https://www.tensorflow.org/ . Accessed 22 September 2022 [6] Hornik, K. Multilayer feedforward networks are universal approximat ors, Neural Net- works, vol. 4, no. 2, 1991, pp. 251-257. https://doi.org/10.1016/0893-6080(91)90009-T , https://doi.org/10.1016/0893-6080(91)90009-T . [7] Hornik, K., et al. Multilayer feedforward networks are universal approximat ors, Neural Net- works, vol. 2, no. 5, 1989, pp. 359-366. https://doi.org/10.1016/0893-6080(89)90020-8 , https://doi.org/10.1016/0893-6080(89)90020-8 . [8] Izhikevich, Eugene M. Dynamical Systems in Neuroscience: The Geometry of Excitab ility and Bursting, Penguin Random House LLC, 2010. [9] McCulloch, W.S., and W. A Pitts. A logical calculus of the ideas immanent in ner- vous activity , Bulletin of Mathematical Biophysics, vol. 5, no. 1, 1943, pp. 115‚Äì1 33, https://doi.org/10.1007/BF02478259 . [10] MetaAI. PyTorchlibrary.PyTorch.org, https://pytorch.org/ . Accessed22September 2022. [11] Minsky, M., and S.A. Papert. Perceptrons: and introduction to computational geometry , MIT Press, 1988. [12] Mirjalili, Vahid, and Sebastian Raschka. Python Machine Learning: Machine Learning and Deep Learning with Python, Scikit-learn, and TensorFlow 2 , Packt Publishing, 2019. [13] Park, Sejun, et al. Minimum Width for Universal Approximation , vol. 1, no. 1, 2020, p. 29. https://arxiv.org/abs/2006.08859 ,https://arxiv.org/abs/2006.08859 . [14] Rosenblatt, Frank. The Perceptron ‚Äì a perceiving and recognizing automaton , Report 85-460-1. Cornell Aeronautical Laboratory, 1957. [15] Rumelhart, D., et al. Learning representations by back-propagating errors , Nature, vol. 323, no. 1, 1986, pp. 533‚Äì536; https://doi.org/10.1038/323533a0 . [16] Trappenberg, Thomas. Fundamentals of Computational Neuroscience , OUP Oxford, 2010.Chapter 3 Classical architecture of artiÔ¨Åcial neural networks A. Niemczynowicz, R.A. Kycia, M. Jaworski 3.1 Introduction ArtiÔ¨Åcial Neural Networks (ANN) and Deep Learning discipline are cu rrently the one of the most active Ô¨Åelds of research in computer science. This activity is also insp ired by a large demand of IT business for new solutions and architectures that are suitable for solving new problems or solving more eÔ¨Éciently problems that are currently solvable by classical algo rithms. Apartfromvariousarchitectures,thereisanissueofhowtoenco dedatatomakethemacceptable as an input to ANN. The chapter is organized as follows: in the next section we acknowled ge standard input data for ANN. Then we review typical structures of multilayers ANN, which ar e currently the most used architectures. Finally we make a list of nonstandard architectures . 3.2 Encoding of data There are various types of data in the world. Many of them have sta ndard ways of encoding to the form that is suitable for ANN processing. We provide an example, and by no means extinguishable list of data types: ‚Ä¢Multidimensional numerical data that are transformable to vector or matrix forms. They are described in various Machine Learning books, e.g., [11] ‚Ä¢Images - transformable to matrix representation. See, e.g., [16] ‚Ä¢Textdata-transformabletovectorsofwords,e.g., BagOfWords ,TFIDF vectors,transformers based on neural networks; see, e.g., [9] ‚Ä¢Graph data - represented as, e.g., incidence matrix. See, e.g., [3] 1112CHAPTER 3. CLASSICAL ARCHITECTURE OF ARTIFICIAL NEURAL NET WORKS 3.3 Multilayer ANN The typical architecture used in industry is a (multilayer) feedforw ard architecture that consists of layers of neurons where output of one layer is the input of anoth er one. The acceleration in this direction was heavily induced by the appearance of two Open Sou rce libraries that allow the construction of complex ANN architectures. These libraries are: ‚Ä¢TensorFlow ([6]) - donated by Google company. ‚Ä¢PyTorch ([10]) - donated by Adam Paszke, Sam Gross, Soumith Chint ala, and Gregory Chanan. The general nomenclature in such architectures is as follows: ‚Ä¢Input layer - layer that gets the input data. ‚Ä¢Output layer - layer that returns the result of processing. ‚Ä¢Hidden layers - all layers between Input and Output layers. Within these frameworks are possible various multilayer architectur es and processing capabilities that will be outlined below. Fully connected multilayer ANN - this is a network where all neurons in a layer are connected with all neurons in neighborhood layers. Advantage of this architec ture is that all neurons ‚Äòsee‚Äô the whole output of the preceding layer. The main disadvantage of this s imple architecture is that the number of connections in ANN grows enormously with the increase of the number of neurons. Convolutional ANN (CNN) -the convolutionisamathematicaloperationofconnectingneighbo r input data (words when processing text, pixels when processing ima ges) to feed neurons with more complete yet local information. This makes sense when the data can be treated as elements of topological space where there is some notion of closure that repre sents some real objects, e.g., a group of neighbor pixels can represent a dog or bird; the context o f the sentence is represented by a sentence ofwords usually in close proximity. The convolutionin gener al is a tool for grouping ‚Äòclose‚Äô data together at the input, and moreover to provide some notion o f group invariance. In typical applications the convolution is realized by the kernels that are discre te versions of translational- invariant functions. However, the general idea is related to equiva riance with respect to group action [4] et al. In typical architecture of CNN the Ô¨Årst few layers a re convolutional layers that combine data and as a result reduce dimensionality of the data. Recurrent Neural Networks (RNN) [13] -these networkscanbe describedasadiscretedynamical system with feedback. The network is feeded by sequence of data and the output (of hidden layers) form the previous step. This kind of ‚Äúrecursive processing‚Äù allows t he network to see correlations between data from diÔ¨Äerent steps. Therefore such networks ar e ideal for text processing or time series predictions. The big drawback of this architecture is the com plicated process of learning. Since the learning process is iterative the gradient used in backward propagation algorithm is computed many times and this can makes it extremely small or to blow u p due to numerical manipulations. This is the so-called vanishing and exploding gradient pr oblem. These are typical problems with gradient-based learning algorithms when the number o f layers increases. HopÔ¨Åeld neural networks are a special kind of RNN. Long Short-Term Memmory ANN (LSTM) [7]-thisisthenetworkwhereeachneuronhasitsown software memory unit. The processing can be represented as a se quence of steps and the input form3.4. OTHER ARCHITECTURES 13 the previoussteps is used to modify output values by meansofthe u se ofthe memory. LSTM can be used in processing data where connection between diÔ¨Äerent portio ns of data is important. Encoder- Decoder architecture [15]- this rather more abstract architectu re consists of two neural networks one for coding data and the second for encoding. The output is the output from the decoder. This neural network is designed for coding of sequences, e.g., translat ing from one language to the other one. Wehn processing large volumes of data (e.g. text) the decode r can lose the main purpose of processing, and therefore the attention mechanism was invented [2]. Generative Adversarial Network (GAN) [5] - is also an architecture consisting of G (the genera- tive model) and D (the discriminative model). They are learned in tande m where D estimates the probability that the output comes from the training data rather th an from G. This ends our non-inclusive overview of typical architectures used in typical industrial applica- tions. In the next section we present some other architectures t hat are used on smaller scale or in the research on ANN. 3.4 Other architectures We can distinguish: ‚Ä¢HopÔ¨Åeld neural networks [8], [12] - they are modeled on a physical sy stem of spins on a lattice. Due to these similarities statistical physics methods can be w idely applied for this architecture. ‚Ä¢Boltzmann machines [14] - is another spin-based approach to neura l networks. ‚Ä¢‚ÄôAlgebraic Neural Networks‚Äô - under this title we collected the typica l multilayer architec- tures, where computation is done using real numbers instead of re al numbers, e.g., complex numbers, various CliÔ¨Äord algebras, and Hypercomplex algebras. Th is is currently a vast Ô¨Åeld of theoretical and practical research. An introduction to the re search in this direction is presented in [1]. 3.5 Summary Due to many and still growing number of applications, ANN is an active a nd extremely promising research area. Therefore each summary is burdened with incomple teness and risk of fast outdating. In this chapter, the general overview of current architectures of ANN was presented with short characteristics. Acknowledgement The article has been supported by the Polish National Agency for Ac ademic Exchange Strategic Partnership Programme under Grant No. BPI/PST/2021/1/0003 1. Bibliography [1] Arena, P., et al. Neural Networks in Multidimensional Domains: Fundamental s and New Trends in Modelling and Control , Springer, 1998.14 BIBLIOGRAPHY [2] Bahdanau, D., et al. Neural Machine Translation by Jointly Learning to Align and Translate ; arXiv:https://arxiv.org/abs/1409.0473 . Accessed 22 September 2022. [3] Cui, Peng, et al., editors. Graph Neural Networks: Foundations, Frontiers, and Applic ations, Springer Nature Singapore, 2022. [4] Finzi, M., et al. Generalizing Convolutional Neural Networks for Equivaria nce to Lie Groups on Arbitrary Continuous Data , arXiv:https://arxiv.org/abs/2002.12880 . Accessed 22 September 2022. [5] Goodfellow, J.J., et al. Generative Adversarial Nets , Proceedings of the International Confer- enceonNeuralInformationProcessingSystems(NIPS2014), vo l.1, no.1, 2014,pp.2672‚Äì2680. [6] GoogleBrainTeam.TensorFlowlibrary.TensorFlow.org, https://www.tensorflow.org/ .Ac- cessed 22 September 2022. [7] Hochreiter, S., and J. Schmidhuber. Long Short-Term Memory , Neural Computation, vol. 9, no. 8, 1997, pp. 1735‚Äì1780, https://doi.org/10.1162/neco.1997.9.8.1735 . [8] HopÔ¨Åeld, J.J. Neural networks and physical systems with emergent collect ive computational abilities, Proceedings of the National Academy of Sciences, vol. 79, no. 8, 1982, pp. 2554‚Äì2558 https://doi.org/10.1073/pnas.79.8.2554 . [9] Lane, Hobson, et al. Natural Language Processing in Action: Understanding, Ana lyzing, and Generating Text with Python , Manning, 2019. [10] MetaAI. PyTorchlibrary.PyTorch.org, https://pytorch.org/ . Accessed22September 2022. [11] Raschka, Sebastian, and Vahid Mirjalili. Python Machine Learning: Machine Learning and Deep Learning with Python, Scikit-learn, and TensorFlow 2 , Packt Publishing, 2019. [12] H. Ramsauer, B. Sch¬® aÔ¨Ç, J. Lehner, P. Seidl, M. Widrich, T. Adle r, L. Gruber, M. Holzleitner, M. Pavlovi¬¥ c, G.K. Sandve, V. GreiÔ¨Ä, D. Kreil, M. Kopp, G. Klambauer , J. Brandstetter, S. Hochreiter, HopÔ¨Åeld Networks is All You Need ,https://arxiv.org/abs/2008.02217 [13] Rumelhart, D.E., et al. Learning representations by back-propagating errors , Nature, vol. 323, no. 1, 1986, pp. 533‚Äì536, https://doi.org/10.1038/323533a0 . [14] Sherrington, D., and S. Kirkpatrick. Solvable Model of a Spin-Glass , Phys. Rev. Lett., vol. 35, no. 26, 1972, pp. 1792‚Äì1796, https://doi.org/10.1103/PhysRevLett.35.1792 . [15] Sutskever, I., et al. Sequence to Sequence Learning with Neural Networks , arXiv:https://arxiv.org/abs/1409.3215 . Accessed 22 September 2022. [16] Tripathi, Suman Lata, et al., editors. Machine Learning Algorithms for Signal and Image Processing , Wiley, 2022.Chapter 4 Dynamical systems approach to artiÔ¨Åcial neural networks R.A. Kycia, A. Siemaszko 4.1 Introduction Data processing in Neural Networks (biological and artiÔ¨Åcial) can de scribed as a time-dependent phenomenon. The mathematical tool to describe the change of a s ystem in time is oÔ¨Äered by Dynamical Systems: Smooth Dynamical systems ‚Äì for a continuous time parameter usually ranging from a connected subset of R, or Discrete Dynamical Systems ‚Äì discrete time steps, varying ove r a subset of Z. Therefore, it is natural to ask if these complex systems can be de scribed by the tools oÔ¨Äered by Dynamical Systems. Currently, there are many dir ections in which the disciplines of Dynamical Systems and ArtiÔ¨Åcial Neural Networks interpenetr ate each other, and in this report, we indicate some of these directions in this fast-pacing Ô¨Åeld. 4.2 Biological Neural Networks To understand the motivation for applying the Dynamical Systems a pproach to ArtiÔ¨Åcial Neural Networks (ANN), we will brieÔ¨Çy overview the modeling of biological neu ral networks. This is a vast subject of Dynamical Neuroscience (‚Äôneurodynamics‚Äô), see, e.g., [8 ], or Chapter 21 of [11]. Even a single biological neuron is a very complicated electro-biochemic al system. The main focus is on modeling neuron excitations ‚Äì when an electrochemical imp ulse passes some threshold, then the neuron ‚ÄôÔ¨Åres‚Äô, producing a sequence of spikes in voltage tr ansmitted to other neurons by interconnectors called synapses. This modeling must take into acco unt the self-sustaining states of inactivity and this producing spikes. In terms of Dynamical Systems , they can be modeled by limit cycles (attracting or repelling). The standard model for describin g these phenomena is a Hodgkin- Huxley model, a four-dimensionalmodel forcell membranevoltage, sodium and potassium densities in a cell, and so-called leakage gating [6]. The phenomenon of oscillation between inactive and spiking states ins pired some researchers to base the computation on such oscillatory behavior, e.g., [2, 3]. More over, the threshold behavior 1516CHAPTER4. DYNAMICALSYSTEMSAPPROACHTOARTIFICIALNEURAL NETWORKS was adapted in the Ô¨Årst model of a neuron ‚Äì the perceptron [12]. 4.3 Physics-motivated Neural Networks One of the systems that can be signiÔ¨Åcantly investigated using the q ualitative and quantitative methods of Dynamical Systems is the HopÔ¨Åeld Neural Network [7]. Th e system is modeled on the crystal lattice of spins, and powerful techniques of statistical p hysics are accessible for solving its parameters. They allow us to estimate the memory capacity and sta bility of memorized patterns. For example, for the continuous version of the HopÔ¨Åeld Model, the s tability can be analyzed using a suitable Lyapunov function, e.g., Chapter 20 of [11]. This network m odel was developed into a core layer in a multilayer feed-forward ANN [13]. The other physics-inspired model on spin glass is called the Boltzmann machine [14]. In this model also the techniques of statistical physics can be applied. 4.4 Modelling ArtiÔ¨Åcial Neural Networks Feed-forward multilayer ANN dominates current practical applicat ions. The mathematical under- standing of their work as a whole has yet to be provided, however, s ome progress is made [16]. The current trends of using Dynamical Systems theory to describ e ANN focus on various direc- tions, some of which we summarize in the following subsections. 4.4.1 Modelling ANN work The description of ANN using continuous Dynamical Systems is a new id ea [17]. In principle, the multilayer ANN is a discrete Dynamical System, where we have two con secutive steps - performing a linear operation on the output from the previous step1, and applying a nonlinear function. We can now devise the idea to model such a network by a continuous Dyn amical System. The original network is recovered by doing a discretization of the model. This app roach is more Ô¨Çexible since powerful mathematical techniques are at our disposal. The proble m of regression using the ODE approach can be formulated as follows [17]: Consider the diÔ¨Äerential equation dz dt=f(A(t),z), z(0) =x, (4.4.1) wherezandfareRd-valued functions, Ais a control that need to be found. The solution of this problemz(t), under linear transformation u(x) =az(x)+b, for real parameters a‚ààRd,b‚ààR, must Ô¨Åt the data y(x), i.e., to minimize the distance ||y(x)‚àíu(x)||in a suitable norm. The information about the structure of ANN is contained in the function f. The problem of the existence of control at the level of Dynamical Systems is transferred in this approach t o the question if the structure of ANN is suitable for modeling the data. Especially interesting in terms of Dynamical Systems are Residual Ne ural Networks, which can be brought to discrete dynamical system [17], and in some cases, t his system can be modeled as the Euler scheme for integrating ODEs [9]. Moreover, the Residual M odel can be rewritten as a control problem of transport equation and then rewritten as a PD E on manifold [10]. 1The initial step is fed by the data.4.5. DYNAMICAL MODELS PREDICTED BY ANN 17 4.4.2 Modelling of learning process The other aspect of the Dynamical System approach is the way how they model the learning algorithm. The ANN learning algorithm aims to Ô¨Ånd a minimum2for a loss function. This problem is usually solved by a gradient descent (GD) method. This method in hy drodynamical limit and using mean-Ô¨Åeld approximation [16, 5], can be converted into a gradie nt Ô¨Çow of ANN weight on a manifold with the Wasserstein metric. This provides new mathematica l tools for determining the convergence of DG methods. Ingeneral,forthecontinuousapproachtoLearningANN, oneobt ainsnonlinearparabolicPDEs, where all tools from their theory, including optimal choice of functio n space, variational calculus, Ô¨Ånding an approximate solution, analyzing the stability and attracto rs, can be applied, for reference see [18]. Another approach to learning is the so-called Deep Equilibrium Model [1 ]. In this approach, the learning is attained by Ô¨Ånding the equilibrium of a Dynamical System that describes ANN. 4.4.3 Neural ODEs Anotherapproachin modelingANN with ODEsareNeuralODEs, the co nceptpresentedin [4]. The idea behind the model is to make the layers continuous. Then the pro pagation through the network can be described by ODE and not a diÔ¨Äerence equation. This opens an opportunity to apply adaptive ODE solvers for learning. The drawback of this approach is the limited approximation capabilities of these architectures, as described in [19]. 4.5 Dynamical Models predicted by ANN The opposite direction of using ANN to model and control Dynamical Systems is currently a vast Ô¨Åeld of research. We do not pretend to review this Ô¨Åeld and only prov ide a reference of review book [15] instead. 4.6 Conclusions Currently, the ArtiÔ¨Åcial Neural Networks and Dynamical Systems theory merge, beneÔ¨Åting both disciplines. Wepresentedsomecurrenttrendsinthisdirection, how ever,thefullreviewisimpossible due to the high volume of results appearing each term. Acknowledgments The article has been supported by the Polish National Agency for Ac ademic Exchange Strategic Partnership Programme under Grant No. BPI/PST/2021/1/0003 1. 2In an ideal situation, it should be a global minimum.18 BIBLIOGRAPHY Bibliography [1] S. Bai, J.Z. Kolter, V. Koltun, Deep equilibrium models , Advances in Neural Information Processing Systems 32 (2019) [2] J. Borresen, S. Lynch, Neuronal computers , Nonlinear Anal. Theory, Meth. and Appl., 71 2372‚Äì2376 (2009) [3] J. Borresen, S. Lynch, Oscillatory threshold logic , PLoS ONE 7(11): e48498. doi:10.1371/journal.pone.0048498 (2012). [4] R.T.Q. Chen, et al., Neural ordinary diÔ¨Äerential equations , Advances in neural information processing systems 31 (2018) [5] L. Chizat, F. Bach, On the global convergence of gradient descent for over- para meterized models using optimal transport . In Advances in neural information processing systems, pages 3036‚Äì3046, (2018) [6] A.L. Hodgkin and A.F. Huxley, A qualitative description of membrane current and its appli - cation to conduction and excitation in nerve , J. Physiol. 117 500‚Äì544, (1952) [7] J.J. HopÔ¨Åeld, Neural networks and physical systems with emergent collect ive computational abilities. Proceedings of the National Academy of Sciences. 79 (8): 2554‚Äì 2558 (1982) [8] E.M. Izhikevich, Dynamical Systems in Neuroscience: The Geometry of Excitab ility and Burst- ing, MIT Press, 2010 [9] Y. Le, A. Zhong, Q. Li, B. Dong, Beyond Finite Layer Neural Networks: Bridg- ing Deep Architectures and Numerical DiÔ¨Äerential Equation s,Proceedings of the 35th International Conference on Machine Learning, PMLR 80:3276‚Äì32 85 (2018); arXiv: https://arxiv.org/abs/1710.10121 [10] Z. Li, Z. Shi, Deep Residual Learning and PDEs on Manifold , arXiv:1708.05115v3[cs.IT] [11] S. Lynch, Dynamical Systems with Applications using Python , Springer 2018 [12] W.S. McCulloch, W. Pitts, A logical calculus of the ideas immanent in nervous activity , The Bulletin of Mathematical Biophysics, 5(4):115‚Äì133, (1943) [13] H. Ramsauer, B. Sch¬® aÔ¨Ç, J. Lehner, P. Seidl, M. Widrich, T. Adle r, L. Gruber, M. Holzleitner, M. Pavlovi¬¥ c, G.K. Sandve, V. GreiÔ¨Ä, D. Kreil, M. Kopp, G. Klambauer , J. Brandstetter, S. Hochreiter, HopÔ¨Åeld Networks is All You Need ,https://arxiv.org/abs/2008.02217 [14] D. Sherrington, S. Kirkpatrick, Solvable Model of a Spin-Glass , Physical Review Letters, 35 (35): 1792‚Äì1796, (1975) [15] Y. Tiumentsev, M. Egorchev, Neural Network Modeling and IdentiÔ¨Åcation of Dynamical Sys - tems, Academic Press, 2019 [16] E. Weinan, M. Chao, W. Lei, S. Wojtowytsch, Towards a Mathematical Understanding of Neural Network-Based Machine Learning: What We Know and Wha t We Don‚Äôt , CSIAMTrans. Appl. Math., 1 , 561-615, (2020)BIBLIOGRAPHY 19 [17] E. Weinan, A Proposal on Machine Learning via Dynamical Systems , Commun. Math. Stat. 5:1‚Äì11 (2017) [18] E. Wienan, C. Ma, L. Wu, Machine learning from a continuous viewpoint, I , Sci. China Math. 63, 2233‚Äì2266 (2020); DOI: https://doi.org/10.1007/s11425-020-1773-8 [19] H. Zhang, X. Gao, J. Unterman, T. Arodz, Approximation Capabilities of Neural ODEs and Invertible Residual Networks , Proceedings of the 37th International Conference on Machine Learning, PMLR 119:11086-11095, 2020.20 BIBLIOGRAPHYChapter 5 Neural networks as universal approximators J.M. Calabuig, Ll.M. Garc¬¥ ƒ±a-RaÔ¨É Since the Ô¨Årst golden age (the 1950s and 1960s) when in 1962, Fran k Rosenblatt introduced and developed the perceptron, ArtiÔ¨Åcial Neural Networks (ANNs) ha ve gone through various stages ranging from enthusiasm to ostracism. When we talk about ANNs, we are talking about math- ematical tools that play an important role in approximation and classiÔ¨Å cation problems. From a mathematical point of view, a natural question that arises is wheth er ArtiÔ¨Åcial Neural Networks are universal approximators in the sense of mathematics. This que stion, which may seem trivial or second-orderin view ofthe applicationsofANNs in applied problems, is neverthelessa centralissue. In essence, the certainty of the results achieved in practical pro blem solved with ArtiÔ¨Åcial Neural Networks rests on the certainty that they are universal approx imators. To Ô¨Ånd the Ô¨Årst answer to this question we have to go back to the work of Cybenko and Hornik [ 1, 2] where basically it is provedthatafeed-forwardNeuralNetworkswithatleastonehid denlayercanapproximateanycon- tinuous function assuming that certain activation functions are us ed (sigmoid activation function). Since then, and as new network topologies emerged with new activat ion functions, an important theoretical eÔ¨Äort have been done in order to prove the characte r of universal approximatorsof ANN [3, 4, 5, 6, 7, 8]. Within the question of whether an ANN can approximate a (continuou s) function there are two issues to be addressed. On the one hand, there is the Hornik/Cybe nko issue which corresponds to the question about if some ANN can approximate a given (continuo us) function to arbitrary precision. However, neither the result nor the proof of it give any in dication of how ‚Äúlarge‚ÄùANNs need to be to achieve a certain approximation accuracy. Then, ano ther issue to be addressed is how many layers and how many neurons per layer an ANN requires, that is , the approximation rates. A distinction must be made between shallow learning and deep learning. In [9] authors study and proof approximation results for ANN with g eneral activation func- tions: a two layer Neural Network with a polynomially-decaying non-s igmoidal activation function. They extend the results for a larger class of activation functions, removing the polynomial decay assumption. This result applies to any bounded, integrable activatio n function. In [10] authors address the study of the approximation of continu ous functions with very deep 2122 BIBLIOGRAPHY networks using the activation function RELU. In this case, not nar row networks (a high number of neurons per layer) are considered and authors prove that const ant-width fully-connected networks of depth of the order of the number of weights provide the fastes t possible approximation rate. In [11] the narrow case is addressed, that is, networks of bounde d width and arbitrary depth. Specially interesting is the work [12] that address the super-narro w case, that is, with only two neurons per layer, showing that given enough layers, a super-nar row Neural Network, with two neurons per layer, is capable to separate any separable binary dat aset and if the datasets exhibit certain type of symmetries, they are better suited for deep repr esentation and may require only few hidden layers to produce desired classiÔ¨Åcation. Less literature is found on the consideration of non-standard act ivation functions. However, this is a Ô¨Åeld to be explored in order to obtain networks that are narrow, with a medium level of depth and a suitable approximation rate. Note that, for example, in tradit ional convolutional networks applied to the reconstruction of medical images (e.g. Nuclear Magne tic Resonance Imaging MRI), the number of weights (neurons+layers) is usually in the order of millio ns. In short, these are free parameters in our model and therefore any reduction in their numb er generates more robust and simpler mathematical models. One of the natural extensions to changing the activation function is to consider that the image of the function is not in Rbut inC[20, 13]. This is the case of complex and hypercomplex- valued Nerural Networks. Beyond being a simple generalization of re al-value activation functions, Complex-Valued Neural Networks (CVNNs) are specially suitable to d eal with modelling problems of complex amplitude ‚Äìamplitude and phase‚Äì the kind of problems that are in the core of wave physics(electromagnetism,light, sound/ultrasounds,andmatte rwaves). CVNNs giveanimportant advantage in practical applications in Ô¨Åelds where signals are massive ly analyzed and processed in time/space, frequency, and phase domains. Hyper-complex ANN a s quaternion and CliÔ¨Äord Neural Networks are further extension of CVNNs ([16, 15, 17, 14, 18, 1 9]). They seems to be specially suit- able in color-information treatment, image reconstruction and seg mentation, robotics and systems control. The question about the character as universal approxim ates and the approximation rates of CVNNs is currently the subject of investigation [21], cf. [22]. Acknowledgement The article has been supported by the Polish National Agency for Ac ademic Exchange Strategic Partnership Programme under Grant No. BPI/PST/2021/1/0003 1. Bibliography [1] Cybenko, G. Approximation by superpositions of a sigmoidal func tion.Mathematics Of Con- trol, Signals And Systems .2, 303-314 (1989,12,1), doi.org/10.1007/BF02551274 [2] Hornik, K., Stinchcombe, M. & White, H. Multilayer feedforward ne t- works are universal approximators. Neural Networks .2, 359-366 (1989), www.sciencedirect.com/science/article/pii/089360808 9900208BIBLIOGRAPHY 23 [3] Leshno, M., Lin, V., Pinkus, A. & Schocken, S. Multilayer feedforw ard networks with a non- polynomial activation function can approximate any function. Neural Networks .6, 861-867 (1993),www.sciencedirect.com/science/article/pii/S08936080 05801315 [4] Pinkus, A. Approximation theory of the MLP model in neural netw orks.Acta Numerica .8pp. 143‚Äì195 (1999) [5] Zhou, D. Universality of deep convolutional neural networks. Ap- plied And Computational Harmonic Analysis .48, 787-794 (2020), www.sciencedirect.com/science/article/pii/S10635203 18302045 [6] Sch¬® afer, A. & Zimmermann, H. Recurrent Neural Networks Ar e Universal Approximators. ArtiÔ¨Åcial Neural Networks ‚Äì ICANN 2006 . pp. 632-640 (2006) [7] Heinecke, A., Ho, J. & Hwang, W. ReÔ¨Ånement and Universal Appro ximation via Sparsely Connected ReLU Convolution Nets. IEEE Signal Processing Letters .27pp. 1175-1179 (2020) [8] Br¬® uel Gabrielsson, R. Universal Function Approximation on Gra phs.Advances In Neural In- formation Processing Systems .33pp. 19762-19772 (2020) [9] Siegel, J. & Xu, J. Approximation rates for neural networks with gen- eral activation functions. Neural Networks .128pp. 313-321 (2020), www.sciencedirect.com/science/article/pii/S08936080 20301891 [10] Yarotsky, D. Optimal approximation of continuous functions b y very deep ReLU net- works.Proceedings Of The 31st Conference On Learning Theory .75pp. 639-649 (2018,7,6), proceedings.mlr.press/v75/yarotsky18a.html [11] Kidger, P. & Lyons, T. Universal Approximation with Deep Narro w Networks. Proceed- ings Of Thirty Third Conference On Learning Theory .125pp. 2306-2327 (2020,7,9), proceedings.mlr.press/v125/kidger20a.html [12] Szymanski, L. & McCane, B. Deep, super-narrow neural netw ork is a universal classiÔ¨Åer. The 2012 International Joint Conference On Neural Networks (IJ CNN). pp. 1-8 (2012) [13] Kobayashi, M. Complex-valued HopÔ¨Åeld neural networks with re al weights in synchronous mode. Neurocomputing .423pp. 535-540 (2021), www.sciencedirect.com/science/article/pii/S09252312 2031660X [14] Kobayashi, M. Bicomplex-valued twin-hyperbolic HopÔ¨Åeld neu- ral networks. Neurocomputing .434 pp. 203-210 (2021), www.sciencedirect.com/science/article/pii/S09252312 2032021X [15] Kobayashi, M. Fixed points of split quaternionic hopÔ¨Åeld neural n etworks. Signal Processing . 136pp. 38-42 (2017), www.sciencedirect.com/science/article/pii/S01651684 16303346 , Hypercomplex Signal Processing [16] Kobayashi, M. Symmetric quaternionic HopÔ¨Åeld neural network s.Neurocomputing .240pp. 110-114 (2017), www.sciencedirect.com/science/article/pii/S09252312 17303351 [17] Parcollet, T., Morchid, M. & Linar` es, G. A survey of quaternion neural networks. ArtiÔ¨Åcial Intelligence Review .53, 2957-2982 (2020,4,1), doi.org/10.1007/s10462-019-09752-124 BIBLIOGRAPHY [18] Vieira, G. & Valle, M. A general framework for hypercomplex-va lued extreme learning ma- chines.Journal Of Computational Mathematics And Data Science .3pp. 100032 (2022), www.sciencedirect.com/science/article/pii/S27724158 22000062 [19] Da Cunha, ¬¥E. & Da Fontoura Costa, L. On hypercomplex networks. Phys- ica A: Statistical Mechanics And Its Applications .591pp. 126714 (2022), www.sciencedirect.com/science/article/pii/S03784371 21009298 [20] Nitta, T. An Extension of the Back-Propagation Algorithm to Complex Numbers. Neural Networks .10, 1391-1415 (1997), www.sciencedirect.com/science/article/pii/S08936080 97000361 [21] Voigtlaender, F. The universal approximation theorem for com plex-valued neural networks. ArXiv.abs/2012.03351 (2020) [22] Vital, W., Vieira, G. & Valle, M. Extending the Universal Approximat ionTheorem fora Broad Class of Hypercomplex-Valued Neural Networks. (arXiv, 2022), arxiv.org/abs/2209.02456Chapter 6 Complex and quaternionic neural networks B. Schneider, D. Berseghyan 6.1 Introduction Neural networks in the real domain have been studied for a long time and achieved promising results in many vision tasks for recent years. However, the exten sions of the neural network models in other number Ô¨Åelds and their potential applications are not fully-in vestigated yet. Complex numbers play an important role in practical applications and f undamental theorems in various Ô¨Åelds of engineering such as electromagnetics, communicat ion, control theory, and quantum mechanics. The application ofcomplex numbers to neural networks has recently attracted attention because they tend to improve the learning ability and conform to the above mentioned applications. They enable the modeling of a point in two-dimensional space as a single entity, rather than as a set of two data items on which 2D geometrical aÔ¨Éne operations a re performed. It has been shown that a neural network with the representation and operat ions of complex numbers results in improved performance of the geometrical aÔ¨Éne transformation in two-dimensional space, whereas the performance of real-valued (conventional) neural networks is comparatively poor. The opera- tions involving complex numbers would improve the performance of ne ural networks for processing two-dimensional data, e.g. book [1]. In the 1870s, William Kingdon CliÔ¨Äord introduced his geometric algebra, building on earlier works of Sir William Rowan Hamilton and Hermann Gunther Grassmann. C liÔ¨Äord intended to describe the geometric properties of vectors, planes, and higher -dimensional objects. Most physi- cists encounter the algebra in the guise of Pauli and Dirac matrix alge bras of quantum theory. Many roboticists or computer graphic engineers use quaternions f or 3D rotation estimation and interpolation, as it is too diÔ¨Écult for them to formulate homogeneous transformations of high-order geometric entities using a point-wise approach. They resort often to tensor calculus for multi- variable calculus. Since robotics and engineering make use of the dev elopments of mathematical physics, many beliefs are automatically inherited; for instance, som e physicists come away from a study of Dirac theory with the view that CliÔ¨Äord‚Äôs algebra is inherently quantum-mechanical. 2526 CHAPTER 6. COMPLEX AND QUATERNIONIC NEURAL NETWORKS Extension of neural networks on hypercomplex number system is o ne of such research eÔ¨Äorts. Input, output, and internal state of a neuron which is the basic co mputational unit are represented by hypercomplex number in these types of neural networks. Quaternion neural networks are models in which computations of th e neurons are based on quaternions, the four-dimensional equivalents of imaginary numbe rs. The quaternion neural net- work also performs superior in terms of convergence speed to a re al-valued neural network with respect to the 3-bit parity check problem, as simulations show. Con sequently, the application of hy- percomplex numbers, particularly quaternions, to neural networ ks has been investigated. Quater- nions are a class of hypercomplex number systems, a four-dimensio nal extension of imaginary numbers. One of beneÔ¨Åts by quaternions is that aÔ¨Éne transforma tion of geometric Ô¨Ågures in three-dimensional space (3D geometrical aÔ¨Éne transformation) , especially spatial rotation, can be represented compactly and eÔ¨Éciently; in recent years, quaternio ns are extensively used in the Ô¨Åelds of robotics, control of satellites, and computer graphics, etc, s ee for example [2]. In that sense, it is thought that it is very useful to employ complex n umbers and quaternions that can calculate two or three dimensional information as a unit as e xpressions of neurons. In fact, it is suggested that complex-valued and quaternionic feed fo rward neural networks have a remarkable learning ability in terms of aÔ¨Éne transformation problems in two or three dimensional space. The role of Neural Networks in today‚Äôs scientiÔ¨Åc community c annot be denied, its vast applications, from engineering to medicine, these are based on cont inuously improving algorithms. Thismotivated ourresearchgroupto begin the approachtowards the creationofamathematical basis for the Ô¨Åeld of Hypercomplex Neural Networks which could brin g better, faster algorithms, and be useful in a wide range of computations. In the underlying mathematical theories, the choice of a system of constants plays an important role, and advancing from a theory built on real numbers to hyperco mplex ones is bound to give improved algorithms, due to the rich analysis of the Ô¨Åeld. 6.2 Elements of quaternionic analysis In this section we present brieÔ¨Çy the basic deÔ¨Ånitions and results of quaternionic analysis which are necessary for our purpose. For more information, we refer the r eader to [8, 9]. LetHbe the set of real quaternions, i.e., that each quaternion ais represented in the form a=a0+a1i+a2j+a3k, with{ak} ‚äÇR,k= 0,1,2,3 andi,j,kare the quaternionic imaginary units. The basic elements deÔ¨Åne arithmetic rules in H, which are given by the following relations: i2=j2=k2=‚àí1;ij=k=‚àíji;jk=i=‚àíkj;ki=j=‚àíik. The quaternionic conjugation of a=a0+a1i+a2j+a3kis given by ¬Ø a:=a0‚àía1i‚àía2j‚àía3k. It is easy seen that a¬Øa= ¬Øaa=a2 0+a2 1+a2 2+a2 3. Note that for a,b‚ààH,ab=¬Øb¬Øa. We identify the space C2with the set Hof quaternions: let ( z1,z2) = (x0+ix1,x2+ix3) be two complex numbers with the imaginary unit i, and letjbe another imaginary unit such that j2=‚àí1 andij+ji= 0 hold. In particular, for a‚ààCand by abuse of notation if ¬Ø adenoted the complex conjugate of a, we haveaj=j¬Øa. The set of elements of the form q=z1+z2j,z1,z2‚ààC, endowed both with a component-wise addition and with the associative multiplica tion is then another way of stating H. The quaternion conjugation gives: z1+z2j:= ¬Øz1‚àíz2jandq¬Øq= ¬Øqq=|z1|2+|z2|2. LetEbe a bounded subset of R4‚àº=C2‚àº=C√óCand denote by BC(E,H) the class of H-valued bounded continuous functions on E. Forf‚ààBC(E,H) we deÔ¨Åne the modulus of continuity of f6.2. ELEMENTS OF QUATERNIONIC ANALYSIS 27 as a non-negative function wf(Œ¥),Œ¥>0, given by wf(Œ¥) := sup |x‚àíy|‚â§Œ¥{|f(x)‚àíf(y)|:x,y‚ààE}. For 0<ŒΩ‚â§1 if sup 0<Œ¥‚â§diam E/braceleftbiggwf(Œ¥) Œ¥ŒΩ/bracerightbigg <‚àû,for 0<ŒΩ‚â§1, thenfbecomesa H¬® oldercontinuouswith exponent ŒΩfunction in E(Lipschitz continuousfor ŒΩ= 1). We will denote by C0,ŒΩ(E,H) :={f‚ààBC(E,H) : sup 0<Œ¥‚â§diam E/braceleftbiggwf(Œ¥) Œ¥ŒΩ/bracerightbigg <‚àû}, the collection of H¬® older continuous functions on E, for 0<ŒΩ‚â§1. We say ([6]) that a closed set EinR4is an Ahlfors-David regular set (in short AD-regular) if there exists a constant c>0 such that for all x‚ààEand 0<r< diam E there holds c‚àí1r3‚â§ H3(E‚à©B(x,r))‚â§cr3, whereB(x,r) is closed ball with center xand radius randH3is the 3-dimensional HausdorÔ¨Ä measure. The AD-regularity condition implies a uniform positive and Ô¨Ån ite bound on Efor the upper and lower density. Moreover, we notice that such condition p roduces a very wide class of surfaces that contains the classes of surfaces classically cons idered in the literature: Liapunov surfaces, smooth surfaces and Lipschitz ones. Finally we would like t o remark that AD-regular sets are not always rectiÔ¨Åable in the sense of Federer [7]. In what follows, ‚Ñ¶ ‚äÇR4stands for a bounded domain with an AD-regular rectiÔ¨Åable boundar y Œì and let ‚Ñ¶+:= ‚Ñ¶; ‚Ñ¶‚àí:=R4\‚Ñ¶+, where both open sets are assumed to be connected. Forcontinuouslyreal-diÔ¨Äerentiablefunction H-valuedfunctions f:=f0+f1i+f2j+f3k: ‚Ñ¶‚ÜíH, the operator œàD:=‚àÇ ‚àÇx0+i‚àÇ ‚àÇx1‚àíj‚àÇ ‚àÇx2+k‚àÇ ‚àÇx3, associated to the structural set H-vectorœà:={1,i,‚àíj,k}is called the Cauchy‚ÄìRiemann operator, which can be written in complex form as: œàD= 2/braceleftbigg‚àÇ ‚àÇ¬Øz1‚àíj‚àÇ ‚àÇ¬Øz2/bracerightbigg A factorization of the Laplacian is given by œàD¬ØœàD=¬ØœàDœàD= ‚àÜH, where ¬ØœàD:= 2/braceleftbigg‚àÇ ‚àÇ¬Øz1+j‚àÇ ‚àÇ¬Øz2/bracerightbigg , and ‚àÜ H[f] := ‚àÜ R4[f0] + ‚àÜR4[f1] + ‚àÜR4[f2] + ‚àÜR4[f3]. A function f: ‚Ñ¶‚ÜíHis called left œà‚àíhyperholomorphic in ‚Ñ¶ if œàD[f](Œæ) = 0 for ‚àÄŒæ‚àà‚Ñ¶.28 CHAPTER 6. COMPLEX AND QUATERNIONIC NEURAL NETWORKS We will denote by œàM(‚Ñ¶,H) :={f‚ààC1(‚Ñ¶,H) :œàD[f](Œæ) = 0,‚àÄŒæ‚àà‚Ñ¶}. Under assumption f‚ààœàM(‚Ñ¶,H) and following similar arguments to those in [4, page 3875] we have the Cauchy integral formula /integraldisplay ŒìKœà(œÑ‚àít)¬∑nœà(œÑ)¬∑f(œÑ)dH3 œÑ=f(t),t‚àà‚Ñ¶+. (6.2.1) For a survey of the theory of œà-hyperholomorphic functions along classical lines we refer the read er to [12]. An easy computation shows that if f=u+vjwithu=f0+if1andv=f2+if3, then œàDf= 0‚áê‚áí/braceleftBigg ‚àÇ¬Øz1u+‚àÇz2¬Øv= 0 ‚àÇ¬Øz2u‚àí‚àÇz1¬Øv= 0, which express the direct relation between the œà-hyperholomorphic functions and solutions of the Cimmino system. The most important examples of a œà-hyperholomorphic function is the function Kœà(q) =1 2œÄ2¬Øz1+ ¬Øz2j (|z1|2+|z2|2)2, z1,z2/\e}atio\slash= 0, which is obtained by applying¬ØœàDto the fundamental solution of the Lapacian ‚àÜ R4. It is known as the Cauchy kernel and it represents a fundamental solution to both operatorsœàDand¬ØœàD. 6.3 Poincar¬¥ e-Bertrand formula for œà-hyperholomorphic sin- gular integrals The Cauchy kernel Kœàgenerates important integrals for us: ‚Ä¢ œàCŒì[f](q) :=/integraldisplay ŒìKœà(Œæ‚àíq)nœà(Œæ)¬∑f(Œæ)dH3 Œæ, q‚ààR4\Œì, ‚Ä¢ œàSŒì[f](q) = 2/integraldisplay ŒìKœà(Œæ‚àíq)nœà(Œæ)(f(Œæ)‚àíf(q))dH3 Œæ+f(q), q‚ààŒì, wherenœà(Œæ) :=n0+n1i‚àín2j+n3kwith (n0,n1,n2,n3)‚ààR4being the outward unit normal vector on Œì. By using ideas from [3], we have Remark 6.3.1 In general, the integral /integraldisplay ŒìKœà(Œæ‚àíq)nœà(Œæ)dH3 Œæ6.3. POINCAR ¬¥E-BERTRANDFORMULAFOR œà-HYPERHOLOMORPHICSINGULARINTEGRALS 29 has no sense for every q‚ààŒì, hence the formula /integraldisplay ŒìKœà(Œæ‚àíq)nœà(Œæ)(f(Œæ)‚àíf(q))dH3 Œæ=/integraldisplay ŒìKœà(Œæ‚àíq)nœà(Œæ)f(Œæ)dH3 Œæ‚àí/parenleftbigg/integraldisplay ŒìKœà(Œæ‚àíq)nœà(Œæ)dH3 Œæ/parenrightbigg f(q) is generaly not valid. In case when the singular integral 2/integraldisplay ŒìKœà(Œæ‚àíq)nœà(Œæ)dH3 Œæ has a Ô¨Ånite value Œ±(q)for‚àÄq‚ààŒì, then œàSŒì[f](q) = 2/integraldisplay ŒìKœà(Œæ‚àíq)nœà(Œæ)f(Œæ)dH3 Œæ+(1‚àíŒ±(q))f(q). While the Ô¨Årst is a œà-hyperholomorphic version of the usual Cauchy type integral the second represents its singular version, whose integral has to be taken in t he sense of Cauchy‚Äôs principal value. Inordertofacilitatetheirusage,wepresentbelowsomebasicprop ertiesofthe œà-hyperholomorphic singular integrals, thus making our exposition self-contained. Theorem 6.3.2 [5] Let‚Ñ¶be a bounded domain in R4with AD-regular boundary Œì. Letf‚àà C0,ŒΩ(Œì,H). Then the following limits exist: lim ‚Ñ¶¬±‚àãq‚ÜíŒæ‚ààŒì(œàCŒì[f](q)) =:œàC¬± Œì[f](Œæ), moreover the following identities hold: œàC¬± Œì[f](Œæ) =1 2[œàSŒì[f](Œæ)¬±f(Œæ)], (6.3.1) for allŒæ‚ààŒì. Theorem 6.3.3 [5] IfŒìis a AD-regular surface, then for f‚ààC0,ŒΩ(Œì,H),0<ŒΩ <1we have the following formula: œàS2 Œì[f](Œæ) =f(Œæ), Œæ‚ààŒì. Lemma 6.3.4 If{t,œÑ} ‚äÇŒì,t/\e}atio\slash=Œæ, then /integraldisplay ŒìœÑKœà(œÑ‚àít)nœà(œÑ)Kœà(œÑ‚àíŒæ)dH3 œÑ= 0. Proof.The proof of Lemma 6.3.4 is similar of the proof of Lemma 3 in [11], theref ore we refer to [11] for identical parts. Lemma 6.3.5 Letf(Œæ,œÑ) :=f0(Œæ,œÑ) |Œæ‚àíœÑ|¬µ,0‚â§¬µ<3, andf0‚ààC0,ŒΩ(Œì√óŒì,H). Then the next formula of changing of integration order is true for all t‚ààŒì: /integraldisplay ŒìœÑKœà(œÑ‚àít)nœà(œÑ)[f(Œæ,œÑ)‚àíf(œÑ,œÑ)]dH3 œÑ/integraldisplay ŒìŒænœà(Œæ)dH3 Œæ= =/integraldisplay ŒìŒæ/integraldisplay ŒìœÑKœà(œÑ‚àít)nœà(œÑ)[f(Œæ,œÑ)‚àíf(œÑ,œÑ)]dH3 œÑnœà(Œæ)dH3 Œæ.30 CHAPTER 6. COMPLEX AND QUATERNIONIC NEURAL NETWORKS Proof.The proof of Lemma6.3.5 is along the same line of the proof of Theorem 22.5 in [10]. /square The Poincar¬¥ e-Bertrand formula in the œà-hyperholomorphic framework is established by our next theorem. Theorem 6.3.6 Let‚Ñ¶be a bounded domain in R4with AD-regular boundary Œìand letf‚ààC0,ŒΩ(Œì√ó Œì,H). Then for all w‚ààŒì /integraldisplay ŒìzKœà(z‚àít)nœà(z)dH3 z/integraldisplay ŒìŒæKœà(Œæ‚àíz)nœà(Œæ)[f(Œæ,z)‚àíf(z,t)]dH3 Œæ= =/integraldisplay ŒìŒæ/integraldisplay ŒìzKœà(z‚àít)nœà(z)dH3 zKœà(Œæ‚àíz)nœà(Œæ)[f(Œæ,z)‚àíf(z,t)]dH3 Œæ+Œ±2(t)f(t,t), and the integrals being understood in the sense of the Cauchy principal value. If ‚Ñ¶ be a bounded domain in R4with a smooth boundary Œì then Œ±=1 2and the formula reduces to the Poincar¬¥ e-Bertrand formula (see, e.g., [11]). Proof.Let /integraldisplay ŒìœÑKœà(œÑ‚àít)nœà(œÑ)dH3 œÑ/integraldisplay ŒìŒæKœà(Œæ‚àíœÑ)nœà(Œæ)[f(Œæ,œÑ)‚àíf(œÑ,t)]dH3 Œæ= =/integraldisplay ŒìœÑKœà(œÑ‚àít)nœà(œÑ)dH3 œÑ/integraldisplay ŒìŒæKœà(Œæ‚àíœÑ)nœà(Œæ)([f(Œæ,œÑ)‚àíf(œÑ,t)]‚àíf(œÑ,œÑ))dH3 Œæ+ +/integraldisplay ŒìœÑKœà(œÑ‚àít)nœà(œÑ)dH3 œÑ/integraldisplay ŒìŒæKœà(Œæ‚àíœÑ)nœà(Œæ)[f(œÑ,œÑ)‚àíf(t,t)]dH3 Œæ+ +/integraldisplay ŒìœÑKœà(œÑ‚àít)nœà(œÑ)dH3 œÑ/integraldisplay ŒìŒæKœà(Œæ‚àíœÑ)nœà(Œæ)f(t,t)dH3 Œæ. In the Ô¨Årst two quaternionic integrals on the right-hand side we can change the order of integration by Lemma 6.3.5 we have /integraldisplay ŒìœÑKœà(œÑ‚àít)nœà(œÑ)dH3 œÑ/integraldisplay ŒìŒæKœà(Œæ‚àíœÑ)nœà(Œæ)[f(Œæ,œÑ)‚àíf(œÑ,t)]dH3 Œæ= =/integraldisplay ŒìŒæ/integraldisplay ŒìœÑKœà(œÑ‚àít)nœà(œÑ)dH3 œÑKœà(Œæ‚àíœÑ)nœà(Œæ)([f(Œæ,œÑ)‚àíf(œÑ,t)]‚àíf(œÑ,œÑ))dH3 Œæ+ +/integraldisplay ŒìŒæ/integraldisplay ŒìœÑKœà(œÑ‚àít)nœà(œÑ)dH3 œÑKœà(Œæ‚àíœÑ)nœà(Œæ)[f(œÑ,œÑ)‚àíf(t,t)]dH3 Œæ+ +/integraldisplay ŒìœÑKœà(œÑ‚àít)nœà(œÑ)dH3 œÑ/integraldisplay ŒìŒæKœà(Œæ‚àíœÑ)nœà(Œæ)f(t,t)dH3 Œæ= =/integraldisplay ŒìŒæ/integraldisplay ŒìœÑKœà(œÑ‚àít)nœà(œÑ)dH3 œÑKœà(Œæ‚àíœÑ)nœà(Œæ)[f(Œæ,œÑ)‚àíf(œÑ,t)]dH3 Œæ‚àí ‚àí/integraldisplay ŒìŒæ/bracketleftbigg/integraldisplay ŒìœÑKœà(œÑ‚àít)nœà(œÑ)dH3 œÑKœà(Œæ‚àíœÑ)/bracketrightbigg nœà(Œæ)f(t,t)dH3 Œæ+6.4. POINCAR ¬¥E-BERTRANDFORMULAFORTHECAUCHY-CIMMINOSINGULARINTEGR ALS31 +/integraldisplay ŒìœÑKœà(œÑ‚àít)nœà(œÑ)dH3 œÑ/integraldisplay ŒìŒæKœà(Œæ‚àíœÑ)nœà(Œæ)f(t,t)dH3 Œæ= (by using Lemma 6.3.4 and the Remark 6.3.1) =/integraldisplay ŒìŒæ/integraldisplay ŒìœÑKœà(œÑ‚àít)nœà(œÑ)dH3 œÑKœà(Œæ‚àíœÑ)nœà(Œæ)[f(Œæ,œÑ)‚àíf(œÑ,t)]dH3 Œæ+Œ±2(t)f(t,t). /square Suppose that f(Œæ,œÑ) =f(Œæ)‚ààC0,ŒΩ(Œì,H) isœà-hyperholomorphic extension into ‚Ñ¶, then the composition formula for œà-hyperholomorphic functions can be written as: Theorem 6.3.7 (Composition formula) Let ‚Ñ¶be a bounded domain in R4with AD-regular bound- aryŒìand letf‚ààC0,ŒΩ(Œì,H). Iff(Œæ)can be extended œà-hyperholomorphically into ‚Ñ¶. Then for all t‚ààŒì, /integraldisplay ŒìœÑKœà(œÑ‚àít)nœà(œÑ)dH3 œÑ/integraldisplay ŒìŒæKœà(Œæ‚àíœÑ)nœà(Œæ)[f(Œæ)‚àíf(œÑ)]dH3 Œæ=Œ±2(t)f(t).(6.3.2) Note that if œàÀúSf:= 2/integraldisplay ŒìŒæKœà(Œæ‚àíœÑ)nœà(Œæ)[f(Œæ)‚àíf(œÑ)]dH3 Œæ, than formula (6.3.2) means that œàÀúS2f= 4Œ±2(t)f(t). Proof.Sincef(Œæ) can be holomorphic extented into ‚Ñ¶, then by Theorem 6.3.2 and Rema rk 6.3.1 œàC+f(z) =f(z), z‚àà‚Ñ¶. By formula (6.3.1) and Remark 6.3.1, we have 2f(z) =œàÀúSf(Œæ)+2(1‚àíŒ±(z))f(z), moreover œàÀúS2f=œàÀúSœàÀúSf=œàÀúS[2Œ±f] = 4Œ±2f. /square 6.4 Poincar¬¥ e-Bertrand formula for theCauchy-Cimmino sin - gular integrals Using the representation of the quaternionic Cauchy kernel Kœàand the normal vector nœàin the complex form, we have: Kœà(Œæ‚àíz)nœà(Œæ) =K1(Œæ,z)+K2(Œæ,z)j, (6.4.1) with K1(Œæ,z) :=1 2œÄ2(¬ØŒæ1‚àí¬Øz1)(n0+in1)+(¬ØŒæ2‚àí¬Øz2)(n2+in3) (|Œæ1‚àíz1|2+|Œæ2‚àíz2|2)2; (6.4.2)32 CHAPTER 6. COMPLEX AND QUATERNIONIC NEURAL NETWORKS and K2(Œæ,z) :=1 2œÄ2(¬ØŒæ2‚àí¬Øz2)(n0+in1)‚àí(¬ØŒæ1‚àí¬Øz1)(n2+in3) (|Œæ1‚àíz1|2+|Œæ2‚àíz2|2)2, (6.4.3) whereŒæ=Œæ1+Œæ2j, z=z1+z2j. Thus, œàCŒì[u+vj](z1,z2) =C1[u,v](z1,z2)+C2[u,v](z1,z2)j,(z1,z2)/‚ààŒì, œàSŒì[u+vj](z1,z2) =S1[u,v](z1,z2)+S2[u,v]j,(z1,z2)‚ààŒì, where C1[u,v](z1,z2) =/integraldisplay Œì[(¬ØŒ∂1‚àí¬Øz1)(n0+in1)+(¬ØŒ∂2‚àí¬Øz2)(n2+in3)]u(Œ∂1,Œ∂2) 2œÄ2(|Œ∂1‚àíz1|2+|Œ∂2‚àíz2|2)2dH3 Œæ1,Œæ2‚àí ‚àí/integraldisplay Œì[(¬ØŒ∂2‚àí¬Øz2)(n0+in1)‚àí(¬ØŒ∂1‚àí¬Øz1)(n2+in3)]¬Øv(Œ∂1,Œ∂2) 2œÄ2(|Œ∂1‚àíz1|2+|Œ∂2‚àíz2|2)2dH3 Œæ1,Œæ2, C2[u,v](z1,z2) =/integraldisplay Œì[(¬ØŒ∂1‚àí¬Øz1)(n0+in1)+(¬ØŒ∂2‚àí¬Øz2)(n2+in3)]v(Œ∂1,Œ∂2) 2œÄ2(|Œ∂1‚àíz1|2+|Œ∂2‚àíz2|2)2dH3 Œæ1,Œæ2+ +/integraldisplay Œì[(¬ØŒ∂2‚àí¬Øz2)(n0+in1)‚àí(¬ØŒ∂1‚àí¬Øz1)(n2+in3)]¬Øu(Œ∂1,Œ∂2) 2œÄ2(|Œ∂1‚àíz1|2+|Œ∂2‚àíz2|2)2dH3 Œæ1,Œæ2. The pair ( C1,C2) of integrals for ( z1,z2)‚ààC2play the role of an analog of a Cauchy type integral in theory of the Cimmino system of partial diÔ¨Äerential equations. Similarly the singular Cauchy-Cimmino integral operators are deÔ¨Åned formally as pair ( S1,S2), of the following singular integrals taken in the sense of Cauchy‚Äôs princ ipal value S1[u,v](z1,z2) = 2/integraldisplay Œì[(¬ØŒ∂1‚àí¬Øz1)(n0+in1)+(¬ØŒ∂2‚àí¬Øz2)(n2+in3)][u(Œ∂1,Œ∂2)‚àíu(z1,z2)] 2œÄ2(|Œ∂1‚àíz1|2+|Œ∂2‚àíz2|2)2dH3‚àí ‚àí2/integraldisplay Œì[(¬ØŒ∂2‚àí¬Øz2)(n0+in1)‚àí(¬ØŒ∂1‚àí¬Øz1)(n2+in3)][¬Øv(Œ∂1,Œ∂2)‚àí¬Øv(z1,z2)] 2œÄ2(|Œ∂1‚àíz1|2+|Œ∂2‚àíz2|2)2dH3+ +u(z1,z2) = = 2/integraldisplay Œì[(¬ØŒ∂1‚àí¬Øz1)(n0+in1)+(¬ØŒ∂2‚àí¬Øz2)(n2+in3)]u(Œ∂1,Œ∂2) 2œÄ2(|Œ∂1‚àíz1|2+|Œ∂2‚àíz2|2)2dH3‚àí ‚àí2/integraldisplay Œì[(¬ØŒ∂2‚àí¬Øz2)(n0+in1)‚àí(¬ØŒ∂1‚àí¬Øz1)(n2+in3)]¬Øv(Œ∂1,Œ∂2) 2œÄ2(|Œ∂1‚àíz1|2+|Œ∂2‚àíz2|2)2dH3+ +(1‚àíŒ±(z1,z2))u(z1,z2), S2[u,v](z1,z2) = 2/integraldisplay Œì[(¬ØŒ∂1‚àí¬Øz1)(n0+in1)+(¬ØŒ∂2‚àí¬Øz2)(n2+in3)][v(Œ∂1,Œ∂2)‚àív(z1,z2)] 2œÄ2(|Œ∂1‚àíz1|2+|Œ∂2‚àíz2|2)2dH3+6.4. POINCAR ¬¥E-BERTRANDFORMULAFORTHECAUCHY-CIMMINOSINGULARINTEGR ALS33 +2/integraldisplay Œì[(¬ØŒ∂2‚àí¬Øz2)(n0+in1)‚àí(¬ØŒ∂1‚àí¬Øz1)(n2+in3)][¬Øu(Œ∂1,Œ∂2)‚àí¬Øu(z1,z2)] 2œÄ2(|Œ∂1‚àíz1|2+|Œ∂2‚àíz2|2)2dH3+ +v(z1,z2) = = 2/integraldisplay Œì[(¬ØŒ∂1‚àí¬Øz1)(n0+in1)+(¬ØŒ∂2‚àí¬Øz2)(n2+in3)]v(Œ∂1,Œ∂2) 2œÄ2(|Œ∂1‚àíz1|2+|Œ∂2‚àíz2|2)2dH3+ +2/integraldisplay Œì[(¬ØŒ∂2‚àí¬Øz2)(n0+in1)‚àí(¬ØŒ∂1‚àí¬Øz1)(n2+in3)]¬Øu(Œ∂1,Œ∂2) 2œÄ2(|Œ∂1‚àíz1|2+|Œ∂2‚àíz2|2)2dH3+ +(1‚àíŒ±(z1,z2))v(z1,z2). Returning to Section 6.3 substitute (6.4.2) and (6.4.3) into Theorem 6 .3.6, we have: let ‚Ñ¶ be a bounded domain in R4with AD-regular boundary Œì and let ( u,v)‚ààC0,ŒΩ(Œì√óŒì,C)√óC0,ŒΩ(Œì√óŒì,C), then for all t‚ààŒì /integraldisplay ŒìœÑ/integraldisplay ŒìŒæ[K1(œÑ‚àít)+K2(œÑ‚àít)j]{[K1(Œæ‚àíœÑ)+K2(Œæ‚àíœÑ)j](u(Œæ,œÑ)‚àíu(œÑ,t)+ +(v(Œæ,œÑ)‚àív(œÑ,t))j)dH3 ŒædH3 œÑ/bracerightbig = =/integraldisplay ŒìœÑ/integraldisplay ŒìŒæ[K1(œÑ‚àít)+K2(œÑ‚àít)j]{K1(Œæ‚àíœÑ)(u(Œæ,œÑ)‚àíu(œÑ,t))+K1(Œæ‚àíœÑ)(v(Œæ,œÑ)‚àív(œÑ,t))j+ +K2(Œæ‚àíœÑ)j(u(Œæ,œÑ)‚àíu(œÑ,t))+K2(Œæ‚àíœÑ)j(v(Œæ,œÑ)‚àív(œÑ,t))jdH3 ŒædH3 œÑ/bracerightbig = =/integraldisplay ŒìœÑ/integraldisplay ŒìŒæ{K1(œÑ‚àít)K1(Œæ‚àíœÑ)(u(Œæ,œÑ)‚àíu(œÑ,t))+ +K1(œÑ‚àít)K1(Œæ‚àíœÑ)(v(Œæ,œÑ)‚àív(œÑ,t))j+ +K1(œÑ‚àít)K2(Œæ‚àíœÑ)j(u(Œæ,œÑ)‚àíu(œÑ,t))+ +K1(œÑ‚àít)K2(Œæ‚àíœÑ)j(v(Œæ,œÑ)‚àív(œÑ,t))j+ +K2(œÑ‚àít)jK1(Œæ‚àíœÑ)(u(Œæ,œÑ)‚àíu(œÑ,t))+ +K2(œÑ‚àít)jK1(Œæ‚àíœÑ)(v(Œæ,œÑ)‚àív(œÑ,t))j+ +K2(œÑ‚àít)jK2(Œæ‚àíœÑ)j(u(Œæ,œÑ)‚àíu(œÑ,t))+ +K2(œÑ‚àít)jK2(Œæ‚àíœÑ)j(v(Œæ,œÑ)‚àív(œÑ,t))j}dH3 ŒædH3 œÑ. Note that /integraldisplay ŒìŒæ/integraldisplay ŒìœÑKœà(œÑ‚àít)nœà(œÑ)dH3 œÑKœà(Œæ‚àíœÑ)nœà(Œæ)[f(Œæ,œÑ)‚àíf(œÑ,t)]dH3 Œæ+Œ±2(t)f(t,t) = =/integraldisplay ŒìŒæ/integraldisplay ŒìœÑ{K1(œÑ‚àít)K1(Œæ‚àíœÑ)(u(Œæ,œÑ)‚àíu(œÑ,t))+ +K1(œÑ‚àít)K1(Œæ‚àíœÑ)(v(Œæ,œÑ)‚àív(œÑ,t))j+34 CHAPTER 6. COMPLEX AND QUATERNIONIC NEURAL NETWORKS +K1(œÑ‚àít)K2(Œæ‚àíœÑ)j(u(Œæ,œÑ)‚àíu(œÑ,t))+ +K1(œÑ‚àít)K2(Œæ‚àíœÑ)j(v(Œæ,œÑ)‚àív(œÑ,t))j+ +K2(œÑ‚àít)jK1(Œæ‚àíœÑ)(u(Œæ,œÑ)‚àíu(œÑ,t))+ +K2(œÑ‚àít)jK1(Œæ‚àíœÑ)(v(Œæ,œÑ)‚àív(œÑ,t))j+ +K2(œÑ‚àít)jK2(Œæ‚àíœÑ)j(u(Œæ,œÑ)‚àíu(œÑ,t))+ +K2(œÑ‚àít)jK2(Œæ‚àíœÑ)j(v(Œæ,œÑ)‚àív(œÑ,t))j}dH3 œÑdH3 Œæ+ +Œ±2(t)(u(t,t)+v(t,t)j). If one separates complex coordinates into above equality, then th e following equalities can be easy obtained formulae for Cimmino system: /integraldisplay ŒìœÑ/integraldisplay ŒìŒæ[K1(œÑ‚àít)K1(Œæ‚àíœÑ)(u(Œæ,œÑ)‚àíu(œÑ,t))+ +K1(œÑ‚àít)K2(Œæ‚àíœÑ)j(v(Œæ,œÑ)‚àív(œÑ,t))j+ +K2(œÑ‚àít)jK1(Œæ‚àíœÑ)(v(Œæ,œÑ)‚àív(œÑ,t))j+ +K2(œÑ‚àít)jK2(Œæ‚àíœÑ)j(u(Œæ,œÑ)‚àíu(œÑ,t))]dH3 ŒædH3 œÑ= =/integraldisplay ŒìŒæ/integraldisplay ŒìœÑ[K1(œÑ‚àít)K1(Œæ‚àíœÑ)(u(Œæ,œÑ)‚àíu(œÑ,t))+ +K1(œÑ‚àít)K2(Œæ‚àíœÑ)j(v(Œæ,œÑ)‚àív(œÑ,t))j+ +K2(œÑ‚àít)jK1(Œæ‚àíœÑ)(v(Œæ,œÑ)‚àív(œÑ,t))j+ +K2(œÑ‚àít)jK2(Œæ‚àíœÑ)j(u(Œæ,œÑ)‚àíu(œÑ,t))]dH3 œÑdH3 Œæ+ +Œ±2(t)u(t,t); and /integraldisplay ŒìœÑ/integraldisplay ŒìŒæ[K1(œÑ‚àít)K1(Œæ‚àíœÑ)(v(Œæ,œÑ)‚àív(œÑ,t))j+ +K1(œÑ‚àít)K2(Œæ‚àíœÑ)j(u(Œæ,œÑ)‚àíu(œÑ,t))+ +K2(œÑ‚àít)jK1(Œæ‚àíœÑ)(u(Œæ,œÑ)‚àíu(œÑ,t))+ +K2(œÑ‚àít)jK2(Œæ‚àíœÑ)j(v(Œæ,œÑ)‚àív(œÑ,t))j]dH3 ŒædH3 œÑ= =/integraldisplay ŒìŒæ/integraldisplay ŒìœÑ[K1(œÑ‚àít)K1(Œæ‚àíœÑ)(v(Œæ,œÑ)‚àív(œÑ,t))j+ +K1(œÑ‚àít)K2(Œæ‚àíœÑ)j(u(Œæ,œÑ)‚àíu(œÑ,t))+ +K2(œÑ‚àít)jK1(Œæ‚àíœÑ)(u(Œæ,œÑ)‚àíu(œÑ,t))+ +K2(œÑ‚àít)jK2(Œæ‚àíœÑ)j(v(Œæ,œÑ)‚àív(œÑ,t))j]dH3 œÑdH3 Œæ+ +Œ±2(t)v(t,t)j.6.4. POINCAR ¬¥E-BERTRANDFORMULAFORTHECAUCHY-CIMMINOSINGULARINTEGR ALS35 Here, we have /integraldisplay ŒìœÑ/integraldisplay ŒìŒæ[K1(œÑ‚àít)K1(Œæ‚àíœÑ)(u(Œæ,œÑ)‚àíu(œÑ,t))‚àí (6.4.4) ‚àíK1(œÑ‚àít)K2(Œæ‚àíœÑ)(v(Œæ,œÑ)‚àív(œÑ,t))‚àí ‚àíK2(œÑ‚àít)K1(Œæ‚àíœÑ)(v(Œæ,œÑ)‚àív(œÑ,t))‚àí ‚àíK2(œÑ‚àít)K2(Œæ‚àíœÑ)(u(Œæ,œÑ)‚àíu(œÑ,t))]dH3 ŒædH3 œÑ= =/integraldisplay ŒìŒæ/integraldisplay ŒìœÑ[K1(œÑ‚àít)K1(Œæ‚àíœÑ)(u(Œæ,œÑ)‚àíu(œÑ,t))‚àí ‚àíK1(œÑ‚àít)K2(Œæ‚àíœÑ)(v(Œæ,œÑ)‚àív(œÑ,t))‚àí ‚àíK2(œÑ‚àít)K1(Œæ‚àíœÑ)(v(Œæ,œÑ)‚àív(œÑ,t))‚àí ‚àíK2(œÑ‚àít)K2(Œæ‚àíœÑ)(u(Œæ,œÑ)‚àíu(œÑ,t))]dH3 œÑdH3 Œæ+ +Œ±2(t)u(t,t); and /integraldisplay ŒìœÑ/integraldisplay ŒìŒæ[K1(œÑ‚àít)K1(Œæ‚àíœÑ)(v(Œæ,œÑ)‚àív(œÑ,t))+ (6.4.5) +K1(œÑ‚àít)K2(Œæ‚àíœÑ)(u(Œæ,œÑ)‚àíu(œÑ,t))+ +K2(œÑ‚àít)K1(Œæ‚àíœÑ)(u(Œæ,œÑ)‚àíu(œÑ,t))‚àí ‚àíK2(œÑ‚àít)K2(Œæ‚àíœÑ)(v(Œæ,œÑ)‚àív(œÑ,t))]dH3 ŒædH3 œÑ= =/integraldisplay ŒìŒæ/integraldisplay ŒìœÑ[K1(œÑ‚àít)K1(Œæ‚àíœÑ)(v(Œæ,œÑ)‚àív(œÑ,t))+ +K1(œÑ‚àít)K2(Œæ‚àíœÑ)(u(Œæ,œÑ)‚àíu(œÑ,t))+ +K2(œÑ‚àít)K1(Œæ‚àíœÑ)(u(Œæ,œÑ)‚àíu(œÑ,t))‚àí ‚àíK2(œÑ‚àít)K2(Œæ‚àíœÑ)(v(Œæ,œÑ)‚àív(œÑ,t))]dH3 œÑdH3 Œæ+ +Œ±2(t)v(t,t). Let N1[f](z) := 2/integraldisplay ŒìK1(Œæ‚àíz)f(Œæ)dH3 Œæ+(1‚àíŒ±(z))f(z),‚àÄz‚ààŒì, and N2[f](z) :=‚àí2/integraldisplay ŒìK2(Œæ‚àíz)f(Œæ)dH3 Œæ+(1‚àíŒ±(z))f(z),‚àÄz‚ààŒì. If one separates complex coordinates in Lemma 6.3.4 we have: /integraldisplay ŒìŒæ/integraldisplay ŒìœÑK1(œÑ‚àíz)K1(Œæ‚àíœÑ)u(Œæ)dH3 œÑdH3 Œæ‚àí/integraldisplay ŒìŒæ/integraldisplay ŒìœÑK1(œÑ‚àíz)K2(Œæ‚àíœÑ)v(Œæ)dH3 œÑdH3 Œæ‚àí36 CHAPTER 6. COMPLEX AND QUATERNIONIC NEURAL NETWORKS ‚àí/integraldisplay ŒìŒæ/integraldisplay ŒìœÑK2(œÑ‚àíz)K1(Œæ‚àíœÑ)v(Œæ)dH3 œÑdH3 Œæ‚àí/integraldisplay ŒìŒæ/integraldisplay ŒìœÑK2(œÑ‚àíz)K2(Œæ‚àíœÑ)u(Œæ)dH3 œÑdH3 Œæ= 0, /integraldisplay ŒìŒæ/integraldisplay ŒìœÑK1(œÑ‚àíz)K1(Œæ‚àíœÑ)v(Œæ)dH3 œÑdH3 Œæ+/integraldisplay ŒìŒæ/integraldisplay ŒìœÑK1(œÑ‚àíz)K2(Œæ‚àíœÑ)u(Œæ)dH3 œÑdH3 Œæ+ +/integraldisplay ŒìŒæ/integraldisplay ŒìœÑK2(œÑ‚àíz)K1(Œæ‚àíœÑ)u(Œæ)dH3 œÑdH3 Œæ‚àí/integraldisplay ŒìŒæ/integraldisplay ŒìœÑK2(œÑ‚àíz)K2(Œæ‚àíœÑ)v(Œæ)dH3 œÑdH3 Œæ= 0. Thus, if functions uandvdepend only on Œæ, then we can write: N2 1‚àíN2 2=I, (6.4.6) N1N2+N2N1= 0. (6.4.7) Remark 6.4.1 Note thatN2 2/\e}atio\slash= 0in (6.4.6). Indeed, if N2 2[f] = 0for allf‚ààC1,ŒΩ(Œì,C). Then the functionN2[f]can be holomorphically extended from Œìinto‚Ñ¶+and by the uniqueness theorem for harmonic functions this extension is given by F(z) =‚àí2/integraldisplay ŒìK2(Œæ‚àíz)f(Œæ)dH3 Œæ, z‚àà‚Ñ¶+. But thenœàC[f]and (6.4.1) imply that the function Gf(z) :=/integraldisplay ŒìK1(Œæ‚àíz)f(Œæ)dH3 Œæ, is holomorphic for any f‚ààC1,ŒΩ(Œì,C)which is not true. Theorem 6.4.2 Letf‚ààC1(‚Ñ¶+,C)is representable in ‚Ñ¶+‚äÇC2by f(z) =/integraldisplay ŒìK1(Œæ‚àíz)f(Œæ)dH3 Œæ,z‚àà‚Ñ¶+. Thenfis holomorphic in ‚Ñ¶+. Proof.Applying the Sokhotski-Plemelj formulae to fwe have f(z) =1 2[N1[f](z)+f(z)],z‚ààŒì. Thus,N2 1[f] =I[f], and from (6.4.6) we have N2 2[f] = 0. But by Remark 6.4.1 we have that functionFdeÔ¨Åned by F(z) :=/integraldisplay ŒìK1(Œæ‚àíz)f(Œæ)dH3 Œæ is holomorphic in ‚Ñ¶+and withF|‚Ñ¶+=uwe completed the proof. /square From Lemma 6.3.4 and [10, page 211], the term /integraldisplay ŒìŒæ/integraldisplay ŒìœÑK1(œÑ‚àít)K1(Œæ‚àíœÑ)dH3 œÑdH3 Œæ= 0‚àÄt‚ààŒì.BIBLIOGRAPHY 37 Then from Section 6.4, we have that /integraldisplay ŒìŒæ/integraldisplay ŒìœÑK2(œÑ‚àít)K2(Œæ‚àíœÑ)dH3 œÑdH3 Œæ= 0. (6.4.8) So, by using (6.4.8) and Theorem 6.3.6, for t‚ààŒì andf‚ààC0,ŒΩ(Œì√óŒì,C) we have /integraldisplay ŒìœÑ/integraldisplay ŒìŒæK2(œÑ‚àít)K2(Œæ‚àíœÑ)[f(Œæ,œÑ)‚àíf(œÑ,t)]dH3 œÑdH3 Œæ= (6.4.9) =/integraldisplay ŒìŒæ/integraldisplay ŒìœÑK2(œÑ‚àít)K2(Œæ‚àíœÑ)[f(Œæ,œÑ)‚àíf(œÑ,t)]dH3 ŒædH3 œÑ. Comparing that last equality with (6.4.4) and (6.4.5), for f‚ààC0,ŒΩ(Œì√óŒì,C) for singular integrals for Cimmino system the structural analog of the Poin¬¥ care-Bertr and formula is true: /integraldisplay ŒìœÑ/integraldisplay ŒìŒæK1(œÑ‚àít)K1(Œæ‚àíœÑ)[f(Œæ,œÑ)‚àíf(œÑ,t)]dH3 ŒædH3 œÑ= =/integraldisplay ŒìŒæ/integraldisplay ŒìœÑK1(œÑ‚àít)K1(Œæ‚àíœÑ)[f(Œæ,œÑ)‚àíf(œÑ,t)]dH3 œÑdH3 Œæ+Œ±2(t)f(t,t). Acknowledgement The article has been supported by the Polish National Agency for Ac ademic Exchange Strategic Partnership Programme under Grant No. BPI/PST/2021/1/0003 1. Bibliography [1] Hirose, A. Complex-Valued Neural Networks - Advances and App lications. John Wiley & Sons Inc. 2013, 304 pp. [2] Isokawa, T., Kusakabe, T., Matsui, N., Peper, F. Quaternion Neu ral Network and Its Appli- cation. In: Palade, V., Howlett, R.J., Jain, L. (eds) Knowledge-Base d Intelligent Information and Engineering Systems. KES 2003. Lecture Notes in Computer Sc ience, vol 2774. Springer, Berlin, Heidelberg, 2003. [3] R. Abreu Blaya, J. Bory Reyes and B. Kats (2015): Cauchy integ ral and singular integral operator over closed Jordan curves. Monatsh Math. 176: 1‚Äì15. [4] R. Abreu Blaya, J. Bory Reyes, A. Guzm¬¥ an Ad¬¥ an and B. Schneid er (2012): Boundary value problems for the Cimmino system via quaternionic analysis. Appl. Math . Comp., 219, 3872‚Äì 3881. [5] R. Abreu Blaya, J. Bory Reyes and B. Schneider (2014): On Cauc hy type integrals related to the Cimmino system of partial diÔ¨Äerential equations. Operator the ory, operator algebras and applications, 81‚Äì92, Oper. Theory Adv. Appl., 242, Birkh¬® auser/S pringer, Basel.38 BIBLIOGRAPHY [6] G. David and S. Semmes (1993): Analysis of and on uniformly rectiÔ¨Å able sets. Mathematical Surveys and Monographs 38, AMS, Providence, R.I. [7] H. Federer (1969): Geometric Measure Theory, Grundlehren M ath. Wiss. 153, Springer, New York. [8] K. G¬® urlebeck and W. Spr¬® ossig (1997): Quaternionic and CliÔ¨Äord Calculus for Physicists and Engineers. John Wiley & Sones, England, 371 pp. [9] V. Kravchenko and M. Shapiro (1996): Integral Representat ions for Spatial Models of Mathe- matical Physics, Pitman Res. Notes in Math. Ser., vol. 351, Longman , Harlow. [10] A. M. Kytmanov (1995): The Bochner-Martinelli Integral and Its Applications. Birkh¬® auser. [11] I. Mitelman, M. Shapiro (1994): Formulae of changing of integra tion order and of inversion for some multidimensional singular integrals and hypercomplex analys is, J. Nat. Geom. 5 (1), 11‚Äì27. [12] M. Shapiro (1995): Some remarks on generalizations of the one -dimensional complex analysis: Hypercomplex approach. Functional Analytic Methods in Complex An alysis and Applications to Partial DiÔ¨Äerential Equations (Trieste, 1993). World ScientiÔ¨Åc P ubl., River Edge, N.J., 379‚Äì401.Chapter 7 Fuzzy neural networks I. PerÔ¨Åljeva, V. Novak IntheÔ¨ÅeldofartiÔ¨Åcialintelligence, neuro-fuzzyreferstocombina tionofartiÔ¨Åcialneuralnetworks and fuzzy logic [4]. A neuro-fuzzy system is commonly known in the literature as a fuzzy neural network (FNN) or a neuro-fuzzy system (NFS). A neuro-fuzzy system (used he reafter) incorporates the human reasoning style of fuzzy systems through the use of fuzzy sets a nd a linguistic model consisting of a set of fuzzy IF-THEN rules. The main strength of neuro-fuzzy s ystems is that they are universal approximators, the result of which allows interpretation by fuzzy I F-THEN rules [2, 3, 4]. The main speciÔ¨Åcity of neuro-fuzzy systems is the presence of two conÔ¨Çicting requirements for fuzzy modeling: interpretability and accuracy. In practice, one of two requirements prevails. As a consequence, the Ô¨Åeld of study of neuro-fuzzy systems is div ided into two areas: linguistic fuzzymodelingfocusedoninterpretability,mainlythe Mamdanimode l; andaccuracy-orientedfuzzy modeling, mainly the Takagi-Sugeno-Kangi (TSK) model. A new line of research in the Ô¨Åeld of data Ô¨Çow mining considers the case when neuro-fuzzy systems are constantly updated with new incoming data. The syste m‚Äôs response lies in its dynamic updates, including not only recursive adaptation of model paramet ers, but also dynamic evolu- tion and reduction of model components to adequately handle conc ept drift and keep the model ‚Äúrelevant‚Äù at all times. Detailed reviews of various approaches to t he development of neuro-fuzzy systems can be found in [2] and [3]. A neuro-fuzzy system is represented as special three-layer fee dforward neural network (ANN) where [4] ‚Ä¢The Ô¨Årst layer corresponds to the input variables, ‚Ä¢The second layer symbolizes the fuzzy rules, ‚Ä¢The third layer represents the output variables, ‚Ä¢The fuzzy sets are converted as (fuzzy) connection weights. The learning procedure is constrained to ensure the semantic prop erties of the underlying fuzzy system. 3940 CHAPTER 7. FUZZY NEURAL NETWORKS Both characteristics: interpretability and accuracy become relev ant when the NFS has already been successfully developed for solving a speciÔ¨Åc problem. This prob lem-oriented perspective ex- poses the limitations of NFS modeled by artiÔ¨Åcial neural networks (A NNs) and raises the question: can NFS be extended to the next generation of Convolutional Neur al Networks (CNNs)? Below we consider the main problems speciÔ¨Åc to neural network computing te chnology. The main problems solved with the help of neural networks are classiÔ¨Å cation and regression. Other problems are their modiÔ¨Åcations. For example, semantic/inst ance segmentation is based on pixel-wise classiÔ¨Åcation; object detection is a regression on rectan gle/polygon areas, time series prediction is a regression, etc. Let us discuss and compare the capabilities of ANN and CNN in solving th ese problems [6, 9]. Both neural networks as computational models have a similar arc hitecture with a common step ‚Äî feature extraction. The main diÔ¨Äerence is how they transfo rm the input. From a CNN point of view, feature extraction is a gradual process focused on data with spatial dependencies. The convolution shifts its window over the data, which leads to invaria nce to data translation. Convolutions gradually extract many complex and abstract featur es. The result of this stage is a vector of descriptive features. On the other hand, ANN feature extraction can be interpreted as a transformation of the input spaceinto a spacemoresuitable fora giventask, i.e., makingdatasam ples separable. Consequently, ANN is typically used for data without spatial dependencies, such as tabular data. The diÔ¨Äerence between ANN and CNN appears in diÔ¨Äerent models of th eir computational units ‚Äì neurons. ANN neuron output a is given as a=g(b+/summationdisplay iwixi) =g(b+wx), while the convolutional neuron output aijis given as aij=g+b(l/summationdisplay m=1l/summationdisplay n=1Wm,nxi+m,j+n), whereWis a convolutional kernel. CNNs are currently the state-of-the-art models in all major comp uter vision tasks, from image classiÔ¨Åcation and object detection to instance segmentation [17, 8 , 9]. CNNs combine three archi- tectural ideas: local receptive Ô¨Åelds to extract elementary feat ures from images; shared weights to extract the same set of elementary features from the entire inpu t image and to lower computational costs; local averaging and sub-sampling to reduce the resolution o f feature maps. Typically, CNNs are built as a sequence of convolutional layers and po oled layers to automat- ically learn higher and higher level features [5, 9]. At the end of the se quence, one or more fully connected layers are used to map the output feature map to the s cores. This structure entails complex internal relationships, which are diÔ¨Éc ult to explain using the Mamdani or Takagi-Sugeno type fuzzy models discussed above. Fo rtunately, the path to explain- ability for CNNs is easier than for other types of NN models, since hum an cognitive abilities contribute to the understanding of visual data. If we agree that the interpretability of a model is something that comes from the design of the model itself then [1] an explainable AI is one that oÔ¨Äers reasonable data processi ng details that make its operation clear or easy to understand.BIBLIOGRAPHY 41 With this observation in mind, we single out one particular fuzzy modelin g technique, known as fuzzy (F-)transforms, as a technique whose computational mod el is similar to the CNN model [14]. It has been proven in many papers [10]‚Äì[14] that the higher degree F-transforms are univer- sal approximators of smooth and discrete functions. The approx imation on a whole domain is a combination of locally best approximations called F-transform compo nents. They are represented by higher degree polynomials and parametrized by coeÔ¨Écients that c orrespond to average values of local and nonlocal derivatives of various degrees. If the F-tra nsform is applied to images, then its parameters are used in regularization, edge detection, charac terization of patches [15], [7], etc. Their computation can be performed by discrete convolutions with k ernels that, up to the second degree, are similar to those widely used in image processing, namely: G aussian, Sobel, Laplacian [16]. Thus, we can draw an analogy with the CNN method of computatio n and call the parameters of the higher degree F-transform features. Moreover, based o n a clear understanding of these fea- tures‚Äô semantic meaning, we say that a CNN with the F-transform k ernels extracts features with a clear interpretation. In addition, the sequential application of F- transform kernels with an up to the second degree gives average (nonlocal) derivatives of higher a nd higher degrees. The following text details the neural network design supported by t he theoretically proven F- transform methodology. The LeNet-5 architecture was chosen a s the prototype architecture, and a new CNN called FTNet was compiled with kernels taken from the F-tran sform theory of the higher degree. The performance of FTNet was examined on several datasets and on them it converges faster in terms of accuracy/loss than the baseline network, subject to t he same number of steps. We compared the F-transform kernels in the Ô¨Årst layer before and af ter training. We observed that the kernels remain unchanged. Moreover, their shapes are similar t o the shapes of extracted kernel groups from the most known CNNs: VGG16 [8], VGG19 [8], InceptionV 3 [9], MobileNet [14], ResNet [14], and AlexNet [17] as the representative examples of CNN s. Acknowledgement The article has been supported by the Polish National Agency for Ac ademic Exchange Strategic Partnership Programme under Grant No. BPI/PST/2021/1/0003 1. Bibliography [1] A. B. Arrieta, N. D¬¥ ƒ±az-Rodr¬¥ ƒ±guez, J. Del Ser, A. Bennetot, S . Tabik, A. Barbado, S. Garc¬¥ ƒ±a, S.Gil-L¬¥ opez, D. Molina, R. Benjamins, R. Chatila, F. Herrera, Explain able ArtiÔ¨Åcial Intelli- gence (XAI): Concepts, taxonomies, opportunities and challenge s toward responsible AI, In- formation Fusion, 58 (2020) 82-115. [2] Kosko, Bart (1992). Neural Networks and Fuzzy Systems: A D ynamical Systems Approach to Machine Intelligence. Englewood CliÔ¨Äs, NJ: Prentice Hall. ISBN 0-13-6 11435-0. [3] Lin, C.-T., Lee, C.S.G.(1996).NeuralFuzzySystems: ANeuro -FuzzySynergismtoIntelligent Systems. Upper Saddle River, NJ: Prentice Hall. [4] Klawonn, F., Kruse R., Nauck, D. and Borgelt, C. (2003). Neuro- Fuzzy-Systeme (Vieweg, Wiesbaden).42 BIBLIOGRAPHY [5] E.A. Popko, I.A. Weinstein, Fuzzy logic module of convolutional neu ral network for handwrit- ten digits recognition, in Journal of Physics: Conference Series 73 8 (2016) 012123. [6] O. Yazdanbakhsh, S. Dick, A deep neuro-fuzzy network for ima ge classiÔ¨Åcation, arXiv preprint arXiv:2001.01686, 2019 [7] X. Gastaldi, Shake-shake regularization, arXiv preprint arXiv:17 05.07485, 2017. [8] J.M. Ogden, E.H. Adelson, J.R. Bergen, P.J. Burt, Pyramid-based computer graphics, RCA Eng. 30 (1985), 4‚Äì15. [9] K.Simonyan,A.Zisserman,Verydeepconvolutionalnetworksf orlarge-scaleimagerecognition, arXiv preprint arXiv:1409.1556,2014. [10] I. PerÔ¨Ålieva, Fuzzy transforms: Theory and applications, Fuz zy sets and systems, 157/8 (2006 )993‚Äì1023. [11] PerÔ¨Ålieva, I., DaÀá nkov¬¥ a, M., Bede, B. Towards a higher degree F-transform. Fuzzy Sets and Systems 2011, sv. 180, s. 3-19. ISSN 1063-6706. [12] I. PerÔ¨Ålieva, M. Holcapek, V. Kreinovich, A new reconstruction from the F-transform compo- nents, Fuzzy Sets and Systems, 288 (2016) 3‚Äì25. [13] P. Hurtik, V. Molek, I. PerÔ¨Ålieva, Novel dimensionality reduction approach for unsupervised learning on small datasets, Pattern Recognition, 103 (2020) 1072 91. [14] V. Molek, I. PerÔ¨Ålieva, Deep Learning and Higher Degree F-Tran sforms: Interpretable Kernels Before and After Learning, International Journal of Computat ional Intelligence Systems, 13/1 (2020) 1404 - 1414. [15] I. PerÔ¨Ålieva, P. VlaÀá s¬¥ anek, Total variation with nonlocal FT-L aplacian for patch-based inpaint- ing, Soft Comput. 23 (2019), 1833‚Äì1841. [16] G. Patane, Data-Driven Fuzzy Transform, IEEE Transaction s on Fuzzy Systems, 30/9 (2022) 3774-3784. [17] K.K.Pal, K.S. Sudeep, Preprocessingfor imageclassiÔ¨Åcationby c onvolutionalneural networks, in IEEE International Conferenceon Recent Trends in Electronics , Information & Communi- cation Technology (RTEICT), IEEE, Bangalore, India, 2016, pp.17 78‚Äì1781."
8,https://arxiv.org,2301.00008,10.48550/arXiv.2301.00008,"Saket Tiwari, George Konidaris",Effects of Data Geometry in Early Deep Learning,"Deep neural networks can approximate functions on different types of data, from images to graphs, with varied underlying structure. This underlying structure can be viewed as the geometry of the data manifold. By extending recent advances in the theoretical understanding of neural networks, we study how a randomly initialized neural network with piece-wise linear activation splits the data manifold into regions where the neural network behaves as a linear function. We derive bounds on the density of boundary of linear regions and the distance to these boundaries on the data manifold. This leads to insights into the expressivity of randomly initialized deep neural networks on non-Euclidean data sets. We empirically corroborate our theoretical results using a toy supervised learning problem. Our experiments demonstrate that number of linear regions varies across manifolds and the results hold with changing neural network architectures. We further demonstrate how the complexity of linear regions is different on the low dimensional manifold of images as compared to the Euclidean space, using the MetFaces dataset.","Effects of Data Geometry in Early Deep Learning Saket Tiwari Department of Computer Science Brown University Providence, RI 02906 saket tiwari@brown.eduGeorge Konidaris Department of Computer Science Brown University Providence, RI 02906 Abstract Deep neural networks can approximate functions on different types of data, from images to graphs, with varied underlying structure. This underlying structure can be viewed as the geometry of the data manifold. By extending recent advances in the theoretical understanding of neural networks, we study how a randomly initialized neural network with piece-wise linear activation splits the data manifold into regions where the neural network behaves as a linear function. We derive bounds on the density of boundary of linear regions and the distance to these boundaries on the data manifold. This leads to insights into the expressivity of randomly initialized deep neural networks on non-Euclidean data sets. We empirically corroborate our theoretical results using a toy supervised learning problem. Our experiments demonstrate that number of linear regions varies across manifolds and the results hold with changing neural network architectures. We further demonstrate how the complexity of linear regions is different on the low dimensional manifold of images as compared to the Euclidean space, using the MetFaces dataset. 1 Introduction The capacity of Deep Neural Networks (DNNs) to approximate arbitrary functions given sufÔ¨Åcient training data in the supervised learning setting is well known [Cybenko, 1989, Hornik et al., 1989, Anthony and Bartlett, 1999]. Several different theoretical approaches have emerged that study the effectiveness and pitfalls of deep learning. These studies vary in their treatment of neural networks and the aspects they study range from convergence [Allen-Zhu et al., 2019, Goodfellow and Vinyals, 2015], generalization [Kawaguchi et al., 2017, Zhang et al., 2017, Jacot et al., 2018, Sagun et al., 2018], function complexity [Mont ¬¥ufar et al., 2014, Mhaskar and Poggio, 2016], adversarial attacks [Szegedy et al., 2014, Goodfellow et al., 2015] to representation capacity [Arpit et al., 2017]. Some recent theories have also been shown to closely match empirical observations [Poole et al., 2016, Hanin and Rolnick, 2019b, Kunin et al., 2020]. One approach to studying DNNs is to examine how the underlying structure, or geometry, of the data interacts with learning dynamics. The manifold hypothesis states that high-dimensional real world data typically lies on a low dimensional manifold [Tenenbaum, 1997, Carlsson et al., 2007, Fefferman et al., 2013]. Empirical studies have shown that DNNs are highly effective in deciphering this underlying structure by learning intermediate latent representations [Poole et al., 2016]. The ability of DNNs to ‚ÄúÔ¨Çatten‚Äù complex data manifolds, using composition of seemingly simple piece-wise linear functions, appears to be unique [Brahma et al., 2016, Hauser and Ray, 2017]. DNNs with piece-wise linear activations, such as ReLU [Nair and Hinton, 2010], divide the input space into linear regions, wherein the DNN behaves as a linear function [Mont ¬¥ufar et al., 2014]. The density of these linear regions serves as a proxy for the DNN‚Äôs ability to interpolate a complex data landscape and has been the subject of detailed studies [Mont ¬¥ufar et al., 2014, Telgarsky, 2015, Serra 36th Conference on Neural Information Processing Systems (NeurIPS 2022).arXiv:2301.00008v1  [cs.LG]  29 Dec 2022et al., 2018, Raghu et al., 2017]. The work by Hanin and Rolnick [2019a] on this topic stands out because they derive bounds on the average number of linear regions and verify the tightness of these bounds empirically for deep ReLU networks, instead of larger bounds that rarely materialize. Hanin and Rolnick [2019a] conjecture that the number of linear regions correlates to the expressive power of randomly initialized DNNs with piece-wise linear activations. However, they assume that the data is uniformly sampled from the Euclidean space Rd, for somed. By combining the manifold hypothesis with insights from Hanin and Rolnick [2019a], we are able to go further in estimating the number of linear regions and the average distance from linear boundaries . We derive bounds on how the geometry of the data manifold affects the aforementioned quantities. To corroborate our theoretical bounds with empirical results, we design a toy problem where the input data is sampled from two distinct manifolds that can be represented in a closed form. We count the exact number of linear regions and the average distance to the boundaries of linear regions on these two manifolds that a neural network divides the two manifolds into. We demonstrate how the number of linear regions and average distance varies for these two distinct manifolds. These results show that the number of linear regions on the manifold do not grow exponentially with the dimension of input data. Our experiments do not provide estimates for theoretical constants, as in most deep learning theory, but demonstrate that the number of linear regions change as a consequence of these constants. We also study linear regions of deep ReLU networks for high dimensional data that lies on a low dimensional manifold with unknown structure and how the number of linear regions vary on and off this manifold, which is a more realistic setting. To achieve this we present experiments performed on the manifold of natural face images. We sample data from the image manifold using a generative adversarial network (GAN) [Goodfellow et al., 2014] trained on the curated images of paintings. SpeciÔ¨Åcally, we generate images using the pre-trained StyleGAN [Karras et al., 2019, 2020b] trained on the curated MetFaces dataset [Karras et al., 2020a]. We generate curves on the image manifold of faces, using StyleGAN, and report how the density of linear regions varies on and off the manifold. These results shed new light on the geometry of deep learning over structured data sets by taking a data intrinsic approach to understanding the expressive power of DNNs. 2 Preliminaries and Background Our goal is to understand how the underlying structure of real world data matters for deep learning. We Ô¨Årst provide the mathematical background required to model this underlying structure as the geometry of data. We then provide a summary of previous work on understanding the approximation capacity of deep ReLU networks via the complexity of linear regions. For the details on how our work Ô¨Åts into one of the two main approaches within the theory of DNNs, from the expressive power perspective or from the learning dynamics perspective, we refer the reader to Appendix C. 2.1 Data Manifold and DeÔ¨Ånitions Figure 1: A 2D surface, here represented by a 2-torus, is embedded in a larger input space, R3. Suppose each point corresponds to an image of a face on this 2-torus. We can chart two curves: one straight line cutting across the 3D space and another curve that stays on the torus. Images corresponding to the points on the torus will have a smoother variation in style and shape whereas there will be images corresponding to points on the straight line that are not faces. We use the example of the MetFaces dataset [Karras et al., 2020a] to illustrate how data lies on a low dimensional manifold. The images in the dataset are 102810283dimensional. By contrast, the number of realistic dimensions along which they vary are limited, e.g. painting style, artist, size and shape of the nose, jaw and eyes, background, clothing style; in fact, very few 102810283 2dimensional images correspond to realistic faces. We illustrate how this affects the possible variations in the data in Figure 1. A manifold formalises the notion of limited variations in high dimensional data. One can imagine that there exists an unknown function f:X!Yfrom a low dimensional space of variations, to a high dimensional space of the actual data points. Such a function f:X!Y, from one open subset XRm, to another open subset YRk, is a diffeomorphism iffis bijective, and bothfandf 1are differentiable (or smooth). Therefore, a manifold is deÔ¨Åned as follows. DeÔ¨Ånition 2.1. Letk;m2N0. A subsetMRkis called a smooth m-dimensional submanifold ofRk(orm-manifold in Rk) iff every point x2Mhas an open neighborhood URksuch that U\Mis diffeomorphic to an open subset  Rm. A diffeomorphism (i.e. differentiable mapping), f:U\M! is called a coordinate chart of M and the inverse, h:=f 1:  !U\M is called a smooth parametrization of U\M. For the MetFaces dataset example, suppose there are 10 dimensions along which the images vary. Further assume that each variation can take a value continuously in some interval of R. Then the smooth parametrization would map f:  \R10!M\R102810283. This parametrization and its inverse are unknown in general and computationally very difÔ¨Åcult to estimate in practice. There are similarities in how geometric elements are deÔ¨Åned for manifolds and Euclidean spaces. A smooth curve, on a manifold M,:I!Mis deÔ¨Åned from an interval Ito the manifold Mas a function that is differentiable for all t2I, just as for Euclidean spaces. The shortest such curve between two points on a manifold is no longer a straight line, but is instead a geodesic . One recurring geometric element, which is unique to manifolds and stems from the deÔ¨Ånition of smooth curves, is that of a tangent space , deÔ¨Åned as follows. DeÔ¨Ånition 2.2. LetMbe anm-manifold in Rkandx2Mbe a Ô¨Åxed point. A vector v2Rkis called a tangent vector of Matxif there exists a smooth curve :I!Msuch that(0) =x;_(0) =v where _(t)is the derivative of att. The set TxM:=f_(0)j:R!Mis smooth(0) =xg of tangent vectors of Matxis called the tangent space of Matx. In simpler terms, the plane tangent to the manifold Mat pointxis called the tangent space and denoted by by TxM. Consider the upper half of a 2-sphere, S2R3, which is a 2-manifold in R3. The tangent space at a Ô¨Åxed point x2S2is the 2D plane perpendicular to the vector xand tangential to the surface of the sphere that contains the point x. For additional background on manifolds we refer the reader to Appendix B. 2.2 Linear Regions of Deep ReLU Networks The higher the density of these linear regions the more complex a function a DNN can approximate. For example, a sincurve in the range [0;2]is better approximated by 4 piece-wise linear regions as opposed to 2. To clarify this further, with the 4 ‚Äúoptimal‚Äù linear regions [0;=2);[=2;);[;3=2); and[3=2;2]a function could approximate the sincurve better than any 2 linear regions. In other words, higher density of linear regions allows a DNN to approximate the variation in the curve better. We deÔ¨Åne the notion of boundary of a linear regions in this section and provide an overview of previous results. We consider a neural network, F, which is a composition of activation functions. Inputs at each layer are multiplied by a matrix, referred to as the weight matrix, with an additional bias vector that is added to this product. We limit our study to ReLU activation function [Nair and Hinton, 2010], which is piece-wise linear and one of the most popular activation functions being applied to various learning tasks on different types of data like text, images, signals etc. We further consider DNNs that map inputs, of dimension nin, to scalar values. Therefore, F:Rnin!Ris deÔ¨Åned as, F(x) =WL(BL 1+WL 1(:::(B1+W1x))); (1) whereWl2Mnlnl 1is the weight matrix for the lthhidden layer, nlis the number of neurons in thelthhidden layer, Bl2Rnlis the vector of biases for the lthhidden layer, n0=ninand:R!R 3is the activation function. For a neuron zin thelthlayer we denote the pre-activation of this neuron, for given input x2Rnin, aszl(x). For a neuron zin the layerlwe have z(x) =Wl 1;z(:::(B1+W1x)); (2) forl>1(for the base case l= 1we havez(x) =W1;zx) whereWl 1;zis the row of weights, in the weight matrix of the lthlayer,Wl, corresponding to the neuron z. We useWzto denote the weight vector for brevity, omitting the layer index lin the subscript. We also use bzto denote the bias term for the neuron z. Neural networks with piece-wise linear activations are piece-wise linear on the input space [Mont ¬¥ufar et al., 2014]. Suppose for some Ô¨Åxed y2Rninasx!yif we havez(x)! bzthen we observe a discontinuity in the gradient rx(bz+Wzz(x))aty. Intuitively, this is because xis approaching the boundary of the linear region of the function deÔ¨Åned by the output of z. Therefore, the boundary of linear regions, for a feed forward neural network F, is deÔ¨Åned as: BF=fxjrF(x)is not continuous at xg: Hanin and Rolnick [2019a] argue that an important generalization for the approximation capacity of a neural network Fis the (nin 1) dimensional volume density of linear regions deÔ¨Åned as volnin 1(BF\K)=volnin(K);for a bounded set KRnin. This quantity serves as a proxy for density of linear regions and therefore the expressive capacity of DNNs. Intuitively, higher density of linear boundaries means higher capacity of the DNN to approximate complex non-linear functions. The quantity is applied to lower bound the distance between a point x2Kand the setBF, which is distance (x;BF) = min neuronszjz(x) bzj=jjrz(x)jj; which measures the sensitivity over neurons at a given input. The above quantity measures how ‚Äúfar‚Äù the input is from Ô¨Çipping any neuron from inactive to active or vice-versa. Informally, Hanin and Rolnick [2019a] provide two main results for a randomly initialized DNN F, with a reasonable initialisation. Firstly, they show that Ehvolnin 1(BF\K) volnin(K)i #fneuronsg; meaning the density of linear regions is bound above and below by some constant times the number of neurons. Secondly, for x2[0;1]nin, Eh distance (x;BF)i C#fneuronsg 1; whereC > 0depends on the distribution of biases and weights, in addition to other factors. In other words, the distance to the nearest boundary is bounded above and below by a constant times the inverse of the number of neurons. These results stand in contrast to earlier worst case bounds that are exponential in the number of neurons. Hanin and Rolnick [2019a] also verify these results empirically to note that the constants lie in the vicinity of 1 throughout training. 3 Linear Regions on the Data Manifold One important assumption in the results presented by Hanin and Rolnick [2019a] is that the input, x, lies in a compact set KRninand that volnin(K)is greater than 0. Also, the theorem pertaining to the lower bound on average distance of xto linear boundaries the input assumes the input uniformly distributed in [0;1]nin. As noted earlier, high-dimensional real world datasets, like images, lie on low dimensional manifolds, therefore both these assumptions are false in practice. This motivates us to study the case where the data lies on some m dimensional submanifold of Rnin, i.e.MRnin wheremnin. We illustrate how this constraint effects the study of linear regions in Figure 2. As introduced by Hanin and Rolnick [2019a], we denote the ‚Äú (nin k) dimensional piece‚Äù of BFasBF;k. More precisely, BF;0=;andBF;kis recursively deÔ¨Åned to be the set of points x2BFnfBF;0[:::[BF;k 1gwith the added condition that in a neighbourhood of xthe setBF;k coincides with hyperplane of dimension nin k. We provide a detailed and formal deÔ¨Ånition for BF;k with intuition in Appendix E. In our setting, where the data lies on a manifold M, we deÔ¨ÅneB0 F;k 4Figure 2: A circle is an example of a 1D manifold in a 2D Euclidean space. The effective number of linear regions on the manifold, the upper half of the circle, are the number of linear regions on the arc from to. In the diagram above, each color in the 2D space corresponds to a linear region. When the upper half of the circle is Ô¨Çattened into a 1D space we obtain a line. Each color on the line corresponds to a linear region of the 2D space. asBF;k\M, and note that dim(B0 F;k) =m k(Appendix E Proposition E.4). For example, the transverse intersection (see DeÔ¨Ånition E.3) of a plane in 3D with the 2D manifold S2is a 1D curve in S2and therefore has dimension 1. Therefore,B0 F;kis a submanifold of dimension 3 2 = 1 . This imposes the restriction km, for the intersection BF;k\Mto have a well deÔ¨Åned volume. We Ô¨Årst note that the deÔ¨Ånition of the determinant of the Jacobian, for a collection of neurons z1;:::;zk, is different in the case when the data lies on a manifold Mas opposed to in a compact set of dimension nininRnin. Since the determinant of the Jacobian is the quantity we utilise in our proofs and theorems repeatedly we will use the term Jacobian to refer to it for succinctness. Intuitively, this follows from the Jacobian of a function being deÔ¨Åned differently in the ambient space Rninas opposed to the manifold M. In case of the former it is the volume of the paralellepiped determined by the vectors corresponding to the directions with steepest ascent along each one of the ninaxes. In case of the latter it is more complex and deÔ¨Åned below. Let Hmbe them dimensional Hausdorff measure (we refer the reader to the Appendix B for background on Hausdorff measure). The Jacobian of a function on manifold M, as deÔ¨Åned by Krantz and Parks [2008] (Chapter 5), is as follows. DeÔ¨Ånition 3.1. The (determinant of) Jacobian of a function H:M!Rk, wherekdim(M) = m, is deÔ¨Åned as JM k;H(x) = supnHk(DMH(P)) Hk(P)Pis ak-dimensional parallelepiped contained in TxM:o ; whereDM:TxM!Rkis the differential map (see Appendix B) and we use DMH(P)to denote the mapping of the set PinTxM, which is a parallelepiped, to Rk. The supremum is taken over all parallelepipeds P. We also say that neurons z1;:::;zkare good atxif there exists a path of neurons from zto the output in the computational graph of Fso that each neuron is activated along the path. Our three main results that hold under the assumptions listed in Appendix A, each of which extend and improve upon the theoretical results by Hanin and Rolnick [2019a], are: Theorem 3.2. GivenFa feed-forward ReLU network with input dimension nin, output dimension 1, and random weights and biases. Then for any bounded measurable submanifold MRninand any k= 1;::::;m the average (m k) dimensional volume of BF;kinsideM, E[volm k(BF;k\M)] =X distinct neurons z1;:::;zkinFZ ME[Yz1;:::;zk]dvolm(x); (3) whereYz1;:::;zkisJM m;H k(x)b1;:::;bk(z1(x);:::;zk(x));times the indicator function of the event that zjis good atxfor eachj= 1;:::;k . Here the function bz1;:::;bzkis the density of the joint distribution of the biases bz1;:::;bzk. This change in the formula, from Theorem 3.4 by Hanin and Rolnick [2019a], is a result of the fact thatz(x)has a different direction of steepest ascent when it is restricted to the data manifold M, for anyj. The proof is presented in Appendix E. Formula 3 also makes explicit the fact that the data manifold has dimension mninand therefore the m k-dimensional volume is a more representative measure of the linear boundaries. Equipped with Theorem 3.2, we provide a result for the density of boundary regions on manifold M. 5Theorem 3.3. For data sampled uniformly from a compact and measurable mdimensional manifold Mwe have the following result for all km: volm k(BF;k\M) volm(M) # neurons k (2CgradCbiasCM)k; whereCgraddepends onjjrz(x)jjand the DNN‚Äôs architecture, CMdepends on the geometry of M, andCbiason the distribution of biases b. The constant CMis the supremum over the matrix norm of projection matrices onto the tangent space,TxM, at any point x2M. For the Euclidean space CMis always equal to 1 and therefore the term does not appear in the work by Hanin and Rolnick [2019a], but we cannot say the same for our setting. We refer the reader to Appendix F for the proof, further details, and interpretation. Finally, under the added assumptions that the diameter of the manifold Mis Ô¨Ånite and Mhas polynomial volume growth we provide a lower bound on the average distance to the linear boundary for points on the manifold and how it depends on the geometry and dimensionality of the manifold. Theorem 3.4. For any point, x, chosen randomly from M, we have: E[distanceM(x;BF\M)]CM; CgradCbiasCM#neurons; whereCM;depends on the scalar curvature, the input dimension and the dimensionality of the manifoldM. The function distance Mis the distance on the manifold M. This result gives us intuition on how the density of linear regions around a point depends on the geometry of the manifold. The constant CM;captures how volumes are distorted on the manifold Mas compared to the Euclidean space, for the exact deÔ¨Ånition we refer the reader to the proof in Appendix G. For a manifold which has higher volume of a unit ball, on average, in comparison to the Euclidean space the constant CM;is higher and lower when the volume of unit ball, on average, is lower than the volume of the Euclidean space. For background on curvature of manifolds and a proof sketch we refer the reader to the Appendices B and D, respectively. Note that the constant CM is the same as in Theorem 3.3. Another difference to note is that we derive a lower bound on the geodesic distance on the manifold Mand not the Euclidean distance in Rkas done by Hanin and Rolnick [2019a]. This distance better captures the distance between data points on a manifold while incorporating the underlying structure. In other words, this distance can be understood as how much a data point should change to reach a linear boundary while ensuring that all the individual points on the curve, tracing this change, are ‚Äúvalid‚Äù data points. 3.1 Intuition For Theoretical Results One of the key ingredients of the proofs by Hanin and Rolnick [2019a] is the co-area formula [Krantz and Parks, 2008]. The co-area formula is applied to get a closed form representation of the k dimensional volume of the region where any set of kneurons,z1;z2;:::;zkis ‚Äúgood‚Äù in terms of the expectation over the Jacobian, in the Euclidean space. Instead of the co-area formula we use thesmooth co-area formula [Krantz and Parks, 2008] to get a closed form representation of the m k dimensional volume of the region intersected with manifold, M, in terms of the Jacobian deÔ¨Åned on a manifold (DeÔ¨Ånition 3.1). The key difference between the two formulas is that in the smooth co-area formula the Jacobian (of a function from the manifold M) is restricted to the tangent plane. While the determinant of the ‚Äúvanilla‚Äù Jacobian measures the distortion of volume around a point in Euclidean space the determinant of the Jacobian deÔ¨Åned as above (DeÔ¨Ånition 3.1) measures the distortion of volume on the manifold instead for the function with the same domain, the function that is 1 if the set of neurons are good and 0 otherwise. The value of the Jacobian as deÔ¨Åned in DeÔ¨Ånition 3.1 has the same volume as the projection of the parallelepiped deÔ¨Åned by the gradients rz(x)onto the tangent space (see Proposition F.1 in Appendix). This introduces the constant CM, deÔ¨Åned above. Essentially, the constant captures how the magnitude of the gradients, rz(x), are modiÔ¨Åed upon being projected to the tangent plane. Certain manifolds ‚Äúshrink‚Äù vectors upon projection to the tangent plane more than others, on an average, which is a function of their geometry. We illustrate how two distinct manifolds ‚Äúshrink‚Äù the gradients differently upon projection to the tangent plane as reÔ¨Çected in the number of linear regions on the manifolds (see Figure 11 in the appendix) for 1D manifolds. We provide intuition 6(a)  (b) Figure 3: The tractrix (a) and circle (b) are plotted in grey and the target function is in blue. This is for illustration purposes and does not match the actual function or domains used in our experiments. for the curvature of a manifold in Appendix B, due to space constraints, which is used in the lower bound for the average distance in Theorem 3.4. The constant CM;depends on the curvature as the supremum of a polynomial whose coefÔ¨Åcients depend on the curvature, with order at most ninand at leastnin m. Note that despite this dependence on the ambient dimension, there are other geometric constants in this polynomial (see Appendix G). Finally, we also provide a simple example as to how this constant varies with ninandm, for a simple and contrived example, in Appendix G.1. 4 Experiments 4.1 Linear Regions on a 1D Curve To empirically corroborate our theoretical results, we calculate the number of linear regions and average distance to the linear boundary on 1D curves for regression tasks in two settings. The Ô¨Årst is for 1D manifolds embedded in 2D and higher dimensions and the second is for the high-dimensional data using the MetFaces dataset. We use the same algorithm, for the toy problem and the high- dimensional dataset, to Ô¨Ånd linear regions on 1D curves. We calculate the exact number of linear regions for a 1D curve in the input space, x:I!RninwhereIis an interval in real numbers, by Ô¨Ånding the points where z(x(t)) =bzfor every neuron z. The solutions thus obtained gives us the boundaries for neurons on the curve x. We obtain these solutions by using the programmatic activation of every neuron and using the sequential least squares programming (SLSQP) algorithm [Kraft, 1988] to solve for jz(x(t)) bzj= 0fort2I. In order to obtain the programmatic activation of a neuron we construct a Deep ReLU network as deÔ¨Åned in Equation 2. We do so for all the neurons for a given DNN with Ô¨Åxed weights. 4.2 Supervised Learning on Toy Dataset We deÔ¨Åne two similar regression tasks where the data is sampled from two different manifolds with different geometries. We parameterize the Ô¨Årst task, a unit circle without its north and south poles, by circle: ( ;)!R2where circle() = (cos;sin)andis the angle made by the vector from the origin to the point with respect to the x-axis. We set the target function for regression task to be a periodic function in . The target is deÔ¨Åned as z() =asin()whereais the amplitude andis the frequency (Figure 3). DNNs have difÔ¨Åculty learning periodic functions [Ziyin et al., 2020]. The motivation behind this is to present the DNN with a challenging task where it has to learn the underlying structure of the data. Moreover the DNN will have to split the circle into linear regions. For the second regression task, a tractrix is parametrized by  tractrix :R1!R2where  tractrix(y) = (y tanhy;sechy)(see Figure 3). We assign a target function z(t) =asin(t). For the purposes of our study we restrict the domain of  tractrix to( 3;3). We choose so as to ensure that the number of peaks and troughs, 6, in the periodic target function are the same for both the manifolds. This ensures that the domains of both the problems have length close to 6.28. Further experimental details are in Appendix H. The results, averaged over 20 runs, are presented in Figures 4 and 5. We note that CMis smaller for Sphere (based on Figure 4) and the curvature is positive whilst CMis larger for tractrix and the curvature is negative. Both of these constants (curvature and CM) contribute to the lower bound 7in Theorem 3.4. Similarly, we show results of number of linear regions divided by the number of neurons upon changing architectures, consequently the number of neurons, for the two manifolds in Figure 8, averaged over 30 runs. Note that this experiment observes the effect of CMCgrad, since changing the architecture also changes Cgradand the variation in Cgradis quite low in magnitude as observed empirically by Hanin and Rolnick [2019a]. The empirical observations are consistent with our theoretical results. We observe that the number of linear regions starts off close to #neurons and remains close throughout the training process for both the manifolds. This supports our theoretical results (Theorem 3.3) that the constant CM, which is distinct across the two manifolds, affects the number of linear regions throughout training. The tractrix has a higher value of CMand that is reÔ¨Çected in both Figures 4 and 5. Note that its relationship is inverse to the average distance to the boundary region, as per Theorem 3.4, and it is reÔ¨Çected as training progresses in Figure 5. This is due to different ‚Äúshrinking‚Äù of vectors upon being projected to the tangent space (Section 3.1). 4.3 Varying Input Dimensions To empirically corroborate the results of Theorems 2 and 3 we vary the dimension ninwhile keeping mconstant. We achieve this by counting the number of linear regions and the average distance to boundary region on the 1D circle as we vary the input dimension in steps of 5. We draw samples of 1D circles in Rninby randomly choosing two perpendicular basis vectors. We then train a network with the same architecture as the previous section on the periodic target function ( asin()) as deÔ¨Åned above. The results in Figure 6 shows that the quantities stay proportional to #neurons , and do not vary asninis increased, as predicted by our theoretical results. Our empirical study asserts how the relevant upper and lower bounds, for the setting where data lies on a low-dimensional manifold, does not grow exponentially with ninfor the density of linear regions in a compact set of Rninbut instead depend on the intrinsic dimension. Further details are in Appendix H. 4.4 MetFaces: High Dimensional Dataset Our goal with this experiment is to study how the density of linear regions varies across a low dimensional manifold and the input space. To discover latent low dimensional underlying structure of data we employ a GAN. Adversarial training of GANs can be effectively applied to learn a mapping from a low dimensional latent space to high dimensional data [Goodfellow et al., 2014]. The generator is a neural network that maps g:Rk!Rnin. We train a deep ReLU network on the MetFaces dataset with random labels (chosen from 0;1) with cross entropy loss. As noted by Zhang et al. [2017], training with random labels can lead to the DNN memorizing the entire dataset. We compare the log density of number of linear regions on a curve on the manifold with a straight line off the manifold. We generate these curves using the data sampled by the StyleGAN by [Karras et al., 2020a]. SpeciÔ¨Åcally, for each curve we sample a random pair of latent vectors: z1;z22Rk, this gives us the start and end point of the curve using the generator g(z1)andg(z2). We then generate 100 images to approximate a curve connecting the two images on the image manifold in a piece-wise manner. We do so by taking 100 points on the line connecting z1andz2in the latent space that are evenly spaced and generate an image from each one of them. Therefore, the ithimage is generated as: z0 i=g(((100 i)z1+iz2)=100) , using the StyleGAN generator g. We qualitatively verify the images to ensure that they lie on the manifold of images of faces. The straight line, with two Ô¨Åxed pointsg(z1)andg(z2), is deÔ¨Åned as x(t) = (1 t)g(z1) +tg(z2)witht2[0;1]. The approximated curve on the manifold is deÔ¨Åned as x0(t) = (1 t)g(z0 i) +tg(z0 i+1)wherei=floor (100t). We then apply the method from Section 4.1 to obtain the number of linear regions on these curves. The results are presented in Figure 9. This leads us to the key observation: the density of linear regions is signiÔ¨Åcantly lower on the data manifold and devising methods to ‚Äúconcentrate‚Äù these linear regions on the manifold is a promising research direction. That could lead to increased expressivity for the same number of parameters. We provide further experimental details in Appendix I. 5 Discussion and Conclusions There is signiÔ¨Åcant work in both supervised and unsupervised learning settings for non-Euclidean data [Bronstein et al., 2017]. Despite these empirical results most theoretical analysis is agnostic to data geometry, with a few prominent exceptions [Cloninger and Klock, 2020, Shaham et al., 8Figure 4: Graph of number of linear regions for tractrix (blue) and sphere (orange). The shaded re- gions represent one standard deviation. Note that the number of neurons is 26 and the number of linear regions are comparable to 26 but different for both the manifolds throughout training. Figure 5: Graph of distance to linear regions for tractrix (blue) and sphere (orange). The distances are normalized by the maximum distance on the range, for both tractrix and sphere. The shaded regions represent one standard deviation. Figure 6: We observe that as the dimension ninis increased, while keeping the manifold dimension constant, the number of linear regions remains proportional to number of neurons (26). Figure 7: We observe that as the dimension ninis increased, while keeping the manifold dimension constant, the average distance varies very little. Figure 8: The effects of changing the architecture on the number of linear regions. We observe that the value of CMeffects the number of linear re- gions proportionally. The number of hidden units for three layer networks are in the legend along with the data manifold. Figure 9: We observe that the log density of num- ber of linear regions is lower on the manifold (blue) as compared to off the manifold (green). This is for the MetFaces dataset. 92015, Schmidt-Hieber, 2019]. We incorporate the idea of data geometry into measuring the effective approximation capacity of DNNs, deriving average bounds on the density of boundary regions and distance from the boundary when the data is sampled from a low dimensional manifold. Our experimental results corroborate our theoretical results. We also present insights into expressivity of DNNs on low dimensional manÔ¨Åolds for the case of high dimensional datasets. Estimating the geometry, dimensionality and curvature, of these image manifolds accurately is a problem that remains largely unsolved [Brehmer and Cranmer, 2020, Perraul-Joncas and Meila, 2013], which limits our inferences on high dimensional dataset to observations that guide future research. We note that proving a lower bound on the number of linear regions, as done by Hanin and Rolnick [2019a], for the manifold setting remains open. Our work opens up avenues for further research that combines model geometry and data geometry and can lead to empirical research geared towards developing DNN architectures for high dimensional datasets that lie on a low dimensional manifold. 6 Acknowledgements This work was funded by L2M (DARPA Lifelong Learning Machines program under grant number FA8750-18-2-0117), the Penn MURI (ONR under the PERISCOPE MURI Contract N00014- 17- 1-2699), and the ONR Swarm (the ONR under grant number N00014-21-1-2200). This research was conducted using computational resources and services at the Center for Computation and Visualization, Brown University. We would like to thank Sam Lobel, Rafael Rodriguez Sanchez, and Akhil Bagaria for reÔ¨Åning our work, multiple technical discussions, and their helpful feedback on the implementation details. We also thank Tejas Kotwal for assistance on deriving the mathematical details related to the 1D Tractrix and sources for various citations. We thank Professor Pedro Lopes de Almeida, Nihal Nayak, Cameron Allen and Aarushi Kalra for their valuable comments on writing and presentation of our work. We thank all the members of the Brown robotics lab for their guidance and support at various stages of our work. Finally, we are indebted to, and graciously thank, the numerous anonymous reviewers for their time and labor as their valuable feedback and thoughtful engagement have shaped and vastly reÔ¨Åne our work. References Zeyuan Allen-Zhu, Y . Li, and Zhao Song. A convergence theory for deep learning via over- parameterization. ArXiv , abs/1811.03962, 2019. M. Anthony and P. Bartlett. Neural network learning - theoretical foundations. In Neural Network Learning - Theoretical Foundations , 1999. Sanjeev Arora, Rong Ge, Behnam Neyshabur, and Yi Zhang. Stronger generalization bounds for deep nets via a compression approach. ArXiv , abs/1802.05296, 2018. Sanjeev Arora, Nadav Cohen, Noah Golowich, and Wei Hu. A convergence analysis of gradient descent for deep linear neural networks. ArXiv , abs/1810.02281, 2019a. Sanjeev Arora, Nadav Cohen, Wei Hu, and Yuping Luo. Implicit regularization in deep matrix factorization. In NeurIPS , 2019b. D. Arpit, Stanislaw Jastrzebski, Nicolas Ballas, David Krueger, Emmanuel Bengio, Maxinder S. Kanwal, Tegan Maharaj, Asja Fischer, Aaron C. Courville, Yoshua Bengio, and S. Lacoste-Julien. A closer look at memorization in deep networks. ArXiv , abs/1706.05394, 2017. Peter L. Bartlett, Vitaly Maiorov, and Ron Meir. Almost linear vc-dimension bounds for piecewise polynomial networks. Neural Computation , 10:2159‚Äì2173, 1998. P. P. Brahma, Dapeng Oliver Wu, and Y . She. Why deep learning works: A manifold disentanglement perspective. IEEE Transactions on Neural Networks and Learning Systems , 27:1997‚Äì2008, 2016. Johann Brehmer and Kyle Cranmer. Flows for simultaneous manifold learning and density estimation. ArXiv , abs/2003.13913, 2020. 10Richard P. Brent. An algorithm with guaranteed convergence for Ô¨Ånding a zero of a function. Comput. J., 14:422‚Äì425, 1971. M. Bronstein, Joan Bruna, Y . LeCun, Arthur Szlam, and P. Vandergheynst. Geometric deep learning: Going beyond euclidean data. IEEE Signal Processing Magazine , 34:18‚Äì42, 2017. Michael M. Bronstein, Joan Bruna, Taco Cohen, and Petar Velivckovi‚Äôc. Geometric deep learning: Grids, groups, graphs, geodesics, and gauges. ArXiv , abs/2104.13478, 2021. Sam Buchanan, Dar Gilboa, and John Wright. Deep networks and the multiple manifold problem. ArXiv , abs/2008.11245, 2021. G. Carlsson, T. Ishkhanov, V . D. Silva, and A. Zomorodian. On the local behavior of spaces of natural images. International Journal of Computer Vision , 76:1‚Äì12, 2007. Minshuo Chen, Haoming Jiang, Wenjing Liao, and Tuo Zhao. EfÔ¨Åcient approximation of deep relu networks for functions on low dimensional manifolds. ArXiv , abs/1908.01842, 2019. Alexander Cloninger and Timo Klock. Relu nets adapt to intrinsic dimensionality beyond the target domain. ArXiv , abs/2008.02545, 2020. G. Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of Control, Signals and Systems , 2:303‚Äì314, 1989. Simon Shaolei Du, Wei Hu, and J. Lee. Algorithmic regularization in learning deep homogeneous models: Layers are automatically balanced. In NeurIPS , 2018. C. Fefferman, S. Mitter, and Hariharan Narayanan. Testing the manifold hypothesis. arXiv: Statistics Theory , 2013. Octavian-Eugen Ganea, Gary B ¬¥ecigneul, and Thomas Hofmann. Hyperbolic neural networks. ArXiv , abs/1805.09112, 2018. Sebastian Goldt, Marc M ¬¥ezard, Florent Krzakala, and Lenka Zdeborov ¬¥a. Modelling the inÔ¨Çuence of data structure on learning in neural networks. ArXiv , abs/1909.11500, 2020. I. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, S. Ozair, Aaron C. Courville, and Yoshua Bengio. Generative adversarial nets. In NIPS , 2014. Ian J. Goodfellow and Oriol Vinyals. Qualitatively characterizing neural network optimization problems. CoRR , abs/1412.6544, 2015. Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. CoRR , abs/1412.6572, 2015. Alfred Gray. The volume of a small geodesic ball of a riemannian manifold. Michigan Mathematical Journal , 20:329‚Äì344, 1974. Victor Guillemin and Alan Pollack. Differential Topology . Prentice-Hall, 1974. B. Hanin and M. Nica. Products of many large random matrices and gradients in deep neural networks. Communications in Mathematical Physics , 376:287‚Äì322, 2018. B. Hanin and D. Rolnick. Complexity of linear regions in deep networks. ArXiv , abs/1901.09021, 2019a. B. Hanin and D. Rolnick. Deep relu networks have surprisingly few activation patterns. In NeurIPS , 2019b. Boris Hanin. Universal function approximation by deep neural nets with bounded width and relu activations. ArXiv , abs/1708.02691, 2019. Boris Hanin and Mihai Nica. Finite depth and width corrections to the neural tangent kernel. ArXiv , abs/1909.05989, 2020. M. Hauser and A. Ray. Principles of riemannian geometry in neural networks. In NIPS , 2017. 11Mikael Henaff, Joan Bruna, and Yann LeCun. Deep convolutional networks on graph-structured data. ArXiv , abs/1506.05163, 2015. K. Hornik, M. Stinchcombe, and H. White. Multilayer feedforward networks are universal approxi- mators. Neural Networks , 2:359‚Äì366, 1989. Arthur Jacot, F. Gabriel, and C. Hongler. Neural tangent kernel: Convergence and generalization in neural networks. In NeurIPS , 2018. Tero Karras, S. Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pages 4396‚Äì4405, 2019. Tero Karras, Miika Aittala, Janne Hellsten, S. Laine, J. Lehtinen, and Timo Aila. Training generative adversarial networks with limited data. ArXiv , abs/2006.06676, 2020a. Tero Karras, S. Laine, Miika Aittala, Janne Hellsten, J. Lehtinen, and Timo Aila. Analyzing and improving the image quality of stylegan. 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pages 8107‚Äì8116, 2020b. Kenji Kawaguchi, L. Kaelbling, and Yoshua Bengio. Generalization in deep learning. ArXiv , abs/1710.05468, 2017. Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. CoRR , abs/1412.6980, 2015. Thomas Kipf and Max Welling. Semi-supervised classiÔ¨Åcation with graph convolutional networks. ArXiv , abs/1609.02907, 2017. Dieter Kraft. A software package for sequential quadratic programming. Tech. Rep. DFVLR-FB 88-28, DLR German Aerospace Center ‚Äî Institute for Flight Mechanics , 1988. S. Krantz and Harold R. Parks. Geometric integration theory. In Geometric Integration Theory , 2008. Daniel Kunin, Javier Sagastuy-Bre Àúna, S. Ganguli, Daniel L. K. Yamins, and H. Tanaka. Neu- ral mechanics: Symmetry and broken conservation laws in deep learning dynamics. ArXiv , abs/2012.04728, 2020. Jaehoon Lee, Lechao Xiao, Samuel S. Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl- Dickstein, and Jascha Sohl-Dickstein. Wide neural networks of any depth evolve as linear models under gradient descent. ArXiv , abs/1902.06720, 2019. Tengyuan Liang, Tomaso A. Poggio, Alexander Rakhlin, and James Stokes. Fisher-rao metric, geometry, and complexity of neural networks. ArXiv , abs/1711.01530, 2019. L. Loveridge. Physical and geometric interpretations of the riemann tensor, ricci tensor, and scalar curvature. In Physical and Geometric Interpretations of the Riemann Tensor, Ricci Tensor, and Scalar Curvature , 2004. H. Mhaskar and T. Poggio. Deep vs. shallow networks : An approximation theory perspective. ArXiv , abs/1608.03287, 2016. Federico Monti, D. Boscaini, Jonathan Masci, Emanuele Rodol `a, Jan Svoboda, and Michael M. Bronstein. Geometric deep learning on graphs and manifolds using mixture model cnns. 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , pages 5425‚Äì5434, 2017. Guido Mont ¬¥ufar, Razvan Pascanu, Kyunghyun Cho, and Yoshua Bengio. On the number of linear regions of deep neural networks. In NIPS , 2014. V . Nair and Geoffrey E. Hinton. RectiÔ¨Åed linear units improve restricted boltzmann machines. In ICML , 2010. Behnam Neyshabur, Srinadh Bhojanapalli, David A. McAllester, and Nathan Srebro. A pac-bayesian approach to spectrally-normalized margin bounds for neural networks. ArXiv , abs/1707.09564, 2018. 12Roman Novak, Yasaman Bahri, Daniel A. AbolaÔ¨Åa, Jeffrey Pennington, and Jascha Sohl-Dickstein. Sensitivity and generalization in neural networks: an empirical study. In International Conference on Learning Representations , 2018. URL https://openreview.net/forum?id=HJC2SzZCW . Jonas Paccolat, Leonardo Petrini, Mario Geiger, Kevin Tyloo, and Matthieu Wyart. Geometric compression of invariant manifolds in neural networks. Journal of Statistical Mechanics: Theory and Experiment , 2021, 2020. Dominique Perraul-Joncas and Marina Meila. Non-linear dimensionality reduction: Riemannian metric estimation and the problem of geometric discovery. arXiv: Machine Learning , 2013. Ben Poole, Subhaneil Lahiri, Maithra Raghu, Jascha Sohl-Dickstein, and Surya Ganguli. Exponential expressivity in deep neural networks through transient chaos. In NIPS , 2016. C. Qi, Hao Su, Kaichun Mo, and Leonidas J. Guibas. Pointnet: Deep learning on point sets for 3d classiÔ¨Åcation and segmentation. 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , pages 77‚Äì85, 2017. M. Raghu, Ben Poole, J. Kleinberg, S. Ganguli, and Jascha Sohl-Dickstein. On the expressive power of deep neural networks. ArXiv , abs/1606.05336, 2017. Nasim Rahaman, Aristide Baratin, Devansh Arpit, Felix Dr ¬®axler, Min Lin, Fred A. Hamprecht, Yoshua Bengio, and Aaron C. Courville. On the spectral bias of neural networks. In ICML , 2019. Joel W. Robbin, Uw Madison, and Dietmar A. Salamon. INTRODUCTION TO DIFFERENTIAL GEOMETRY . Preprint, 2011. Levent Sagun, Utku Evci, V . U. G ¬®uney, Yann Dauphin, and L. Bottou. Empirical analysis of the hessian of over-parametrized neural networks. ArXiv , abs/1706.04454, 2018. Andrew M. Saxe, James L. McClelland, and Surya Ganguli. Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. CoRR , abs/1312.6120, 2014. Johannes Schmidt-Hieber. Deep relu network approximation of functions on a manifold. ArXiv , abs/1908.00695, 2019. Thiago Serra, Christian Tjandraatmadja, and S. Ramalingam. Bounding and counting linear regions of deep neural networks. In ICML , 2018. Uri Shaham, Alexander Cloninger, and Ronald R. Coifman. Provable approximation properties for deep neural networks. ArXiv , abs/1509.07385, 2015. Samuel L. Smith and Quoc V . Le. A bayesian perspective on generalization and stochastic gradient descent. ArXiv , abs/1710.06451, 2018. Weijie J. Su, Stephen P. Boyd, and Emmanuel J. Cand `es. A differential equation for modeling nesterov‚Äôs accelerated gradient method: Theory and insights. In J. Mach. Learn. Res. , 2016. Christian Szegedy, W. Zaremba, Ilya Sutskever, Joan Bruna, D. Erhan, Ian J. Goodfellow, and R. Fergus. Intriguing properties of neural networks. CoRR , abs/1312.6199, 2014. Matus Telgarsky. Representation beneÔ¨Åts of deep feedforward networks. ArXiv , abs/1509.08101, 2015. Joshua B. Tenenbaum. Mapping a manifold of perceptual observations. In NIPS , 1997. Pauli Virtanen, Ralf Gommers, Travis E. Oliphant, Matt Haberland, Tyler Reddy, David Cournapeau, Evgeni Burovski, Pearu Peterson, Warren Weckesser, Jonathan Bright, St ¬¥efan J. van der Walt, Matthew Brett, Joshua Wilson, K. Jarrod Millman, Nikolay Mayorov, Andrew R. J. Nelson, Eric Jones, Robert Kern, Eric Larson, C J Carey, ÀôIlhan Polat, Yu Feng, Eric W. Moore, Jake VanderPlas, Denis Laxalde, Josef Perktold, Robert Cimrman, Ian Henriksen, E. A. Quintero, Charles R. Harris, Anne M. Archibald, Ant ÀÜonio H. Ribeiro, Fabian Pedregosa, Paul van Mulbregt, and SciPy 1.0 Contributors. SciPy 1.0: Fundamental Algorithms for ScientiÔ¨Åc Computing in Python. Nature Methods , 17:261‚Äì272, 2020. doi: 10.1038/s41592-019-0686-2. 13Z. Wan. Geometric interpretations of curvature. In GEOMETRIC INTERPRETATIONS OF CURVA- TURE , 2016. Tingran Wang, Sam Buchanan, Dar Gilboa, and John Wright. Deep networks provably classify data on curves. ArXiv , abs/2107.14324, 2021. Yue Wang, Yongbin Sun, Ziwei Liu, Sanjay E. Sarma, Michael M. Bronstein, and Justin M. Solomon. Dynamic graph cnn for learning on point clouds. ACM Transactions on Graphics (TOG) , 38:1 ‚Äì 12, 2019. Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and Philip S. Yu. A comprehensive survey on graph neural networks. IEEE Transactions on Neural Networks and Learning Systems , 32:4‚Äì24, 2019. C. Zhang, S. Bengio, M. Hardt, B. Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization. ArXiv , abs/1611.03530, 2017. Liu Ziyin, Tilman Hartwig, and Masahito Ueda. Neural networks fail to learn periodic functions and how to Ô¨Åx it. ArXiv , abs/2006.08195, 2020. Checklist 1. For all authors... (a)Do the main claims made in the abstract and introduction accurately reÔ¨Çect the paper‚Äôs contributions and scope? [Yes] (b) Did you describe the limitations of your work? [Yes] (c)Did you discuss any potential negative societal impacts of your work? [N/A] Our work is primarily theoretical with few toy experiments we do not see its applicability (d)Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes] 2. If you are including theoretical results... (a)Did you state the full set of assumptions of all theoretical results? [Yes] See Appendix A for a list (b) Did you include complete proofs of all theoretical results? [Yes] 3. If you ran experiments... (a)Did you include the code, data, and instructions needed to reproduce the main experi- mental results (either in the supplemental material or as a URL)? [Yes] See Appendix J (b)Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes] See experimental sections in the Appendix and main body (c)Did you report error bars (e.g., with respect to the random seed after running experi- ments multiple times)? [Yes] Except for the cases where there are multiple graphs that are overlapping (Figure 6,7, 8) because it would make interpreting them difÔ¨Åcult. (d)Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] Appendix J 4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... (a) If your work uses existing assets, did you cite the creators? [Yes] (b) Did you mention the license of the assets? [Yes] (c)Did you include any new assets either in the supplemental material or as a URL? [No] (d)Did you discuss whether and how consent was obtained from people whose data you‚Äôre using/curating? [N/A] (e)Did you discuss whether the data you are using/curating contains personally identiÔ¨Åable information or offensive content? [N/A] 5. If you used crowdsourcing or conducted research with human subjects... 14(a)Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A] (b)Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A] (c)Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A] 15A Assumptions We Ô¨Årst make explicit the assumptions on the distribution of weights and biases. A1: The conditional distribution of any set of biases bz1;:::;bzkgiven all other weights and biases has a density z1;:::;zk(b1;:::;bk)with respect to Lebesgue measure on Rk. A2: The joint distribution of all weights has a density with respect to Lebesgue measure on R#weights. A3: The data manifold Mis smooth. A4: (Only needed for Theorem 3) the diameter of M deÔ¨Åned by dM = supx;y2MdistanceM(x;y)is Ô¨Ånite. A5: (Only needed for Theorem 3) a geodesic ball in manifold Mhas polynomial volume growth of orderm. B Additional Background on Manifolds We provide further background on the theory of manifolds. In this section we Ô¨Årst provide the background, deÔ¨Ånition and an interpretation for the scalar curvature of a manifold at a point. Every smooth manifold is also equipped with a Riemannian metric tensor (or metric tensor in short). Given any two vectors, vandw, in the tangent space of a point xon a manifold M, the metric tensor deÔ¨Ånes a parallel to the dot product in Euclidean spaces. The metric tensor, at a point x, is deÔ¨Åned by the smooth functions gij:M!R;i;j2f1;:::;kg. Where the matrix deÔ¨Åned by Gx= [gij(x)] =2 64g11(x)::: g 1n(x) ......... gn1(x)::: gnn(x)3 75 is symmetric and invertible. The inner product of u;v2TxMis then deÔ¨Åned byhu;viM=uTGxv. the inner product is symmetric, non-degenerate, and bilinear, i.e. hku;viM=khu;viM=hu;kviM; hu+w;viM=hu;viM+hw;viM; hu;viM=hv;uiM: As can be seen, these properties also hold for the Euclidean inner product (with Gx=Ifor allx). Let the inverse of G= [gij(x)]be denoted by [gij(x)]. Building on this deÔ¨Ånition of the metric tensor the Ricci curvature tensor is deÔ¨Åned as Rij= 1 2nX a;b=1@2gij @xa@xb+@2gab @xi@xj @2gib @xj@xa @2gjb @xi@xa gab +nX a;b;c;d =11 2@gac @xi@gbd @xj+@gic @xa@gjd @xb @gic @xa@gjb @xd gabgcd  1 4nX a;b;c;d =1@gjc @xi+@gic @xj @gij @xc gabgcd: For geometric interpretations of the above tensors we refer the reader to the work by Loveridge [2004]. Another quantity, from the theory of manifolds, which we utilise in our proofs and theorems, is scalar curvature (or Ricci curvature). The curvature is a measure how much the volume of a geodisic ball on the manifold M, e.g. S2, deviates from a d 1sphere in the Ô¨Çat space, e.g. R3. The volume on the manifold deviates by an amount proportional to the curvature. We illustrate this idea in Ô¨Ågure 10. We refer the reader to works by Gray [1974] and Wan [2016] for further technical details. Since our main theorems relate to the volume of linear regions the scalar curvature plays an important role. 16(a)  (b) Figure 10: The geodesic circle on S2(blue region in (a)) does not have the same area as the Ô¨Çat circle (b), both of radius . One can imagine cutting the blue top off the sphere‚Äôs surface and trying to ‚ÄúÔ¨Çatten‚Äù it. Such an effort will lead to failure, if the material of the sphere does not ‚Äùstretch‚Äù, since the geodesic ball, on S2, cannot be mapped to a circle in R2in a distance preserving manner. Thus, the area of the two blue regions in (a) and (b) vary. This deviation in the area spanned by the two spheres, despite their radii being the same, is proportional to the scalar curvature. Formally, the scalar curvature of a manifold Mat a pointxwith metric tensor [gij]and Ricci tensor [Rij]is deÔ¨Åned as C=nX i;j=1gijRij: Another important concept is that of Hausdorff measure . Since the volumes are ‚Äúdistorted‚Äù on a manifold it requires careful consideration when deÔ¨Åning a measure and integrating using it on a manifold. The m dimensional Hausdorff measure, of a set S, is deÔ¨Åned as Hm(S):= sup >0infn1X i=1(diamUi)djS[1 i=1Ui;diamUi<o : Next we introduce the deÔ¨Ånition of the differential map that is used in DeÔ¨Ånition 3.1, for the determinant of the Jacobian. The differential map of a smooth function Hfrom a manifold Mto a manifoldSat a pointx2Mis the smooth map dH:TxM!TxSsuch that the tangent vector corresponding to any smooth curve :I!Matx,0(0)2TxM, maps to the tangent vector of HinTH(x)N. This is the analog of the total derivative of ‚Äúvanilla calculus‚Äù. More intuitively, the differential map captures how the function changes along different directions on Nas its input changes along different directions on M, this also has an analog to how rows of the Jacobian matrix are viewed in calculus. In DeÔ¨Ånition 3.1 we use the speciÔ¨Åc case where the function Hmaps from manifoldMto the Euclidean space Rkand the tangent space of a Euclidean space is the Euclidean space itself. Finally, a paralellepiped‚Äôs, PinTxM, mapping via the differential map gives us the points in Rkthat correspond to this set P. C Related Work There have been various approaches to explain the efÔ¨Åcacy of DNNs in approximating arbitrarily complex functions. We brieÔ¨Çy touch upon two such promising approaches. Broadly, the theory of DNNs can be viewed from two lenses: expressive power [Hornik et al., 1989, Bartlett et al., 1998, Poole et al., 2016, Raghu et al., 2017, Kawaguchi et al., 2017, Neyshabur et al., 2018, Hanin, 2019] and learning dynamics [Saxe et al., 2014, Su et al., 2016, Smith and Le, 2018, Jacot et al., 2018, Lee et al., 2019, Arora et al., 2019a,b]. These approaches are not independent of one another but 17complementary. For example, Kawaguchi et al. [2017] argue theoretically how the family of DNNs generalize well despite the large capacity of the function class. Neyshabur et al. [2018] provide PAC-Bayes generalization bounds which are improved upon by Arora et al. [2018]. Hanin [2019] shows that Deep ReLU networks of Ô¨Ånite width can approximate any continuous, convex or smooth functions on a unit cube. These works look at DNNs from the lens of expressive power. More recently, there has been a surge in explaining how various algorithms arrive at these almost accurate function approximations by applying different theoretical models of DNNs. Jacot et al. [2018] provide results for convergence and generalization of DNNs in the inÔ¨Ånite width limit by introducing a the neural tangent kernel (NTK). Hanin and Nica [2020] provide Ô¨Ånite depth and width corrections for the NTK. Another line of work within the learning dynamics literature looks at implicit regularization that emerge from the learning algorithm and over-parametrised DNNs [Arora et al., 2019a,b, Du et al., 2018, Liang et al., 2019]. Researchers have begun to incorporate data geometry into the theoretical analyses of DNNs by applying the assumption that the data lies on a general manifold. First we note the works looking at DNNs from the lens of expressive power combined with the idea of data geometry. Shaham et al. [2015] demonstrate that the size of the neural network depends on the curvature of the data manifold and the complexity of the function, whilst depending weakly on the input data dimension, for their construction of sparsely-connected 4-layer neural networks. Cloninger and Klock [2020] show that their construction of deep ReLU nets achieve near optimal approximation rates which depend only on the intrinsic dimensionality of the data. Chen et al. [2019] exploit the low dimensional structure of data to enhance the function approximation capacity of Deep ReLU networks by means of theoretical guarantees. Schmidt-Hieber [2019] shows that sparsely connected deep ReLU networks can approximate a Holder function on a low dimensional manifold embedded in a high dimensional space. Simultaneously, researchers have incorporated data geometry into the learning dynamics line of work [Goldt et al., 2020, Paccolat et al., 2020, Buchanan et al., 2021, Wang et al., 2021]. Buchanan et al. [2021] apply the NTK model to study how DNNs can separate two curves, representing the data manifolds of two separate classes, on the unit sphere. Goldt et al. [2020] introduce the Hidden Manifold Model for structured data sets to capture the dynamics of two-layer neural networks trained with stochastic gradient descent. Rahaman et al. [2019] provide empirical results on which data manifolds are learned faster. Finally, the work by Novak et al. [2018] comes the closes in studying the number of linear regions on the data manifold. They study the change in input output Jacobian, and as a consequence the number of linear regions, for DNNs with piece-wise linearities. They provide empirical studies by counting the number of linear regions along lines connecting data points as a proxy for number of linear regions on the data manifold. Our work Ô¨Åts into the study of expressive power of DNNs. The number of linear regions is a good proxy for the practical expressive power or approximation capacity of Deep ReLU networks [Mont ¬¥ufar et al., 2014]. The results surrounding the density of linear regions make the fewest simplifying assumptions both on the data and the architecture of the DNN. The results by Hanin and Rolnick [2019a] bound the number of linear regions orders of magnitude tighter than previous results by deriving bounds for the average case and not the worst case. Moreover, they demonstrate the validity empirically in a setting with very few simplifying assumptions. We introduce the manifold hypothesis to this setting in order to obtain tighter bounds for the Ô¨Årst time. This introduces a toolbox of ideas from differential geometry to analyse the approximation capacity of deep ReLU networks. In addition to the theoretical works listed above, there has been signiÔ¨Åcant empirical work that applies DNNs to non-Euclidean data [Bronstein et al., 2017, 2021]. Here the data is assumed to be sampled from manifolds with certain geometric properties. For example, Ganea et al. [2018] design DNNs for data sampled from Hyperbolic spaces of arbitrary dimensionality and modify the forward and backward passes accordingly. There have been numerous applications of modiÔ¨Åed DNNs, namely graph convolutional networks, to graph data that incorporate the idea that graphs are discrete samples from a smooth manifold [Henaff et al., 2015, Monti et al., 2017, Kipf and Welling, 2017], see the survey by Wu et al. [2019] for a comprehensive review. Graph convolutional networks have also been applied to point cloud data for applications in graphics [Qi et al., 2017, Wang et al., 2019]. D Proof Sketch In this section we provide an overview of how the three main theorems are proved. Theorem 3.2 provides an equality for measuring the volume of m kdimensional boundary regions on the 18manifold. To this effect, we introduce the idea of viewing boundary regions as submanifolds on the data manifold instead of hyperplanes (Proposition 6). We then prove an equality between the volume of boundary regions and the Jacobian of the neurons over the manifold. We utilise the smooth coarea formula that, intuitively, is applied to integrate a function using level sets on a manifold. This completes the proof for Theorem 3.2. To prove Theorem 3.3 we Ô¨Årst prove that the Jacobian of a function on a manifold can be denoted using the volume of paralellepiped of vectors in the ambient space subject to a linear transform (Proposition 8). Using this result and combining it with Theorem 3.2 we can then give an inequality for the density of linear regions. As can be expected this volume depends on the aforementioned projection, which in turn is related to the geometry of the manifold. Finally, for proving Theorem 3.4 we Ô¨Årst provide an inequality over the tubular neighbourhood of the boundary region. We then use this result to lower bound the geodesic distance between the boundary region and any random point on the manifold. The proof strategy follows that of Hanin and Rolnick [2019a] but there are major deviations when it comes to accounting for the geometry of the data manifold. To the best of our knowledge, we are utilising elements of differential topology that are unique to machine learning when it comes to developing a theoretical understanding of DNNs. E Proof of Theorem 3.2 We follow the proof strategy used by Hanin and Rolnick [2019a] but deviate from it to account for our setting where x2M. LetSzbe the set of values at which the neuron zhas a discontinuity in the differential of its output (or the neuron switches between the two linear regions of the piece-wise linear activation ), Sz:=fx2Rninjz(x) bz= 0g: We also have O:=n x2Rninj8j= 1;:::;L9neuronzwithl(z) =js.t.0(z(x) bz)6= 0o : Further, fSz:=Sz\O: We state propositions 9 and 10 by Hanin and Rolnick [2019a] as we apply them to prove Theorem 3.2, relabeling them as needed. Proposition E.1. (Proposition 9 by Hanin and Rolnick [2019a]) Under assumptions A1 and A2, we have, with probability 1, BF=[ neurons zfSz: By extending the notion of Szto multiple neurons we have eSz1;:::;zk:=k\ j=1eSzj; meaning that the set eSz1;:::;zkis, intuitively, the collection of inputs in Rinwhere the neurons zj;j= 1;:::;k; switch between linear regions for and at which the output of Fis affected by the outputs of these neurons. We refer the reader to section B of the appendix in the work by Hanin and Rolnick [2019a] for an intuitive explanation of proposition E.1. Before proceeding we provide a formal deÔ¨Ånition and intuition for the set BF;k, BF;k=fxjx2BFnfBF;0[:::[BF;k 1g=BF; kand for any ball of radius >0; B(x;)\BF; kis subset to a n kdimensional hyperplane g: Following the explanation provided by Hanin and Rolnick [2019a], BF;kis thenin kdimensional piece ofBF. Suppose the boundaries of linear regions for nin= 2are unions of polygon boundaries, as depicted in Figure 2 of the main body of the paper, then BF;1are all the open line segments of these polygons and BF;2are the end points. Next we state Proposition 10 by Hanin and Rolnick [2019a]. 19Proposition E.2. (Prosposition 10 by Hanin and Rolnick [2019a]) Fixk= 1;:::;n in, andkdistinct neuronsz1;:::;zkinF. Then, with probability 1, for every x2BF;kthere exists a neighbourhood in whichBF;kcoincides with a nin k dimensional hyperplane. We now present Proposition E.4, and its proof, which incorporates the additional constraint that x2M, which is an m-dimensional manifold in Rnin. To prove the proposition we need the deÔ¨Ånition of tranversal intersection of two manifolds [Guillemin and Pollack, 1974]. DeÔ¨Ånition E.3. Two submanifolds, M1andM2, ofSare said to intersect transversally if at every point of intersection their tangent spaces, at that point, together generate the tangent space of the manifold,S, by means of linear combinations. Formally, for all x2M1\M2 TxS=TxM1+TxM2; if and only if M1andM2intersect transversally. For example, given a 2D hyperplane, P, and the surface of a 3D sphere, S2, intersect in the ambient spaceR3. We have that this intersection is transverse if and only if Pis not tangent to S2. For the case where a 2D hyperplane, P, intersects with S2at a pointpbut does not intersect tranversally it coincides exactly with the tangent plane of S2at pointfpg=S2\P, i.e.TpS=P. Note that in either case the tangent space of the 2D hyperplane Pat any point of intersection is the plane itself. Proposition E.4. Fixk= 1;:::;m andkdistinct neurons z1;:::;zkinF. Then, with probability 1, for every x2BF;k\Mthere exists a neighbourhood in which BF;kcoincides with an m k dimensional submanifold in Rin. Proof. From Proposition E.2 we already know that BF;kis anin k-dimensional hyperplane in some neighbourhood of x, with probability 1, for any x2BF;k\M. Let this hyperplane be denoted byPk. This is an n kdimensional submanifold of Rnin. The tangent space of this hyperplane atxis the hyperplane itself. Therefore, from assumptions A1 and A2 we have that the probability that this hyperplane intersects the manifold Mtransversally with probability 1. In other words the probability that this plane Pkcontains or is contained in TxMis0. Finally, we have the intersection, M\Hk, has dimension dim(M) + dim(Hk) nin[Guillemin and Pollack, 1974], which is equal tom k. One implication of Proposition E.4 is that for any kmthem (k+ 1) dimensional volume of BF;k\Mis 0. In addition to that, Proposition E.4 implies that, with probability 1, volm k(BF;k) =X distinct neurons z1;:::;zkvolm k(eSz1;:::;zk\M): (4) The Ô¨Ånal step in the proof of Theorem 3.2 is to prove the following result. Proposition E.5. Letz1;:::;zkbe distinct neurons in Fandkm. Then for a bounded m Hausdorff measurable manifold Membedded in Rnin, Eh volm k eSz1;:::;zk\Mi =Z MEh Yz1;:::;zk(x)i dx; whereYz1;:::;zk(x)equals JM m;H k(x)b1;:::;bk(z1(x);:::;zk(x)); times the indicator function of the event that zj, forj= 1;:::;k , is good at xfor everyjand Hk:Rnin!Rkis such thatHk(x) = [z1(x);:::;zk(x)]T. The expectation is over the distribution of weights and biases. Proof. Letz1;:::;zkbe distinct neurons in FandMbe anm dimensional compact Haudorff measurable manifold. We seek to compute the mean of volm k(eSz1;:::;zk\M)over the distribution of weights and biases. We can rewrite this expression as Z Sz1;:::;zk\M1zjis good atxdvolm k(x): (5) 20The mapHkis Lipschitz and C1almost everywhere. We Ô¨Årst note the smooth coarea formula (theorem 5.3.9 by Krantz and Parks [2008]) in context of our notation. Suppose mkand Hk:Rnin!RkisC1andMRninis anm dimensional C1manifold in Rnin, then Z Mg(x)JM k;Hk(x)dvolm(x) =Z RkZ M\H 1 k(y)g(y)dvolm k(y)dvolk(x); (6) for everyHm-measurable function gwhereJM k;Hkis as deÔ¨Åned in DeÔ¨Ånition 3.1. We denote preactivations and biases of neurons as z(x) = [z1(x);:::;zk(x)]Tandbz= [bz1;:::;bzk]T. From the notation in A1, we have that bz=bz1;:::;bzk; is the joint conditional density of bz1;:::;bzkgiven all other weights and biases. The mean of the term in equation 5 over the conditional distribution of bz1;:::;bzk,bz, is therefore Z Rkbdvolk(b)Z fz=bg\M1zjis good atxdvolm k(x); (7) where we denote [b1;:::;bk]Tasb. Thus applying the smooth co-area formula (Equation 6) to the expression in 7 shows that the average 5 is equal to Z MYz1;:::;zk(x)dx: Finally, we take the average over the remaining weights and biases and commute the expectation with thedxintegral. We can do this since the integrand is non-negative. This gives us the result: Eh volm k eSz1;:::;zk\Mi =Z MEh Yz1;:::;zk(x)i dx; (8) as required. Finally, taking the summation over all possible sets of distinct neurons z1;:::;zkand combining equation 4 with Proposition E.5 completes the proof for Theorem 3.2. F Proof of Theorem 3.3 To prove the upper bound in Theorem 3.3 we Ô¨Årst show that the (determinant of) Jacobian for the functionHk:M!Rk,Hk(x) = [z1(x);:::;zk(x)]T, as deÔ¨Åned in 3.1 is equal to the volume of the parallelopiped deÔ¨Åned by the vectors Hk(rzj(x)), forj= 1;:::;k , whereHk:Rk!TxMis an orthogonal projection onto the orthogonal complement of the kernel of the differential DMHk. Intuitively, this shows that with the added assumption x2Min Theorem 3.3 how exactly we can incorporate the geometry of the data manifold Minto the upper bound provided by Hanin and Rolnick [2019a] in corollary 7. Proposition F.1. GivenHk:M!Rksuch thatHk(x) = [z1(x);:::;zk(x)]Tand the differential DMHkis surjective at xthen JM k;Hk(x) =p det(Gram (Hk(rz1(x));:::;Hk(rzk(x)))); (9) whereHk:Rn!Rkis a linear map and Gram denotes the Gramian matrix. Proof. We Ô¨Årst deÔ¨Åne the orthogonal complement of the kernel of the differential DMHk. For a manifoldMRnand a Ô¨Åxed point xwe have that TxMis am dimensional hyperplane. If we choose an orthonormal basis e1;:::;enofRnsuch thate1;:::;emspansTxMfor a Ô¨Åxedxwe can denote all vectors in TxMusingmcoordinates corresponding to this basis. Therefore, for any vectory2Rkwe can get the orthogonal projection of yontoTxMusing amnmatrix which we denote asPx, wherePxy(matrix multiplied by a vector) represents a vector in TxMcorresponding to the basis e1;:::;em. For any manifold MinRnand function Hk:M!Rkwe have that DMHk:TxM!Rkat a Ô¨Åxed point xis linear function. Therefore we can write DMHk(v) =Av 21wherev2TxMis denoted using the aforementioned basis of TxM. This implies that Ais akm matrix. Therefore, the kernel of DMHkfor a Ô¨Åxed point x2Mis ker(DMHk) =n zjAz= 0andz2TxMo : Since we can create a canonical basis for the space ker(DMHk)starting from the basis e1;:::;emin Rnusing the Gram-Schmidt process given the matrix Awe have that for any y2Rnwe can project it orthogonally onto ker(DMHk). The orthogonal complement of ker(DMHk)is therefore deÔ¨Åned by ker(DMHk)?=n ajaz= 0for allz2ker(DMHk)anda2TxMo : Similar to the previous argument, we construct a canonical basis starting from e1;:::;emfor ker(DMHk)?and therefore we can denote the orthogonal projection onto ker(DMHk)?as a linear transformation. We denote this linear projection for Ô¨Åxed xusingk. We denote the basis vectors e1;::::;emas amnmatrixEwhere each row icorresponds to the vectorei. Therefore, the orthogonal projection of any vector y2RnisEy. Now we can get the matrixAusingErzj(x)corresponding to each row jforj= 1;:::;m . This uses the fact that the direction of steepest ascent on zj(x)restricted to the tangent space TxMof the manifold Mis an orthogonal projection of the direction of steepest ascent in Rn. Finally, from lemma 5.3.5 by Guillemin and Pollack [1974] we have that JM k;Hk(x) =Hk(DMHk(P))=Hk(P); for any parallelepiped Pcontained in (ker(DMHk))?. Arguing similar to the proof of lemma 5.3.5 by Guillemin and Pollack [1974] we get that JM k;Hk(x) =q det((A)TA) =p detGram (Erz1(x);:::;Erzk(x)); thereby showing that Hk(y) =Eyis a linear mapping. Although we state Proposition F.1 for neurons zj(x);j= 1;:::;k in the proof, it applies to any function that satisfy the conditions laid out in the proposition. Equipped with Proposition F.1 we prove Theorem 3.3. When the weights and biases of Fare independent obtain an upper bound on bz1;:::;bzk(b1;:::;bk)as k j=1bzj(b1;:::;bk) sup neuronszbz(b)k =Ck bias: Hence, Yz1;:::;zkCk biasJM k;Hk: From Proposition 9 we have that JM k;Hkis equal to the k-dimensional volume of the paralellopiped spanned byx(rzj(x))forj= 1;:::;k . Therefore, we have JM k;Hkk j=1jjErzj(x)jjjjEjjkk j=1jjrzj(x)jj; (10) wherejjEjjdenotes the matrix norm which is deÔ¨Åned as jjEjj= supn jjEyjjy2Rk;jjyjj= 1o : Note thatEdoes not depend on F(orz1;:::;zk) but only on TxMor more generally the geometry of Mat any point x. From Theorem 3.2 by Hanin and Nica [2018] we have, for any Ô¨Åxed x, Eh k j=1jjrzj(x)jji  Cgradk ; (11) where, Cgrad= sup zsup x2RninE[jjrz(x)jj2k]1=kCeCPd j=11 nj; 22(a)  (b) Figure 11: We illustrate how vectors project differently on tangent planes of two different manifolds: circle (a) and tractrix (b). In case of the tractrix the tangents (and the projection of vectors onto them) are on the inside of the tractrix whereas for the sphere the tangents are always on the outside of the sphere. Since the projections of vectors onto the tangent space are an essential aspect of our proof we end up with the term CM, which quantiÔ¨Åes the ‚Äúshrinking‚Äù of these vectors upon projection, in the inequalities for Theorems 3.3 and 3.4. whereinC > 0depends only on and not on the architecture of Fandnjis the width of the hidden layerj. LetCMbe deÔ¨Åned as CM:= supn Cjthere exists a set, S, of non zero m k-dimensional Hausdorff measure such thatjjExjjC8x2So Therefore, combining equations 11, 10 and result from Theorem 3.2 we have E[volm k(BF;k\M)] volm(M) number of neurons k (2CgradCbiasCM)k; where the expectation is over the distribution of weights and biases. G Proof of Theorem 3.4 We Ô¨Årst prove the following proposition Proposition G.1. For a compact m-dimensional submanifold MinRn,m;n1andm<n let SRnbe a compact Ô¨Åxed continuous piece-wise linear submanifold with Ô¨Ånitely many pieces and given anyU > 0. LetS0=;and letSkbe the union of the interiors of all k-dimensional pieces of Sn(S0[:::[Sk 1). Denote by Tthe-tubuluar neighbourhood of any XMsuch that T(X) =n yjdM(y;X)<andy2Mo ; where2(0;U),dMis the geodesic distance between the point yand setXon the manifold M, we have volm(T(S))dX k=n mvolk(Sk\M)!n kn kCk;;U; whereCk;;U>0is a constant that depends on the average scalar curvature (Sk\M)?andU, and !n kis the volume of the unit ball in Rn k. Proof. DeÔ¨Ånedto be the maximal dimension of linear pieces in S. Letx2T(X\M). Suppose x =2T(X\M)for allk=n m;:::;d 1. Then the intersection of a geodesic ball of radius  aroundswithSis a ball inside Sd\M. Using the convexity of this ball, with respect to the manifold M[Robbin et al., 2011], there exists a point yinSd\Msuch that the geodesic : [0;1]!Mwith 23(0) =yand(1) =xis perpendicular to Sd\Maty. Formally,TSd\MMatyis perpendicular to_(0)2TMaty. LetB(N(Sd\M))be the union of all the balls along the Ô¨Åber of the submanifold Sd\M. Therefore, we have volm(T(S\M)volm(B(N(Sd\M)) + volm(T(Sd 1\M)); (12) whereSd 1:=[d 1 k=0Sk. We also note that volm(B(N(Sd\M)) = volm+d n(Sd\M)voln d(B((M\Sd)?)); whereB((M\Sd)?)is the average volume of an ball in the submanifold of Morthogonal toM\Sd. This volume depends on the average scalar curvature, (M\Sd)?of the submanifold (M\Sd)?. As shown by Wan [2016], for a Ô¨Åxed point x2(M\Sd)? voln d(B(x;(M\Sd)?)) =!n dn d 1 (x)(M\Sd)? n d+ 22+O(4) ; where!n dis the volume of the unit ball of dimension n d,B(x;(M\Sd)?)is the geodesic ball of radiusin the manifold (M\Sd)?centered atxand(M\Sd)?(x)denotes the scalar curvature at pointx. Gray [1974] provides the second order expansion of the formula above. Given that 2(0;U), for allk2fn m;n m+ 1;:::;dg, then we have a smallest Ck;;U such that volk(B(x;(M\Sk)?))Ck;;Uk: (13) The above inequality follows from assumption A5. Using the above inequalities 12, 13 and repeating the argument d 1 n+mtimes we get the result of the proposition. We also note that Ck;;U increases monotonically with U, this also follows from the volume being monotonically increasing and positive for >0. Finally, we can now prove Theorem 3.4. Let x2M be uniformly chosen. Then, for all 2(0;U), using Markov‚Äôs inequality and Proposition G.1, we have E[distanceM(x;Bf\M)]Pr(distanceM(x;BF\M)>) =(1 Pr(distanceM(x;BF\M)<=)) (1 ninX k=nin mvolk(Sk\M)!n knin kCnin k;;U (1 ninX k=nin mCnin k;;U(CgradCbiasCMf#neuronsg)k : Note that as we increase Uthe constants Cn k;;U increase, although not strictly, for all k. To Ô¨Ånd the supremum of the expression on the right hand side, of the last inequality, in 2(0;U)we multiply and divide the expression by CgradCbiasCM#neurons to get the polynomial pU() = 1 ninX k=nin mCnin k;;Uk ; where=CgradCbiasCM#neurons and2(0;U0)whereU0=UC gradCbiasCM#neurons . Let dMbe the diameter of the manifold M, deÔ¨Åned by dM= supx;y2MdistanceM(x;y). We assume thatdMis Ô¨Ånite. Taking the supremum over all U2(0;dM]orU02(0;d0 M], whered0 M= dMCgradCbiasCM#neurons, gives us the constant CM; CM;= sup U02(0;d0 M]fsup 2(0;U0)fpU()gg: SincedMis Ô¨Ånite the constant above exists and is Ô¨Ånite. We make a note on the existence of this constantCM;in the absence of the constraint that the diameter of manifold Mis Ô¨Ånite. As U increases the constants Cnin k;;U also increase and are all positive. The solution for p0 U() = 0; > 0, which we denote by U, is unique and keeps decreasing as Uincreases. The uniqueness of the solution follows from the fact that the coefÔ¨Åcients Cnin k;;U are all positive. We also note 24Figure 12: We plot the optima for a simpliÔ¨Åed polynomial as described in Section G.1. The in- dividual plots correspond to ninincreasing from nin= 2tonin= 30 (left to right) with mvarying from 1tonin 1on the x-axis. Figure 13: We plot the optima for a simpliÔ¨Åed polynomial as described in Section G.1. The in- dividual plots correspond to mincreasing from m= 1tom= 29 (left to right) with ninvarying fromm+ 1to30on the x-axis. thatpU(U)need not be equal to sup2(0;U0)fpU()gbecauseUneed not lie in (0;U0). In all such cases sup2(0;U0)fpU()g=pU(U0). Given the polynomial pU()above if we can assert that there exists a CU, and the corresponding CU0, such that for all U > CU, and corresponding U0> CU0, we have sup2(0;U0)fpU()g=pU(U)<1and for all 0< UCUwe have sup2(0;U0)fpU()g=pU(U0)<1. Therefore, CM;exists and is Ô¨Ånite if the previous assertion holds, proving this assertion is beyond the scope of our current work and particularly challenging. Finally, taking the average over distribution of weights gives us the inequality E[distanceM(x;Bf\M)]CM; CgradCbiasCM#neurons; whereCM;is a constant which depends on the average scalar curvature of the manifold M. This completes the proof of Theorem 3.4. G.1 Variations in Supremum We illustrate the dependence of the the constant CM;on varying values of nin;musing a simple example. We Ô¨Åx the coefÔ¨Åcient of the polynomial p()to be all 1, this not always the case but we do so to illustrate the relationship between the optima and the exponents for simplest such polynomial: psimpliÔ¨Åed () = 1 ninX k=nin mk We plot the supremums of this simpliÔ¨Åed polcynomial CsimpliÔ¨Åed = sup2(0;1)psimpliÔ¨Åed ()for each ninfrom thef2;:::;30gand varying min Figure 12. Similarly, we vary ninwith Ô¨Åxedmand report the supremums CsimpliÔ¨Åed in Figure 13. We notice that for a Ô¨Åxed ninthe supremum decreases with m and for a Ô¨Åxed mthe supremum increases with nin. We programatically calculate the supremum being reported by restricting the domain of psimpliÔ¨Åed to(0;1). We solve for the supremum by using the fminbound method from the scipy package [Virtanen et al., 2020]. The function uses Brent‚Äôs method [Brent, 1971] to Ô¨Ånd the supremum. H Toy Supervised Learning Problems For the two supervised learning tasks with different geometries (tractrix and sphere), we uniformly sample 1000 data points from each 1D manifold to come up with samples of (xi;yi)pairs. We then add Gaussian noise to y. We train a DNN with 2 hidden layers, with 10 and 16 neurons in each layer and a single linear output neuron, for a total of 26 neurons with piece-wise linearity, using the 25Figure 14: The test errors for the cases where data is sampled from the tractrix (blue) and the circle (green). We see that the tractrix converges slower but the magnitude of the errors remains comparable as training progresses across the two manifolds. PyTorch library. The optimization is performed using the Adam optimizer [Kingma and Ba, 2015] with a learning rate of 0.01. We ensure a reasonable Ô¨Åt of the model by reducing the test time mean squared error (see Figure 14). We then calculate the exact number of linear regions on the respective domains by Ô¨Ånding the points where z(x(t)) =bzfor every neuron zandxis on the 1D manifold. We do this by adding neurons, z, one by one at every layer and using the SLSQP [Kraft, 1988] to solve forjz(x(t)) bzj= 0 intfor tractrix andjz(x()) bzj= 0 infor the circle. Note that this methodology can be extended to solve for linear regions of a deep ReLU network for any 1D curvex(:)in any dimension. We then split a linear region depending on where this solution lies compared to previous layers. For every epoch, we then uniformly randomly sample points from the 1D manifold, by sampling directly from andt, to measure average distance to the nearest linear boundaries. The experiment was run on CPUs, from training to counting of number of linear regions. The intel cpus had access to 4 GB memory per core. A total of, approximately, 24 cpu hours were required for all the experiments in this section. This was run on an on demand cloud instance. All implementations are in PyTorch, except for SLQSP for which we used sklearn. H.1 Varying Input Dimensions The experimental setup, hyperparameters, network architecture, target function and methods are all the same as described for the toy supervised learning problem for the case where the geometry is a sphere. The only difference is that the input dimension varies, nin. I High Dimensional Dataset We utilise the ofÔ¨Åcial implementation of pretrained StyleGAN generator to generate curves of images that lie on the manifold of face images. SpeciÔ¨Åcally, for each curve we sample a random pair of latent vectors: z1;z22Rk, this gives us the start and end point of the curve using the generatorg(z1)andg(z2). We then generate 100 images to approximate a curve connecting the two images on the image manifold in a piece-wise manner. We do so by taking 100 points on the line connectingz1andz2in the latent space that are evenly spaced and generate an image from each one of them. Therefore, the ithimage is generated as: xi=g(((100 i)z1+iz2)=100) , using the StyleGAN generator g. We qualitatively verify the images to ensure that they lie on the manifold of images of faces. 4 examples of these curves, sampled as above, are illustrated in the video here: https://drive.google.com/Ô¨Åle/d/1p9B8ATVQGQYoiMh3Q22D-jSaI0USsoNx/view?usp=sharing. These two constructions allow us to formulate two curves in the high-dimensional setting. The straight line, with two Ô¨Åxed points g(z1)andg(z2), is deÔ¨Åned as x(t) = (1 t)g(z1) +tg(z2)with t2[0;1]. The approximated curve on the manifold is deÔ¨Åned as x0(t) = (1 t)g(zi) +tg(zi+1) 26(a) LR: 0.025, momentum: 0.5, BS: 64  (b) LR: 0.005, momentum: 0.75, BS: 64 (c) LR: 0.01, momentum: 0.75, BS: 128 Figure 15: We report the log density of linear regions for various hyperparameters. Lr refers to the learning rate and BS is the batch size. wherei=floor (100t). This once again gives us two curves and we solve for the zeros of jz(x(t)) bzj= 0andjz(x0(t)) bzj= 0fort2[0;1]using SLQSP as described in Appendix H. The neural network, used for classiÔ¨Åcation in our MetFaces experiment, is feed forward with ReLU activation. There are two hidden layers with 256 and 64 neurons in the Ô¨Årst and second layers respectively. We downsample the images to 1281283. We augment the dataset using random horizontal Ô¨Çips of the images. All inputs are normalized. We use a batch size of 32. The neural network is trained using SGD. The learning rate is 0.01 and the momentum is 0.5. The total time required, for these experiments on MetFaces dataset, was approximately 36 GPU hours on a Titan RTX GPU that has 24 GB memory. This was run on an on demand cloud instance. We chose hyperparameters by trial and error, targeting a better Ô¨Åt for the training data for the results reported in Figure 9 of the main body of the paper. We report further results for density of linear regions with varying hyperparameters in Figure 15. We also report the training and testing accuracy for the various sets of hyperparameters in Figure 16. Note that Figure 16(a) corresponds to the test and train accuracies on MetFaces reported in the main body of the paper (Figure 9). Note all of these results are for the same architecture as described above. 27(a) LR: 0.01, momentum: 0.5, BS: 32  (b) LR: 0.025, momentum: 0.5, BS: 64 (c) LR: 0.005, momentum: 0.75, BS: 64  (d) LR: 0.01, momentum: 0.75, BS: 128 Figure 16: We report the test and train accuracies across 5 random seeds above. 28"
9,https://arxiv.org,2301.00009,10.48550/arXiv.2301.00009,"Saket Tiwari, Omer Gottesman, George Konidaris",On the Geometry of Reinforcement Learning in Continuous State and Action Spaces,"Advances in reinforcement learning have led to its successful application in complex tasks with continuous state and action spaces. Despite these advances in practice, most theoretical work pertains to finite state and action spaces. We propose building a theoretical understanding of continuous state and action spaces by employing a geometric lens. Central to our work is the idea that the transition dynamics induce a low dimensional manifold of reachable states embedded in the high-dimensional nominal state space. We prove that, under certain conditions, the dimensionality of this manifold is at most the dimensionality of the action space plus one. This is the first result of its kind, linking the geometry of the state space to the dimensionality of the action space. We empirically corroborate this upper bound for four MuJoCo environments. We further demonstrate the applicability of our result by learning a policy in this low dimensional representation. To do so we introduce an algorithm that learns a mapping to a low dimensional representation, as a narrow hidden layer of a deep neural network, in tandem with the policy using DDPG. Our experiments show that a policy learnt this way perform on par or better for four MuJoCo control suite tasks.","ON THE GEOMETRY OF REINFORCEMENT LEARNING INCONTINUOUS STATE AND ACTION SPACES Saket Tiwari Department of Computer Science Brown University Providence, RI 02906 saket_tiwari@brown.eduOmer Gottesman Department of Computer Science Brown University Providence, RI 02906 George Konidaris Department of Computer Science Brown University Providence, RI 02906 ABSTRACT Advances in reinforcement learning have led to its successful application in com- plex tasks with continuous state and action spaces. Despite these advances in practice, most theoretical work pertains to Ô¨Ånite state and action spaces. We pro- pose building a theoretical understanding of continuous state and action spaces by employing a geometric lens. Central to our work is the idea that the transition dynamics induce a low dimensional manifold of reachable states embedded in the high-dimensional nominal state space. We prove that, under certain conditions, the dimensionality of this manifold is at most the dimensionality of the action space plus one. This is the Ô¨Årst result of its kind, linking the geometry of the state space to the dimensionality of the action space. We empirically corroborate this upper bound for four MuJoCo environments. We further demonstrate the applicability of our result by learning a policy in this low dimensional representation. To do so we introduce an algorithm that learns a mapping to a low dimensional representation, as a narrow hidden layer of a deep neural network, in tandem with the policy using DDPG. Our experiments show that a policy learnt this way perform on par or better for four MuJoCo control suite tasks. 1 I NTRODUCTION The goal of a reinforcement learning (RL) agent is to learn an optimal policy that maximises the return which is the time discounted cumulative reward (Sutton & Barto, 1998). Recent advances in RL research have lead to agents successfully learning in environments with enormous state spaces, such as games (Mnih et al., 2015; Silver et al., 2016), and robotic control in simulation (Lillicrap et al., 2016; Schulman et al., 2015; 2017a) and real environments (Levine et al., 2016; Zhu et al., 2020; Deisenroth & Rasmussen, 2011). However, we do not have an understanding of the intrinsic complexity of these seemingly large problems. For example, in most popular deep RL algorithms for continuous control, the agent‚Äôs policy is parameterised by a a deep neural network (DNN) (Lillicrap et al., 2016; Schulman et al., 2015; 2017a) but we do not have theoretical models to guide the design of DNN architecture required to efÔ¨Åciently learn an optimal policy for various environments. There have been approaches to measure the difÔ¨Åculty of an RL environment from a sample complexity perspective (Antos et al., 2007; Munos & Szepesvari, 2008; Bastani, 2020) but these models fall short of providing recommendations for the policy and value function complexity required to learn an optimal policy. We view the complexity of RL environments through a geometric lens. We build on the intuition behind the manifold hypothesis , which states that most high-dimensional real-world datasets actually lie on low-dimensional manifolds (Tenenbaum, 1997; Carlsson et al., 2007; Fefferman et al., 2013; Bronstein et al., 2021); for example, the set of natural images are a very small, smoothly-varying 1arXiv:2301.00009v1  [cs.LG]  29 Dec 2022subset of all possible value assignments for the pixels. A promising geometric approach is to model the data as a low-dimensional structure‚Äîa manifold ‚Äîembedded in a high-dimensional ambient space. In supervised learning, especially deep learning theory, researchers have shown that the approximation error depends strongly on the dimensionality of the manifold (Shaham et al., 2015; Pai et al., 2019; Chen et al., 2019; Cloninger & Klock, 2020), thereby connecting the complexity of the underlying structure of the dataset to the complexity of the DNN. As in supervised learning, researchers have applied the manifold hypothesis in RL‚Äîi.e. hypothesized that the effective state space lies on a low dimensional manifold (Mahadevan, 2005; Machado et al., 2017; 2018; Banijamali et al., 2018; Wu et al., 2019; Liu et al., 2021). Despite its fruitful applications, this assumption‚Äîof a low-dimensional underlying structure‚Äîhas never been theoretically and empirically validated in any RL setting. Our main result provides a general proof of this hypothesis for allcontinuous state and action RL environments by proving that the effective state space is a manifold and upper bound its dimensionality by, simply, the dimensionality of the action space plus one. Although our theoretical results are for deterministic environments with continuous states and actions, we empirically corroborate this upper bound on four MuJoCo environments (Todorov et al., 2012), with sensor inputs, by applying the dimensionality estimation algorithm by Facco et al. (2017). Our empirical results suggest that in many instances the bound on the dimensionality of the effective state manifold is tight. To show the applicability and relevance of our theoretical result we empirically demonstrate that a policy can be learned using this low-dimensional representation that performs as well as or better than a policy learnt using the higher dimensional representation. We present an algorithm that does two things simultaneously: 1)learns a mapping to a low dimensional representation, called the co-ordinate chart , parameterised by a DNN, and 2)uses this low-dimensional mapping to learn the policy. Our algorithm extends DDPG (Lillicrap et al., 2016) and uses it as a baseline with a higher-dimensional representation as the input. We empirically show a surprising new DNN architecture with a bottleneck hidden layer of width equal to dimensionality of action space plus one performs on par or better than the wide architecture used by Lillicrap et al. (2016). These results demonstrate that our theoretical results, which speaks to the underlying geometry of the problem, can be applied to learn a low dimensional or compressed representation for learning in a data efÔ¨Åcient manner. Moreover, we connect DNN architectures to effectively learning a policy based on the underlying geometry of the environment. 2 B ACKGROUND AND MATHEMATICAL PRELIMINARIES We Ô¨Årst describe the continuous time RL model and Markov decision process (MDP). This forms the foundation upon which our theoretical result is based. Then we provide mathematical background on various ideas from the theory of manifolds that we employ in our proofs and empirical results. 2.1 C ONTINUOUS -TIMEREINFORCEMENT LEARNING We analyze the setting of continuous-time reinforcement learning in a deterministic Markov decision process (MDP) which is deÔ¨Åned by the tuple M= (S;A;f;fr;s0;)over timet2[0;T).SRds is the set of all possible states of the environment. ARdais the rectangular set of actions available to the agent. f:SA R+!S andf2C1is asmooth function that determines the state transitions: s0=f(s;a; )is the state the agent transitions to when it takes the action aat states for the time period . Note thatf(s;a;0) =s, meaning that the agent‚Äôs state remains unchanged if an action is applied for a duration of = 0. The reward obtained for reaching state sisfr(s), determined by the reward function fr:S!R.stdenotes the state the agent is at time tandatis the action it takes at time t.s0is the Ô¨Åxed initial state of the agent at t= 0, and the MDP terminates att=T. The agent does not have access to the functions fandfr, and can only observe states and rewards at a given time t2[0;T). The agent is equipped with a policy, :S!A , that determines its decision making process. We denote the set of all the possible policies by . Simply put, the agent takes action (s)at state s. The goal of the agent is to maximise the discounted return J() =RT 0e l fr(sl)dl, where st+=f(st;(st);)for inÔ¨Ånitesimally small and allt2[0;T). We deÔ¨Åne the action tangent 2mapping ,g:SA! Rds, for an MDP as g(s;a) = lim !0+f(s;a; ) s =@f(s;a; ) @: Intuitively, this captures the direction in which the agents state changes at state supon taking an action a. For notational convenience we will denote g(s;(s))asg:S!Rdsand name it the action Ô¨Çow of the environment deÔ¨Åned for a policy . Note thatgis a well deÔ¨Åned function. Intuitively, gis the direction of change in the agent‚Äôs state upon following a policy at statesfor an inÔ¨Ånitesimally small time. The curve in the set of possible states, or the state-trajectory of the agent, is a differential equation whose integral form is as follows, s t=s0+Zt 0g(s l)dl: (1) This solution is also unique (Wiggins, 1989) for a Ô¨Åxed start state, s0, and policy, . The above curve is a smooth curve if the policy is also smooth. Therefore, given an MDP, M, and a smooth deterministic policy,2, the agent traverses a continuous time state-trajectory or curve HM;: [0;T)!S . The value function at time tfor a policyis the cumulative future reward starting at time t: v(st) =ZT te l t fr(s l)dl: (2) Note that the objective function, J(), is the same as v(s0). Our speciÔ¨Åcation is very similar to classical control and continuous time RL (Cybenko, 1989; Doya, 2000) with the key difference being how we deÔ¨Åne the transition function, f. 2.2 M ANIFOLDS MDPs, in practice, have a low-dimensional underlying structure resulting in them having fewer degrees of freedom than their nominal dimensionality. In the Cheetah MujoCo environment, with image observations, the goal of the RL agent is to learn a policy to make the Cheetah move forward as fast as possible, where the Cheetah is constrained to a plane. The actions available to the agent are providing torques at each one of the 6 joints. In the case of learning from visual input in a MuJoCo environment like 2D cheetah, one can describe the cheetah‚Äôs state by its ‚Äúpose‚Äù and position instead of the 128128pixels of the image. The idea of a low dimensional manifold embedded in a high dimensional state space formalises this. A functionh:X!Y, from one open subset XRl1, to another open subset YRl2, is a diffeomorphism if his bijective, and both handh 1are differentiable. Intuitively, a low dimensional surface embedded in a high dimensional Euclidean space can be parameterised by a differentiable mapping, and if this mapping is bijective we term it a diffeomorphism. Here Xis said to be diffeomorphic to Y. A manifold is deÔ¨Åned as follows. DeÔ¨Ånition 2.1. A subsetMRkis called a smooth m-dimensional submanifold of Rk(orm- manifold in Rk) iff every point p2Mhas an open neighborhood URksuch thatU\Mis diffeomorphic to an open subset ORm. A diffeomorphism, :U\M!Ois called a coordinate chart of M and the inverse,  := 1:O!U\Mis called a smooth parameterisation of U\M. We illustrate this with an example in Figure 1. If MRkis a non-empty smooth m-manifold then mk, reÔ¨Çecting the idea that a manifold is of lower or equal dimension than its ambient space. A smooth curve :I!Mis deÔ¨Åned from an interval IRto the manifold Mas a function that is inÔ¨Ånitely differentiable for all t. The derivative of attis denoted as _(t). The length of a curve :I!Mis deÔ¨Åned as L() =R Ijj_(t)jjdt, wherejjjj denotes the vector norm. In the Euclidean space the distance between two points is the length of the unique straight line connecting them. Similarly, the geodesic distance between two points a;b2Mis deÔ¨Åned as the length of the shortest such curve, : [0;1]!M, such that(0) =aand(1) =b. We denote the geodesic distance betweena;b2Mon the manifold Mis denoted by the function dM(a;b). The set of derivatives of the curve at time t,_(t), for all possible smooth , form a set that is called the tangent space. The tangent space characterises the geometry of the manifold and it is deÔ¨Åned as follows. 3Figure 1: The surface of an open cylinder of unit radius, denoted by S2, inR3is a 2D manifold embedded in a 3D space. More formally, S2=f(x;y;z )jx2+y2= 1;z2( h;h)gwhere the cylinder‚Äôs height is 2h. One can smoothly parameterise S2as (;b) = (sin;cos;b). The coordinate chart is (x;y;z ) = (sin 1x;z). DeÔ¨Ånition 2.2. LetMbe anm-manifold in Rkandp2Mbe a Ô¨Åxed point. A vector v2Rkis called a tangent vector of Matpif there exists a smooth curve :I!Msuch that(0) =p;_(0) =v: The setTpM:=f_(0)j:R!Mis smooth;(0) =pgof tangent vectors of Matpis called the tangent space of Matp. Continuing our example, the tangent space of a point pinS2is the vertical plane tangent to the cylinder at that point. For a small enough and a vector v2TpS2there exists a unique curve : [ ;]!S2such that(0) =pand_(0) =v. 3 S TATE SPACE GEOMETRY The state space is typically thought of as a dense Euclidean space in which all states lie, but it is not necessarily the case that all such states are reachable by the agent. Two main factors constrain the states available to an agent: 1)the transition function and the actions that are available to an agent, and2)the start state s0. We therefore deÔ¨Åne Seas the effective set of states . Under the assumptions thatis the set of all smooth policies, :S!A , for a continuous time MDP, M, and those made in section 2.1 we deÔ¨Åne the effective set of states as follows. DeÔ¨Ånition 3.1. For an MDP ,M, the effective set of states, Se, is deÔ¨Åned as the union of the sets of states of all possible continuous curves traversed by the agent for the set of all smooth policies. Formally,Se=[2fsjs=HM;(t)for somet2(0;T)g;where is the set of everywhere smooth policies with domain S, andHM;: [0;t)!S is the curve deÔ¨Åned by the policy given the MDPM. We make three additional assumptions: 1.Full rank Jacobian: fhas a full rank Jacobian, in variables [a;t], for alls2S;a2A andt2(0;T), and for a Ô¨Åxed policy the solution for the equation s t=s, if it exists, is unique int. 2.No small orbits: there exists an  > 0such that for all t <  and a Ô¨Åxed policy the solution for the equation s t=s, if it exists, is unique in t. 3.Action space restriction: there exists an r>0and an open restriction A0 sofA, for every states2Se, such thatA0 sA and for all 0<<r , if we have f(s;a1;) =f(s;a2;) for some Ô¨Åxed a1;a22A0thena1=a2. We provide further explanations for these assumptions and how they restrict our study in Appendix A. Under these two assumptions for the setting of continuous RL as in Section 2.1 we have the result stated below. Theorem 3.2. The set of effective states, Se, is a smooth manifold of dimensionality at most da+ 1. We provide the proof in Appendix B. Intuitively, the action set limits the directions in which the agent can go. For example, a single action executed for a Ô¨Ånite time interval makes the agent traverse a 1D 4Figure 2: Consider a hypothetical scenario as in the image above where upon taking an action even though the agent moves locally at state sin the direction of the red arrow (along g(s)) as time progresses it ends up travelling along the green arrow which is on the manifold Se. There is some constraint that restricts the state space ‚Äúavailable‚Äù to an agent. Thus, the agent can only reach the states that have a valid solution for the Equation 1 for any smooth policy. curve. The union of all such curves is the only subset of the state space the agent can reach. The key technical idea is to construct a coordinate chart, and therefore a diffeomorphism, from a subset of low dimensional euclidean space to a subset of the state space. We Ô¨Åx a state sand consider the function :A(0;r)!S such that(a;) =f(s;a; ). This result pertains to the global geometry of the state manifold, meaning the state manifold everywhere has this low dimensional structure. Our result formalises the long-held idea that there is in fact a lower dimensional structure to the state manifold; in particular, this is true when da+ 1<ds, which is almost always the case for RL environments. It also formalises the idea that the agent can only observe data for states reachable by interacting with the environment using the set of actions at its disposal. Henceforth, we refer to Seas the state manifold. We also present an immediate corollary of Theorem 3.2. Corollary 3.3. For everya2A and Ô¨Åxeds2Se, the action tangent mapping, g(s;a), maps from the set of actionsAtoTsSeand therefore there exists a vector va2TsSesuch thatva=g(s;a)for alla2A. This implies that locally an agent is moving along a tangent in the tangent space of a state in the state manifold as it acts upon the environment. The corollary follows from DeÔ¨Ånition 2.2, and its proof is in Appendix B. Intuitively, Corollary 3.3 implies that locally the agent‚Äôs state changes in a constrained manner dictated by the actions available to it and the local geometry of the manifold, i.e., the tangent space and its projection onto the state manifold. We illustrate this in Figure 2. We note that the idea of actions mapping to the tangent space was assumed earlier by Liu et al. (2021) for constrained RL but Corollary 3.3 both proves this assumption and shows that it holds in general for continuous RL environments. 4 C ONNECTIONS BETWEEN CONTINUOUS TIMEDETERMINISTIC RL AND EMPIRICAL RL We have presented our results in the continuous time RL setting, which is an underutilized theoretical tool in the study of RL. Here, and in the experimental section, we argue that it is in fact a useful model for theoretical analysis in the continuous state and action, discrete time, setting. The primary intuition is that, in the context of simulated robotic control problems, our work approximates the agents behavior during the transition period (t;t+ 1) as the action atchanges toat+1in a smooth manner and similarly for sttost+1. In this context, the discrete-time observations can be viewed as time-uniform samples from an underlying continuous time process. This is in keeping with with robotic control, in a robot with various joints we can only measure the joints over intervals which are then attributed to discrete time measurements. Similarly, we can actuate the motors up to a frequency and those can be considered as discrete control signals, even though the underlying physical process is continuous time. 5(a) Walker2D  (b) Cheetah  (c) Reacher  (d) Swimmer Figure 3: State manifold dimensionality, in blue, is close to da+ 1(horizontal red line) and far below ds(horizontal green line) for various environments, estimated using the algorithm by Facco et al. (2017). More formally, consider a discrete time MDP with continuous state and action spaces, a discrete state trajectory is the sequence of states fstgT t=1and similarly for actions, fatgT t=1. Let#:A[0;1]A! Abe a function that acts as a smooth transformation between two successive actions at;at+1such that the agent transition from sttost+1in the continuous time model, as described in Section 2.1. In other words, #(at;l;at+1) =at+lis the discrete to continuous time transformation of actions. We postulate that there exists an operator H#, dependent on the MDP M, such that a discrete trajectory can be transformed into a continuous time trajectory. Proving such an augmentation exists, given that the underlying physical process as described in Section 2.1 and the discrete trajectory is sampled at discrete time intervals as described above, is beyond the scope of our current work. Intuitively, since the discrete time trajectories are temporally spaced out measurements of continuous trajectories the existence of such an operator can be considered an ‚Äúinversion‚Äù of the sampling process. Finally, we address the assumption of deterministic transitions. For our empirical analysis we use the popular MuJoCo environments for robotic control with sensor inputs (Todorov et al., 2012) and these environments are deterministic for all practical intents and purposes (see Appendix C). We argue further that our theoretical model has broader applicability. One way to model transitions is to assume that the underlying state transitions are deterministic but the observations the agent receives have additive noise. More formally, a simplistic way to model stochastic transition F(s;a; ) = f(s;a; ) +N(0;)is the stochastic transition function such that there is additive noise to this deterministic transition. We postulate that the effective set of states can be modeled as a manifold with each observation the agent makes lying at a certain distance to the state manifold and the distance is normally distributed. This is also the idea behind many manifold learning paradigms: the data lies at a distance to a low-dimensional manifold and this distance is distributed according to some probability law (Fefferman et al., 2013; Pai et al., 2019). 5 E MPIRICAL VALIDATION Our empirical validation is two fold. First, we show that the bound on the manifold dimensionality as in Theorem 3.2 holds in practice. Second, we demonstrate the practical relevance of our result by learning a ‚Äúbottleneck‚Äù representation using which an RL agent can learn effectively. 5.1 E MPIRICAL DIMENSIONALITY ESTIMATION To empirically corroborate our main result (Theorem 3.2) we perform experiments in the MuJoCo domains provided in the OpenAI Gym (Brockman et al., 2016). These are all continuous state and action spaces with da< dsfor simulated robotic control tasks. The states are typically sensor measurements such as angles, velocities or orientation, and the actions are torques provided at various joints. We estimate the dimensionality of the state manifold Se. To sample data from the manifold, we record the trajectories from multiple evaluation runs of DDPG across different seeds (Lillicrap et al., 2016), with two changes: we use GELU activation (Hendrycks & Gimpel, 2016) instead of ReLU, in both policy and value networks, and also use a single hidden layer network of width 400 instead of 2 hidden layers for both the networks. This is inline with the assumptions for Theorem 3.2 in Section 3, that the policy is smooth. Performance is comparable to the original DDPG setting (see the Appendix G). For background on DDPG refer to Appendix F. We then randomly sample states from the evaluation trajectories to obtain a subsample of states, D=fsign i=1Se. We estimate 6the dimensionality with 10 different subsamples of the same size to provide an error region for the estimates. We employ the dimensionality estimation algorithm introduced by Facco et al. (2017), which estimates the intrinsic dimension of datasets characterized by non-uniform density and curvature, to empirically corroborate Theorem 3.2. Further details about the dimensionality estimation procedure are presented in Appendix E. The estimates for four MuJoCo environments are shown in Figure 3. For all environments the estimate remains in the neighbourhood of da+ 1in keeping with Theorem 3.2. 5.2 L EARNING VIA THE LOW-DIMENSIONAL MANIFOLD REPRESENTATION We now validate the relevance of Theorem 3.2 using a popular policy gradient method, deep deter- ministic policy gradient (DDPG) Lillicrap et al. (2016), which is discrete time. DDPG is a framework for learning in environments with continuous action spaces, and the policy and value function are parameterised by DNNs. Let be the parameters of the policy DNN. The agent learns by updating the policy parameters with respect to the discrete time discounted return, J():  +rJ(); whereis the learning rate for the policy and J()is the discounted return objective. For further details and background on DDPG please see Appendix F. We do so by learning mapping to a low dimensional manifold of size da+ 1from the control input space that feeds into the policy. The central idea is to show that this compressed representation can be used to learn a control policy with RL without any loss‚Äîand possibly even a gain‚Äî of performance. Our goal in validating the applicability of this low dimensional representation is to learn an isometric coordinate chart from the high dimensional representation for states in Seto a low dimensional Euclidean space Rm, wheremda+ 1as noted in Theorem 3.2, and then compare learning a policy using this compression of the state to learning from the full state. We denote the coordinate chart by :Se!Rda+1parameterised by  . For a coordinate chart to be isometric it must preserve the geodesic distance on the manifold Se, meaning for s1;s22Sewe have that dSe(s1;s2) = jj (s1; )  (s2 )jj2, wheredSe(s1;s2)is the geodesic distance between s1ands2on the manifoldSeandjjjj 2denotes the Euclidean norm. Note that imposing isometry on a coordinate chart is a stronger condition than it being a diffeomorphism because it is distance preserving, in addition to being a bijection and differentiable. Such isometric mappings have been used in the past to learn the underlying manifold for high-dimensional data (Tenenbaum et al., 2000; Basri & Jacobs, 2017; Pai et al., 2019). This is done to ensure tractability of the objective that we introduce below. Given a dataset of states D=fsigN i=1Sewe minimize the loss L ( ) =1 NX si;sj2D  dSe(s1;s2) jj (s1; )  (s2 )jj22; (3) which is similar to the loss for learning low-dimensional manifold embeddings introduced by Tenen- baum et al. (2000). The computation and estimation of the geodesic distance, dSe, is particularly challenging. One approach is to use a graph, as a discrete approximation of a manifold, to calculate this geodesic distance (Yan et al., 2007; Dong et al., 2011). The graph is constructed with each datapoint as a node and an edge in between two datapoint if one of them is knn-nearest neighbor of the other in the dataset. In addition to that there is a distance attribute associated with every edge that is the Euclidean distance between the two points. Therefore, the geodesic distance between two datapoints can be approximated the sum of these edge-wise attributes for the edges constituting the shortest path between the two points. Another practical challenge in the empirical estimation of the lossL is sampling pairs of states for which we can obtain approximate geodesic distances with reasonable compute time. In summary, we calculate dSein three steps: 1)sample data from replay buffer and therefore the manifold Se,2)construct aknn-nearest neighbor graph with edge attributes, and3)estimate the distance between two states using the sum of edge attributes of the shortest path between these edges. The exact procedure employed by us is detailed in Appendix H and illustrated with a simple example in Figure 10, further algorithmic details are also provided in Appendix I. Although there has been signiÔ¨Åcant work in learning isometric mappings (Pai et al., 2019; Basri & Jacobs, 2017) and embeddings (Tenenbaum et al., 2000; Zha & Zhang, 2003), for data sampled from a manifold, these prior works assume that the data distribution remains static throughout the training 7Figure 4: We introduce a bottleneck hidden layer of width da+1which is the output of the coordinate chart, , i.e. the green colored base of the DNN. The output of this coordinate chart is fed into the manifold loss (Equation 3). process. However, the state distribution changes with the policy in RL and this mapping feeds into the policy itself, effecting the agents performance. We learn this isometric mapping,  , in tandem with the policy to account for the distribution shift. To do so we introduce an intermediate hidden layer of size da+ 1in the policy DNN. The output of this layer is then trained by performing gradient descent on the loss L . In this architecture  and the gradient is as follows:  +rJ()  rL  ; (4) whereand are the learning rates for the policy and the coordinate chart respectively. Note that update for parameters  is then     r L  , in addition to the update with respect to the objective for increasing the discounted return (See Appendix I and Algorithm 1 for further details). This is because for all i2n we haveriL( ) = 0 . We illustrate the architecture and how the representation feeds into the loss, for the policy network, in Figure 4. Note that the architecture and gradient updates for the DNN parameterising the Qfunction remains the same as used by Lillicrap et al. (2016), for control inputs, which is a two hidden layer DNN with 400 neurons in the Ô¨Årst hidden layer, 300 in the second one and a one dimensional output. We present results for 4 different MuJoCo environments: Cheetah, Walker2D, Swimmer and Reacher in Figure 5 and for three out of four of these environments the performance is either the same or better than the DDPG baseline. Our results strongly suggest that a compressed isometric representation of dimesionality da+ 1, learned using gradient updates can be used for learning a policy. This furthers our argument: there is a low dimensional structure to RL problems and it can be employed to learn more efÔ¨Åciently. Algorithmic details are given in Appendix I. Ablation studies for the newly introduced hyper-parameter  in Appendix J and comparison to training in absence of manifold loss is provided in Appendix M, due to space limitations. In addition to this, we observe that the manifold loss (Equation 3) also drops steadily as training progresses, meaning that the representation being learnt,  (s; ), is a low-dimensional isometric representation which retains the manifold geometry. The graphs for how the manifold loss, L , evolves in each case is given in Figure 6 along with the interpretation. We explain the reasons for Reachers‚Äô failure for our algorithm in Appendix K. In summary, to learn an isometric mapping to the Euclidean space the agent might require more thandim(Se)dimensions, for the case of Reacher. The additional constraint that we place, that the learnt coordinate chart has to be isometric, is detrimental in this case. As has been done in manifold learning literature, we need methods for learning the low-dimensional mapping that do not rely on learning an isometry (Roweis & Saul, 2000; Zhang & Zha, 2004). We demonstrate the effects of changing the width of the bottleneck layer on the Cheetah domain in Figure 12, in the Appendix. We also need better methods to approximate the geodesic distance between data points in the very high-dimensional setting. We also report additional results for learning with soft actor critic algorithm (Haarnoja et al., 2018) in conjunction with manifold representation learning in Appendix L. 6 R ELATED WORK There has been signiÔ¨Åcant empirical work that assumes the set of states to be a manifold in RL. The primary approach has been to study discrete state spaces as data lying on a graph which has an underlying manifold structure. Mahadevan & Maggioni (2007) provided the Ô¨Årst such framework 8(a) Walker2D  (b) Cheetah  (c) Reacher  (d) Swimmer Figure 5: For all the environments we use  = 10 5, in comparison to = 10 4, and the rest of the hyper-parameters are the same as reported by Lillicrap et al. (2016), over 6 random seeds. (a) Walker2D  (b) Cheetah  (c) Reacher  (d) Swimmer Figure 6: We observe that the the loss L gradually decreases in all instances except Reacher where the performance is sub-par. For the Walker2D environment we see an increase in manifold error that matches the downward spike in performance in Figure 5 but the return for our method still surpasses the baseline in this case. to utilise the manifold structure of the state space in order to learn value functions. Machado et al. (2017) and Jinnai et al. (2020) showed that PVFs can be used to implicitly deÔ¨Åne options and applied them to high dimensional discrete action MDPs (Atari games). Wu et al. (2019) provided an overview of varying geometric perspectives of the state space in RL and also show how the graph Laplacian is applied to learning in RL. Another line of work, that assumes the state space is a manifold, is focused on learning manifold embeddings or mappings. Several other methods apply manifold learning to learn a compressed representation in RL (Bush & Pineau, 2009; Antonova et al., 2020; Liu et al., 2021). Jenkins & Mataric (2004) extend the popular ISOMAP framework (Tenenbaum, 1997) to spatio-temporal data and they apply this extended framework to embed human motion data which has applications in robotic control. Bowling et al. (2005) demonstrate the efÔ¨Åcacy of manifold learning for dimensionality reduction for a robot‚Äôs position vectors given additional neighbourhood information between data points sampled from robot trajectories. At the same time, continuous RL has been applied to continuous robotic control (Doya, 2000; Deisenroth & Rasmussen, 2011; Duan et al., 2016). We apply continuous state, action and time RL as a theoretical model to study the geometry of popular continuous RL environments for the Ô¨Årst time. More recently, in various papers that take a theoretical approach to deep learning the intrinsic dimension of the data manifold and its geometry play an important role in determining the complexity of the learning problem (Shaham et al., 2015; Cloninger & Klock, 2020; Goldt et al., 2020; Paccolat et al., 2020; Buchanan et al., 2021). Schmidt-Hieber (2019) shows that, under assumptions over the function being approximated, the statistical risk deep ReLU networks approximating a function can be bounded by an exponential function of the manifold dimension. Basri & Jacobs (2017) theoretically and empirically show that SGD can learn isometric maps from high-dimensional ambient space down tom-dimensional representation, for data lying on an m-dimensional manifold, using a two-hidden layer neural network with ReLU activation where the second layer is only of width m. Similarly, Ji et al. (2022) show that the sample complexity of off-policy evaluation depends strongly on the intrinsic dimensionality of the manifold and weakly on the embedding dimension. Coupled with our result, these suggest that the complexity of RL problems and data efÔ¨Åciency would be inÔ¨Çuenced more by the dimensionality of the state manifold, which is upper bounded by da+ 1, as opposed to the ambient dimension. Finally, our work could be applied in conjunction with recent work that studies RL algorithms in light of the underlying structure of deep Q-functions (Kumar et al., 2021). 97 D ISCUSSION AND CONCLUSION We have shown that that the dimensionality of the manifold is upper bounded by da+ 1and have empirically veriÔ¨Åed it (Figure 3). This proves that the popular manifold assumption (Mahadevan, 2005; Machado et al., 2017; 2018; Banijamali et al., 2018; Wu et al., 2019; Liu et al., 2021) holds under certain conditions in continuous-time reinforcement learning. It also shows that there is an underlying lower dimensional structure to the MDPs. Additionally, we demonstrate the applicability of this result in a practical setting by showing that a DDPG agent can learn efÔ¨Åciently in this highly compressed, low-dimensional space. Our newly introduced architecture and simultaneous low-dimensional representation learning along with policy learning performs on par or better than the baseline DDPG approach, for MuJoCo environments with sensor inputs. Overall, we show a theoretical bound on intrinsic dimensionality of continous RL problem and also show the efÔ¨Åcacy of this low-dimensional representation in learning a policy. This opens up room for new theoretical and empirical advances paving way for better DNN architecture design and representation learning algorithms. 8 R EPRODUCIBILITY STATEMENT We offer the following details for all the experiments we have performed, in the main body or appendix: hyperparameters, sample sizes, GPU-hours, CPU-hours, code, neural network architectures, Python libraries used, input sizes, and external code bases. We also provide the code with instructions on running it in the supplementary material. All the details to run the code and its location are in Appendix O. All the hyperparameters, for our experiments in Section 5.2, can be found in appendices I, J and K. REFERENCES Alessio Ansuini, Alessandro Laio, Jakob H. Macke, and Davide Zoccolan. Intrinsic dimension of data representations in deep neural networks. In NeurIPS , 2019. Rika Antonova, Maksim Maydanskiy, Danica Kragic, Sam Devlin, and Katja Hofmann. Ana- lytic manifold learning: Unifying and evaluating representations for continuous control. ArXiv , abs/2006.08718, 2020. Andr√°s Antos, Csaba Szepesvari, and R√©mi Munos. Learning near-optimal policies with bellman- residual minimization based Ô¨Åtted policy iteration and a single sample path. Machine Learning , 71:89‚Äì129, 2007. Francis R. Bach and David M. Blei (eds.). Proceedings of the 32nd International Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015 , volume 37 of JMLR Workshop and Conference Proceedings , 2015. JMLR.org. URL http://proceedings.mlr.press/ v37/ . Ershad Banijamali, Rui Shu, Mohammad Ghavamzadeh, Hung Hai Bui, and Ali Ghodsi. Robust locally-linear controllable embedding. In AISTATS , 2018. Ronen Basri and David W. Jacobs. EfÔ¨Åcient representation of low-dimensional manifolds using deep networks. ArXiv , abs/1602.04723, 2017. Osbert Bastani. Sample complexity of estimating the policy gradient for nearly deterministic dynamical systems. ArXiv , abs/1901.08562, 2020. Marc G. Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning environ- ment: An evaluation platform for general agents (extended abstract). In IJCAI , 2013. Michael Bowling, Ali Ghodsi, and Dana F. Wilkinson. Action respecting embedding. Proceedings of the 22nd international conference on Machine learning , 2005. Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym. ArXiv , abs/1606.01540, 2016. 10Michael M. Bronstein, Joan Bruna, Taco Cohen, and Petar Velivckovi‚Äôc. Geometric deep learning: Grids, groups, graphs, geodesics, and gauges. ArXiv , abs/2104.13478, 2021. Sam Buchanan, Dar Gilboa, and John Wright. Deep networks and the multiple manifold problem. ArXiv , abs/2008.11245, 2021. Keith Bush and Joelle Pineau. Manifold embeddings for model-based reinforcement learning under partial observability. In NIPS , 2009. G. Carlsson, T. Ishkhanov, V . D. Silva, and A. Zomorodian. On the local behavior of spaces of natural images. International Journal of Computer Vision , 76:1‚Äì12, 2007. Minshuo Chen, Haoming Jiang, Wenjing Liao, and Tuo Zhao. EfÔ¨Åcient approximation of deep relu networks for functions on low dimensional manifolds. ArXiv , abs/1908.01842, 2019. Alexander Cloninger and Timo Klock. Relu nets adapt to intrinsic dimensionality beyond the target domain. ArXiv , abs/2008.02545, 2020. G. Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of Control, Signals and Systems , 2:303‚Äì314, 1989. Marc Peter Deisenroth and Carl Edward Rasmussen. Pilco: A model-based and data-efÔ¨Åcient approach to policy search. In ICML , 2011. Wei Dong, Moses Charikar, and K. Li. EfÔ¨Åcient k-nearest neighbor graph construction for generic similarity measures. In WWW , 2011. Kenji Doya. Reinforcement learning in continuous time and space. Neural Computation , 12:219‚Äì245, 2000. Yan Duan, Xi Chen, Rein Houthooft, John Schulman, and P. Abbeel. Benchmarking deep reinforce- ment learning for continuous control. In ICML , 2016. Elena Facco, Maria d‚ÄôErrico, Alex Rodriguez, and Alessandro Laio. Estimating the intrinsic dimen- sion of datasets by a minimal neighborhood information. ScientiÔ¨Åc Reports , 7, 2017. C. Fefferman, S. Mitter, and Hariharan Narayanan. Testing the manifold hypothesis. arXiv: Statistics Theory , 2013. Matthias Fey and Jan E. Lenssen. Fast graph representation learning with PyTorch Geometric. In ICLR Workshop on Representation Learning on Graphs and Manifolds , 2019. Sebastian Goldt, Marc M√©zard, Florent Krzakala, and Lenka Zdeborov√°. Modelling the inÔ¨Çuence of data structure on learning in neural networks. ArXiv , abs/1909.11500, 2020. Aditya Grover and Jure Leskovec. node2vec: Scalable feature learning for networks. Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , 2016. Tuomas Haarnoja, Haoran Tang, P. Abbeel, and Sergey Levine. Reinforcement learning with deep energy-based policies. In ICML , 2017. Tuomas Haarnoja, Aurick Zhou, P. Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In ICML , 2018. Matthew J. Hausknecht and Peter Stone. The impact of determinism on learning atari 2600 games. InAAAI Workshop: Learning for General Competency in Video Games , 2015. Kaiming He, X. Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , pp. 770‚Äì778, 2016. Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv: Learning , 2016. Odest Chadwicke Jenkins and Maja J. Mataric. A spatio-temporal extension to isomap nonlinear dimension reduction. In ICML ‚Äô04 , 2004. 11Xiang Ji, Minshuo Chen, Mengdi Wang, and Tuo Zhao. Sample complexity of nonparametric off-policy evaluation on low-dimensional manifolds using deep networks. ArXiv , abs/2206.02887, 2022. Yuu Jinnai, Jee Won Park, Marlos C. Machado, and George Dimitri Konidaris. Exploration in reinforcement learning with deep covering options. In ICLR , 2020. Steven G. Krantz and Harold R. Parks. The implicit function theorem. 2002. Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classiÔ¨Åcation with deep convolu- tional neural networks. Communications of the ACM , 60:84 ‚Äì 90, 2012. Aviral Kumar, Rishabh Agarwal, Dibya Ghosh, and Sergey Levine. Implicit under-parameterization inhibits data-efÔ¨Åcient deep reinforcement learning. ArXiv , abs/2010.14498, 2021. Sergey Levine, Chelsea Finn, Trevor Darrell, and P. Abbeel. End-to-end training of deep visuomotor policies. ArXiv , abs/1504.00702, 2016. Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Manfred Otto Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. CoRR , abs/1509.02971, 2016. Puze Liu, Davide Tateo, Haitham Bou-Ammar, and Jan Peters. Robot reinforcement learning on the constraint manifold. In CoRL , 2021. Marlos C. Machado, Marc G. Bellemare, and Michael Bowling. A laplacian framework for option discovery in reinforcement learning. ArXiv , abs/1703.00956, 2017. Marlos C. Machado, Clemens Rosenbaum, Xiaoxiao Guo, Miao Liu, Gerald Tesauro, and Mur- ray Campbell. Eigenoption discovery through the deep successor representation. ArXiv , abs/1710.11089, 2018. Sridhar Mahadevan. Proto-value functions: developmental reinforcement learning. Proceedings of the 22nd international conference on Machine learning , 2005. Sridhar Mahadevan and Mauro Maggioni. Proto-value functions: A laplacian framework for learning representation and control in markov decision processes. J. Mach. Learn. Res. , 8:2169‚Äì2231, 2007. Tomas Mikolov, Kai Chen, Gregory S. Corrado, and Jeffrey Dean. EfÔ¨Åcient estimation of word representations in vector space. In ICLR , 2013. V olodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin A. Riedmiller. Playing atari with deep reinforcement learning. ArXiv , abs/1312.5602, 2013. V olodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, Martin A. Riedmiller, Andreas Fidjeland, Georg Ostrovski, Stig Pe- tersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis. Human-level control through deep rein- forcement learning. Nature , 518(7540):529‚Äì533, 2015. doi: 10.1038/nature14236. URL https://doi.org/10.1038/nature14236 . R√©mi Munos and Csaba Szepesvari. Finite-time bounds for Ô¨Åtted value iteration. J. Mach. Learn. Res., 9:815‚Äì857, 2008. John Nash. C1 isometric imbeddings. Annals of Mathematics , 60(3):383‚Äì396, 1954. ISSN 0003486X. URLhttp://www.jstor.org/stable/1969840 . Jonas Paccolat, Leonardo Petrini, Mario Geiger, Kevin Tyloo, and Matthieu Wyart. Geometric compression of invariant manifolds in neural networks. Journal of Statistical Mechanics: Theory and Experiment , 2021, 2020. Gautam Pai, Ronen Talmon, Alexander M. Bronstein, and Ron Kimmel. Dimal: Deep isometric manifold learning using sparse geodesic sampling. 2019 IEEE Winter Conference on Applications of Computer Vision (WACV) , pp. 819‚Äì828, 2019. 12Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d 'Alch√©-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems 32 , pp. 8024‚Äì8035. Curran Associates, Inc., 2019. F. Pedregosa, G. Varoquaux, A. Gramfort, V . Michel, B. Thirion, O. Grisel, M. Blondel, P. Pretten- hofer, R. Weiss, V . Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research , 12:2825‚Äì2830, 2011. Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. Deepwalk: online learning of social representations. Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining , 2014. Sam T. Roweis and Lawrence K. Saul. Nonlinear dimensionality reduction by locally linear embed- ding. Science , 290 5500:2323‚Äì6, 2000. Johannes Schmidt-Hieber. Deep relu network approximation of functions on a manifold. ArXiv , abs/1908.00695, 2019. John Schulman, S. Levine, P. Abbeel, Michael I. Jordan, and P. Moritz. Trust region policy optimiza- tion. ArXiv , abs/1502.05477, 2015. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. ArXiv , abs/1707.06347, 2017a. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. CoRR , abs/1707.06347, 2017b. URL http://arxiv.org/abs/ 1707.06347 . Uri Shaham, Alexander Cloninger, and Ronald R. Coifman. Provable approximation properties for deep neural networks. ArXiv , abs/1509.07385, 2015. David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, L. Sifre, George van den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Vedavyas Panneershelvam, Marc Lanctot, Sander Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy P. Lillicrap, Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis. Mastering the game of go with deep neural networks and tree search. Nature , 529:484‚Äì489, 2016. Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. CoRR , abs/1409.1556, 2015. Richard S Sutton and Andrew G Barto. Introduction to reinforcement learning , volume 135. MIT press Cambridge, 1998. J. Tenenbaum, V . de Silva, and J. Langford. A global geometric framework for nonlinear dimension- ality reduction. Science , 290 5500:2319‚Äì23, 2000. Joshua B. Tenenbaum. Mapping a manifold of perceptual observations. In NIPS , 1997. Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems , pp. 5026‚Äì5033, 2012. George E. Uhlenbeck and Leonard Salomon Ornstein. On the theory of the brownian motion. Physical Review , 36:823‚Äì841, 1930. Stephen Wiggins. Introduction to applied nonlinear dynamical systems and chaos. 1989. Yifan Wu, G. Tucker, and OÔ¨År Nachum. The laplacian in rl: Learning representations with efÔ¨Åcient approximations. ArXiv , abs/1810.04586, 2019. 13Shuicheng Yan, Dong Xu, Benyu Zhang, HongJiang Zhang, Qiang Yang, and Stephen Lin. Graph embedding and extensions: A general framework for dimensionality reduction. IEEE Transactions on Pattern Analysis and Machine Intelligence , 29:40‚Äì51, 2007. Greg Yang and Edward J. Hu. Feature learning in inÔ¨Ånite-width neural networks. ArXiv , abs/2011.14522, 2020. Hongyuan Zha and Zhenyue Zhang. Isometric embedding and continuum isomap. In Proceedings of the 20th International Conference on Machine Learning (ICML-03) , pp. 864‚Äì871, 2003. Zhenyue Zhang and Hongyuan Zha. Principal manifolds and nonlinear dimensionality reduction via tangent space alignment. SIAM J. ScientiÔ¨Åc Computing , 26:313‚Äì338, 2004. Henry Zhu, Justin Yu, Abhishek Gupta, Dhruv Shah, Kristian Hartikainen, Avi Singh, Vikash Kumar, and Sergey Levine. The ingredients of real-world robotic reinforcement learning. ArXiv , abs/2004.12570, 2020. 14A A SSUMPTIONS We list all the assumptions by section and consequently by theorems. We also provide some intuition on what these assumptions mean and how they limit our study. Assumption made in Section 2.1 about continuous-time RL: 1.We assume that the transition function, f, is inÔ¨Ånitely differentiable or smooth. This is done to ensure that the manifold it forms is also smooth. This might not always hold in practice but the transitions can be Cksmooth, meaning k times differentiable. We use this assumption in the construction of a diffeomorphism in the deÔ¨Ånition of a manifold. 2.The set of actions, A, is rectangular and open. This is pretty standard in practice for continuous environments. This much less an assumption but done so to simplify the analysis and avoid any confusion. The only reason we need the open part is to construct a diffeomorphism. We do not state the entirety of the assumptions made in Section 3 but we provide further intuition for them: 1.We also use assumption that the Jacobian of fis full rank, in [a;t]. This assumption is needed for applying the implicit function theorem in the proof of Proposition B.2. 2.The no small orbits assumption is a strong one but it is essential to proving Theorem 3.2. This might not hold strictly in practice but it can be argued that it holds almost always with probability 1, essentially the probability of an agent making an orbit is almost surely 0. Moreover, as long as there is a positive such that there are no orbits for time t <  this condition is satisÔ¨Åed. In other words, this condition is akin to saying there is no reversibility, of state transitions, in inÔ¨Ånitesimally small time intervals. It is naturally true in environments with physical dynamics, e.g., acceleration, velocity or displacement cannot be instantaneously reversed. 3.The action space restriction condition helps us construct a bijection for the proof of Theorem 3.2. Intuitively, the idea is that if two actions have the same local effect on how the agents state changes in some state then they can be collapsed into a single action for a Ô¨Åxed state s. B P ROOF OF THEOREM 3.2 We Ô¨Årst state the implicit function theorem (for more details see theorem 3.3.1 in Chapter 3 by Krantz & Parks (2002) and its proof using the inverse function theorem). In the statement of the implicit function theorem we use Fto denote a general function, that satisfy the conditions stated in the theorem. Theorem B.1. (Implicit Function Theorem) LetF:Rd1+d2!Rd2be a continuously differentiable function. Let Rd1+d2have coordinates (x;y)and Ô¨Åx a point (x0;y0) = (x0;1;:::;x 0;d1;y0;1;:::;y 0;d2) withF(x0;y0) =0, where 02Rd2. The appropriate Jacobian is deÔ¨Åned as: JF;y(x;y) =2 664@F1 @y1(x0;y0):::@F1 @yd2(x0;y0) ......... @Fd2 @y1(x0;y0):::@Fd2 @yd2(x0;y0)3 775: If this Jacobian matrix is invertible then there exists an open set VRd1containing x0such that there exists a uniquely continuous differentiable function h:V!Rd2such thath(x0) =y0, and F(x;h(x)) = 0 for all x2V. Additionally, h 1is continuously differentiable in the h-image ofV. Using the implicit function theorem we Ô¨Årst prove our main result for the exact case where the dimensionality of the effective state space, Se, isda+ 1. We do so under additional assumptions in the following proposition. Proposition B.2. Under the assumptions of Section 2.1 and 3 with the added assumption that the transition function, f, is injective inA(0;L), for someL, for Ô¨Åxeds2S the effective state space, SeRds, is a manifold of is da+ 1. 15Proof. Given any state s2Sewe can construct an open neighbourhood that maps to an open subset ofRda+1. Since we know that s2Sethere exists a state s02Sesuch thatf(s0;a;) =sfor some a2A and2(0;L). Now, for a Ô¨Åxed s0consider the map  s0:A( ;+)!Sesuch that  s0(a;t) =f(s0;a;t)where 0<< , such that+<L , anda2A. We need to show that this map,  s0, is a diffeomorphism and maps to an open subset of Se. Since the transition function, f, is smooth we have that  s0is also smooth in its range. Also note that fis injectiveA(0;L)for Ô¨Åxeds0and therefore it maps A( ;+)uniquely to a set U. This setU is a subset ofSeby deÔ¨Ånition. Note that Uis open becauseA( ;+)is open because a bijection maps open sets to open sets. This leads to the conclusion that  s0:A( ;+)!USeis a smooth bijection. Thereby providing us a smooth parameterisation from Rda+1toU. This means that its inverse,   1 s0exists. Now to show that this inverse is differentiable we apply the implicit function theorem. LetF(x;y;t) =x  s0(y;t), note thats0is Ô¨Åxed, and as in above the variables are x;yandt. The functionFis restricted to the domain such that x2Se,y2A andt2( ;+). Since the Jacobian offis full rank, by assumption, the appropriate Jacobian, JF;[y;t]as deÔ¨Åned in Theorem B.1, is invertible and Fis also continuously differentiable. Similarly, the condition F(x;y;t) =0holds forx=s,y=aandt=. Therefore, by the implicit function theorem there exists a unique function hand an open neighbourhood Vcontaining [a;]2Rda+1such thatF(h(y;t);y;t) = 0 andhis differentiable. Since his unique we need to show that h= s0for the domain V\(A( ;+)). The proof follows from uniqueness of h. We have, for the domain V\(A ( ;+)), F( s0(y;t);y;t) =f(s0;y;t)  s0(y;t) = 0 . Therefore since his unique and fis an injection for a Ô¨Åxeds0we haveh= s0, which means   1 s0is continuously differentiable in its domain By Theorem B.1. We can similarly construct these diffeomorphisms from Rda+1toUSefor alls2Se. Finally, by DeÔ¨Ånition 2.1 we have that Seis ada+ 1dimensional manifold. Now we prove the general case without the assumption of injectivity on f. To do so we construct an surjection f0:SA0R+!S whereA0Rd0is an open set such that d0da. We state the two additional assumptions made in Section 3. The Ô¨Årst one being that for any policy 2we do not have any critical points or loops in the differential Ô¨Åeld deÔ¨Åned by g. Formally, we have the following conditions g(s)6=0for alls2Sand for a Ô¨Åxed policy the solution for the equation s t=s, if it exists, is unique in t. The second one is that if two actions have the same outcome at one state then they have the same outcome at all states. More formally, there exists an r>0such that for all0<<r , if we havef(s;a1;) =f(s;a2;)for some Ô¨Åxed a1;a22A and any one s2S then it holds for allS. Under the assumptions of Section 2.1 and the ones stated above we restate the main theorem below before presenting the proof. Theorem 3.2. The set of effective states, Se, is a smooth manifold of dimensionality at most da+ 1. Proof. With assumption 3 for every state we have a restriction A0 sthat essentially turns f(s;a;t ) into a bijection for every a2 A0 s. Now consider a function f0:SeA0 s0( ;+) which is a restriction of f‚Äôs domain toA0 s0(0;T)for a Ô¨Åxeds0. Now for a Ô¨Åxed s, we can construct an open neighbourhood using  s0:A0 s0( ;+)constructed as in Proposition B.2,  s0(a;t) =f0(s0;a;t)such that s0(a;) =sfor somea2A0 s0and>> 0and also+< , whereis as in assumption 2. Now we know that this is a bijection from assumptions 1, 2 and 3 in Section 3.1 as long as we choose andsmall enough to satisfy the requirements for Proposition B.2. The restricted set A0 s0ensures that each action, for a Ô¨Åxed time tands, maps to a unique state. Further, the assumption on no orbits ensures that for a Ô¨Åxed action a2A0 s0there is a unique solution tof(s0;a;t) =s00for a Ô¨Åxeda;s0ands00int. Therefore, we constructed a full rank smooth bijection  s0which maps an open set A0( ;+)to an open subset of Sefor everys2Se. We can now apply the same arguments of Proposition B.2 to argue that there exists a diffeomorphism  s0 form an open set in A0 s0( ;+)to an open set inSe. This holds true for all s. 16Table 1: Transition Variances for MuJoCo Environments Environment std (Se) Cheetah 9:410 16 Walker2D 2:710 15 Reacher 7:810 16 Swimmer 1:0410 15 Finally, we conclude by stating that the dimensionality of the manifold is equal to the of dim(A0 s0 ( ;+)) + 1 which is less than or equal to da+ 1. Finally, we prove Corollary 3.3. The proof is fairly straightforward following DeÔ¨Ånition 2.2 and DeÔ¨Ånition 3.1. Since the manifold Seis deÔ¨Åned as the union of all continuous curves from all possible smooth policies we can Ô¨Ånd a curve such that for any s2Seanda2A for some policy (s) =ain the neighborhood of sSe\B(s;)whereB(s;)is a Euclidean ball of radius >0centered ats. This would mean that for this policy, starting at s0, there is a curve such that for some time tand some interval  >0we have(t) =sand therefore _(t) =vwherev2TsSe. This vector vcan now be uniquely mapped to the action and therefore can be labeled vaas in Corollary 3.3. We also note that this vector is unique independent of the choice of or. C E MPIRICAL VALIDATION OF OUR ASSUMPTIONS We validate two of our assumptions. We Ô¨Årst demonstrate that the MuJoCo environments, for which we present results in Section 5, are deterministic for all practical intents and purposes. Second, we show that the Jacobian of the tranistion function with respect to the action for a Ô¨Åxed time and state is full rank for all of these environments, thereby suggesting that assumption 1 in Section 3 is ‚Äúpartially satisÔ¨Åed‚Äù. C.1 D ETERMINISTIC TRANSITIONS IN MUJOCOENVIRONMENTS To estimate the stochasticity of the transitions in the environments we estimate the transition standard deviation : std(Se) =EsU[Se];aU[A] (s0 E[s0])2jSt=s;St+1=s0;At=a ; whereU[]represents the uniform distribution over a set. This captures how much variance is in the transition dynamics. We estimate this value by randomly and uniformly sampling states, s, from an agents trajectories over the learning process. We then obtain randomly and uniformly sample actions,a, from the set of actions A. Then for a Ô¨Åxed (s;a)we take observe the one step transition that the environment returns: s0, a 100 times. For a Ô¨Åxed swe sample 30 such actions a. Therefore, we estimate the quantity std(Se)using approximately 6 million samples, for each environment. The results, divided by the standard deviation of states, are presented in Table 1 suggest that for all practical intents and purposes these environments are deterministic. C.2 F ULL RANK JACOBIAN ASSUMPTION To validate assumption 1 from Section 3, we empirically estimate the Jacobian. Our assumption states that the function f(s;a;t )is full rank in [a;t]for alls2S. Since we are working in practical framework the value of tis Ô¨Åxed to be 1, meaning the environment simulates the application of action afor a single unit of time and returns the next state s0. Therefore, we are only able to estimate the Jacobian of f(s;a;1)for the variable a, see thattis Ô¨Åxed to 1. To do so, we sample sU[Se], as described in the previous section. We then sample aU[A], as described in the previous section. Finally we estimate the Jacobian for the variable aand the function f(s;a;1), see Theorem B.1 for deÔ¨Ånition of the Jacobian. We do so using the scipy function approx_fprime . Finally, once we have the dadsJacobian matrix we estimate the rank of this matrix using the scipy function estimate_rank . We tabulate all the results in Table 2. This procedure comes as close as we can to validating assumption 1 in Section 3. 17Table 2: Transition Variances for MuJoCo Environments Environment daRank Cheetah 6 6 Walker2D 6 6 Reacher 2 2 Swimmer 2 2 (a)  (b)  (c)  (d) Figure 7: Frames from the Atari game Seaquest. We present 4 frames that are temporally successive from left to right. D C ONNECTION TO DISCRETE ACTION ENVIRONMENTS One of the most popular RL environments with discrete states and actions are the Atari games from the arcade learning environment (Bellemare et al., 2013). Mnih et al. (2013) provided the Ô¨Årst result in learning policies with only high-dimensional images as the input to a DNN. Atari games are deterministic given a Ô¨Åxed policy (Hausknecht & Stone, 2015). In case of images, there are two considerations when it comes to the underlying structure of data: 1.Underlying structure of images: the states, which are frames from a video game, in Atari games have a low dimensional underlying structure. Even though the frames are 210160 pixel images there is a low dimensional underlying structure to them. For example, consider the frames from the Atari game Seaquest in Figure 7. These frames can be identiÔ¨Åed uniquely with far fewer variables such as: position and heading of the submarine, position and heading of the sharks, oxygen capacity, the scores, the number of lives, position of the bullet and the score. 2.Underlying structure from the transitions: Since we know that the agent can only change its state and observation by taking actions it limits the states available to it. In the Seaquest example, we know that the agent can only access some subset of all the possible conÔ¨Åg- urations of the low-dimensional manifold of Seaquest images. Meaning only a subset of conÔ¨Ågurations of shark positions, submarine positions etc. are realizable. We further illustrate these ideas in Figure 8. These two factors combined together impose a low- dimensional structure on the state manifold. Development of such a theoretical model, which accurately captures these restrictions or dual structures, is beyond the scope of our current work where we primarily deal with the structure imposed by the transitions. E D IMENSIONALITY ESTIMATION BY FACCO ET AL . (2017) We describe the algorithm for dimensionality estimation in context of sampled data from the state manifoldSe. Let the dataset be randomly sampled points from a manifold Seembedded in Rds denoted byD=fsigN i=1. For a point sifrom the datasetDletfri;1;ri;2;ri;3;:::gbe a sorted list of distances of other points in the dataset from siand they set r0= 0. Then the ratio of the two nearest neighbors is i=ri;2=ri;1whereri;1is the distance to the nearest neighbor in Dofsiandri;2is the distance to the second nearest neighbor. Facco et al. (2017) show that the logarithm of the probability distribution function of the ratio of the distances to two nearest neighbors is distributed inversely proportional to the degree of the intrinsic dimension of the data and we follow their algorithm for estimating the intrinsic dimensionality. We describe the methodology provided by Facco et al. 18(a) Figure 8: The structure imposed by transitions is illustrated above. Suppose the agent starts at the red state. If the agent were to execute the down action it gets to the blue state. There are three ""directions"" in which the agent (and the submarine) can transition. If the agent chooses to execute the action left it gets to the yellow state, similarly green state on right and white state on down. The agent traverses along this surface of admissible images, which are constrained by the underlying structure of the images, given that they are from the Atari game Seaquest and from the set of possible transitions along this manifold. Here discrete actions are represented by continuous curves, in black, on a manifold. (2017) in context of data sampled by an RL agent from a manifold. Without loss of generality, we assume thatfsigN i=1are in the ascending order of ri. We then Ô¨Åt a line going through the origin for f(log(i); log(1 i=N)gN i=1. The slope of this line is then the empirical estimate of dim(Se). We refer the reader to the supplementary material provided by Facco et al. (2017) for the theoretical justiÔ¨Åcation of this estimation technique. The step by step algorithm is restated below. 1. Compute ri;1andri;2for all data points i. 2. Compute the ratio of the two nearest neighbors i=ri;2=ri;1. 3.Without loss of generality, given that all the points in the dataset are sorted in ascending order ofithe empirical measure of cdf is i=N. 4.We then get the dataset Ddensity =f(log(i); log(1 i=N)gthrough which a straight line passing through the origin is Ô¨Åt. The slope of the line Ô¨Åtted as above is then the estimate of the dimensionality of the manifold. F DDPG B ACKGROUND An agent trained with the DDPG algorithm learns in the discrete time but with continuous states and actions. With abuse of notation, a discrete time and continuous state and action MDP is deÔ¨Åned by the tupleM= (S;A;P;fr;s0;), whereS;A,s0andfrare the state space, action space, start state and reward function as above. The transition function P:SAS is the transition probability function, such that P(s;a;s0) = Pr(St+1=s0jSt=s;At=a), is the probability of the agent transitioning from stos0upon the application of action afor unit time. The policy, in this setting, is stochastic, meaning it deÔ¨Ånes a probility distribution over the set of actions such that(s;a) = Pr(At=ajSt=s). The discount factor is also discrete in this setting such that an 19analogous state value function is deÔ¨Åned as v(st) =Esl;al;P""TX l=tl tfr(sl;al)jst# ; which is the expected discounted return given that the agent takes action according to the policy , transitions according to the discrete dynamics Pandstis the state the agent is at time t. Note that this is a discrete version of the value function deÔ¨Åned in Equation 2. The objective then is to maximiseJ() =v(s0). One abstraction central to learning in this setting is that of the state-action value function Q:SA! R, for a policy , is deÔ¨Åned by: Q=Esl;al;P""TX l=tl tfr(sl;al)jst;at# ; which is the expected discounted return given that the agent takes action atat statestand then follows policyfor its decision making. An agent, trained using the DDPG algorithm, parametrises the policy and value functions with two deep neural networks. The policy, :S!A , is parameterised by a DNN with parameters and the action value function, q:SA! R,is also parameterised by a DNN with ReLU activation with parameters Q. Although, the policy has an additive noise, modeled by an Ornstein-Uhlenbeck process (Uhlenbeck & Ornstein, 1930), for exploration thereby making it stochastic. Lillicrap et al. (2016) optimise the parameters of the Qfunction,Q, by optimizing for the loss LQ=1 NNX i=1(yi Q(si;ai;Q))2; (5) whereyiis the target value set as yi=ri+Q(s0 i+1;(si+1;);Q). The algorithm updates the parametersQbyQ Q+QrQLQ, whereLQis deÔ¨Åned as in Equation 5. The gradient of the policy parameters is deÔ¨Åned as rJ() =1 NX iraQ(s;a;Q)js=si;a=(si)rQ(s;)js=si; (6) and the parameters are updated in the direction of increasing this objective. G DDPG MODIFIED ARCHITECTURE COMPARISON We provide the comparison between single hidden layer network and multiple hidden layer network because our results in section 4 are for single hidden layer. The same architecture is used by Lillicrap et al. (2016) for the policy and value function DNNs which is two hidden layers of width 300 and 400 with ReLU activation. Here we provide the comparison to single hidden layer width 400 and MUP (Yang & Hu, 2020) with GELU activation for the architecture used by Lillicrap et al. (2016). We provide this comparison in Figure 9 and note that the performance remains comparable for both the architectures. All results are averaged over 6 different seeds. We use a PyTorch based implementation for DDPG with modiÔ¨Åcations for MUP parametrisation and the use of GELU units. The base implementation of the DDPG algorithm can be found here:https://github.com/rail- berkeley/rlkit/blob/master/examples/ddpg.py. The hyperparameters are as in the base implementation. (a) Walker2D  (b) Cheetah  (c) Reacher  (d) Swimmer Figure 9: Comparison of single hidden layer (blue) and multiple hidden layer (red) architectures for DNNs. 20H S AMPLING STRATEGY AND GEODESIC DISTANCE ESTIMATION We use graphs as a discrete abstraction of manifolds to sample efÔ¨Åciently whilst preserving local properties of the sampled nodes. In many representation learning aprroaches the main objective is to learn to map individual data points to dense vectors in a low dimensional space via stochastic gradient descent (Mikolov et al., 2013; Perozzi et al., 2014; Grover & Leskovec, 2016). One of the primary objectives is to preserve relationships between data points. Graphs, with individual data points mapping to nodes and the relationships between them ecnoded as the edges, are used as an abstraction to represent the whole dataset with all the charecteristic relationships. In our case we seek to preserve local isometry in the learnt mapping,  . Meaning, given a point s2DSewe would like to preserve the geodesic distance between sand the points in its ‚Äúneighbourhood‚Äù upon being mapped to a lower dimensional representation. We Ô¨Årst deÔ¨Åne the graph that is a discrete representation the state manifold, Se, and state the above objective more formally. An undirected graph G= (V;E)consists of nodes and edges, where V= fvigNV i=1are the individual nodes and an edge e= (vi;vj)2Eimplies that there is a an edge between the nodesviandvjfor somei;j. Since the graph,G, is undirected (vi;vj)2E=)(vj;vi)2E. Given a node, vi, the one-hop neighborhood of this node is deÔ¨Åned as all the nodes vj2Vsuch that (vi;vj)2E. Now we deÔ¨Åne a graph, deonted by GD= (D;ED)that forms a discrete abstraction over the manifoldSeusing the datasetD. We Ô¨Årst set all the ndoes to be the states in our dataset, D. A datapoint‚Äôs one hop neighbourhood as the knnpoints nearest by the Euclidean distance in the dataset D. This means that for every state si2D we haveknnedges toknndistinct nearest points in D. This is aknn-nearest neighbors construction of the graph from a dataset D, sampled from a manifold (Yan et al., 2007; Dong et al., 2011). We augment the edges of this graph GDwith an additional set of edge attributes, the Euclidean distance between two points (si;sj)2EDi.e.jjsi sjjj2. Therefore, the augmented graph now becomes GD= (D;ED;AD)whereADis an ordered set with the Euclidean distances between the two nodes in ED. For most practical applications, the number nodes of this graph GDis in thousands and consequently the number of edges are knntimes the number of nodes. Since we learn the mapping  using SGD we can sample batches of state pairs and the distances between them. To do so we randomly sample a subset of states from D. We then construct a subgraph which consists of nodes khhops away from these randomly subsampled points. We then use this randomly sampled subgraph to compute the pairwise distance between them using breadth Ô¨Årst search. We denote this procedure by the function Random-K-Hop-Subgraph (GD;kh). Since this randomly sub-sampled graph is much smaller in size whilst preserving the distances we postulate that the distance thus obtained between pairs of states, as the sum of the distance attributes of the shortest paths, is a good approximation of the geodesic distance dSe. We remind the reader that this geodesic distance is needed for the manifold loss introduced in Equation 3. This three step procedure: 1)sampling data on the manifold, 2)constructing a knn-nearest neighbors graph from this data, and 3)randomly sampling kh-hop subgraphs, is illustrated with an example of the Swiss roll manifold in Figure 10. I A LGORITHMIC DETAILS An RL agent, trained using the DDPG algorihm, collects data from trajectories as tuples: (s;a;s0;r) wheresis the state at which the agent takes an action aand transitions to state s0and obtains the rewardr. The algorithm stores a buffer of these tuples and therefore we have access to the set of available states s, sampled from the state manifold Se. As described in the previous section, our algorithms requires a dataset sampled from the manifold to estimate the manifold loss in Equation 3. Therefore our algorithm subsamples the states from this buffer to obtain the aforementioned dataset,D, at every episode. Then our algorithm (Algorithm 1) uses this dataset to obtain a sample of pairs of states and geodesic distances between these states, as described in the previous section, to perform SGD on the parameters of the mapping to the low-dimensional space,  , on the loss L . All the results have the same values for knn= 6;kh= 4 and we use a batch size of 128for the subsampling the graphs as described in Appendix H for the function Random-K-Hop-Subgraph. These were obtained after manual tuning. The other parameters, are set to the defualt ones for DDPG = 0:01,= 0:99,Q= 0:001and the replay buffer size is 106. We provide details for running the code in Appendix O. 21(a) First we obtain a set of points sampled from a manifold. (b) In the second step, edges are drawn between theknn= 4nearest points. (c) Finally, nodes and edges (in red) are subsam- pled based on the kh= 2hop neighbourhood of randomly sampled nodes. Figure 10: The three step sub-sampling for estimating geodesic distances and batched learning in SGD is illustrated with the example of the Swiss-roll manifold. We use the scikit-learn package (Pedregosa et al., 2011) for sampling the data, torch-geometric package for generating knn-nearest neighbors graph and also for obtaining the kh-hop subgraphs (Fey & Lenssen, 2019). 22Algorithm 1 DDPG with Manifold Representaion Learning ;Q parameterised by ;Qrespectively. ;QPr() Initialize the parameters of the target 0;Q0 ;Q. Initialise replay buffer B forEpisode 1 to Max Episodes do Initialise the Ornstein-Uhlenbeck process for exploration noise: X forTime stept, 0 toTdo Set actionat=(st;) +Xt. Execute action atand observe rt;st+1storing it inB Sample a minibatch of Ntransitions (si;ai;ri;si+1)fromB Compute the loss LQand update the parameters Q Construct the dataset Dfrom all the states sifrom the sampled Ntuples Construct the graph GDusingknn-nearest neighbors Subsample graphG0 D0(D0;ED0;A0 D0) =Random-K-Hop-Subgraph (GD;kh) Obtain the set of state pairs and geodesic distances, from G0 D0, to calculate L  Update the policy as in Equation 3:  +rJ()  rL  Update the target networks: Q0 Q+ (1 )Q0 0 + (1 )0 end for end for Figure 11: We observe the effect of increasing the hyperparameter  on the discounted return in the Walker2D environment. J A DDITIONAL EXPERIMENTS AND ABLATION STUDIES All results reported here that are reported are an average over 6 different seeds. For all but the Reacher environment we see the manifold loss decrease as training progresses, as expected. Meaning the agent is able to learn a low-dimensional isometric representation,  , as well as a policy that operates on this low-dimensional input. We observe that for the Reacher environment our Algorithm is unable 23Figure 12: The discounted return for varying the width of the bottleneck layer for the Cheetah domain withda+ 1 = 7 . We see the performance peaks at width 8. (a) Mean Evaluation Returns  (b) Manifold Loss Figure 13: We observe that the manifold loss decreases as we increase the width of the bottleneck layer and the performance improves. All the hyper-parmaeters are the same as in Appendix I. to simulataneously learn a low dimensional mapping and a policy. This is a true despite searching across all the hyperparameters. We present the ablation study over the hyperparameter  in Figure 11. This suggests that further increasing the learning rate,  , improves performance of the agent, in case of walker but it begins to detireorate after a certain point,  = 1:5. Finally, we demonstrate the effects of changing the width of the bottleneck layer on the Cheetah domain in Figure 12. Most importantly, we observe that the performance peaks at width equal to 8, with both 7 and 8 have similar returns. From the results for dimensionality estimation, in Figure 3, we know that the estimate lies between 7 and 8. This suggests that in case of Cheetah there is an optimal isometric ‚Äúcompression‚Äù that allows the agent to perform optimally and better than the baseline. This furthers our argument that an RL agent can learn efÔ¨Åciently on this low dimensional manifold by utilising the underlying structure. The error margins are omitted for the clarity of exposition since there are multiple curves on the same graph. 24K E XPLAINING FAILURE IN REACHER ENVIRONMENT Here we explain the failure case of the Reacher environment. As noted in Section 5.2 isometry is a stronger condition that mere diffeomorphism. The Nash embedding theorem states that the an m-dimensional C1manifold can be embedded isometrically into a Euclidean space of dimensionality at mostm(m+ 1) or(3m+ 11)=2(Nash, 1954), therefore the embedding dimension required for learning an isometric embedding for Semight be greater than da+ 1. For example, a circle which is a 1D manifold in 2D Euclidean space cannot be isometrically embedded into 1D. Therefore, the objective we train on is stronger than learning a coordinate chart. We hypothesize that for the reacher environment the agent is unable to learn this isometry. As reported in Figure 13, as we increase the width, and therefore the embedding dimension, the manifold loss decreases and the agents performance as measured by mean discounted return improves. As we increase the width to 7 the performance is on par with the baseline approach. The error margins are omitted for the clarity of exposition since there are multiple curves on the same graph. L L EARNING VIA LOW-DIMENSIONAL REPRESENTATION FOR SOFT ACTOR CRITIC The soft actor critic (SAC) algorithm (Haarnoja et al., 2018) provides a method for learning policy in a more stable manner compared to previous algorithms like DDPG (Lillicrap et al., 2016), TRPO (Bach & Blei, 2015), and PPO (Schulman et al., 2017b). L.1 B ACKGROUND ON SOFT ACTOR CRITIC The goal of the SAC algorithm is to train an RL agent acting in the MDP M= (S;A;P;fr;s0;), which is as described in Appendix F. The SAC agent optimises for maximising the modiÔ¨Åed objective: J() =TX t=0Est;at;P[fr(st;at) +H((;st;))]; whereHterm is the entropy of the policy . This additional entropy term improves exploration (Schulman et al., 2017a; Haarnoja et al., 2017). Haarnoja et al. (2018) optimise this objective by learning 4 DNNs: the (soft) state value function V(s;V), two instances of the (soft) state-action value function: Q(s1;at;Q i)wherei2f1;2g, and a tractable policy (st;at;). To do so they maintain a dataset Dos state-action-reward-state tuples: D=f(si;ai;ri;s0 i)g. The soft value function is trained to minimize the following squared residual error, JV(V) =EsD1 2  V(s;V) Ea Q(s;a;Q) log(s;a;)2 ; (7) where the minimum of the values from the two value functions Qiis taken to empirically estimate this expectation. The soft Q-function parameters can be trained to minimize the soft Bellman residual JQ(Q) =Es;a;r;s0D1 2  Q(s;a;Q) r V(s0;V)2 ; (8) where Vare the parameters of the target value function, which are updated at a slower rate compared to the parameters V, as is also done in Algorithm 1. The policy parameters are learned by minimizing the expected KL-divergence, J() =EsD DKL (s;;);exp(Q(s;;Q)) ZQ(s) ; (9) whereZQ(s)normalizes the distribution. In addition to this we add the manifold loss as described in Equation 3 to the policy objective. In keeping with the notation above, the learning rates for the functionsV;Q; and areV;Q;and respectively. Algorithm 2 details how the agent performs this modiÔ¨Åed learning. 25Algorithm 2 SAC with Manifold Representation Learning ;Qi;Vparameterised by ;Q i;Vrespectively. ;Q i;VPr() Initialize the parameters of the target V V. Initialise empty dataset D forEpisode 1 to Max Episodes do forTime stept, 0 toTdoSample action at(st;). Observe successive state st+1P(st+1jst;at). Append to datasetD D[f (st;at;rt;st+1)g. end for Construct the graph GDusingknn-nearest neighbors Subsample graphG0 D0(D0;ED0;A0 D0) =Random-K-Hop-Subgraph (GD;kh) Obtain the set of state pairs and geodesic distances, from G0 D0, to calculate L  forGradient steps do Update the value function parameters: V V VrVJV(V); whereJVis as in Equation 7. Update theQ-function parameters: Q i Q i QrJQ(Q);i2f1;2g; whereJQis as in Equation 8. Update the policy following the objective in Equation 9:  +rJ()  rL : Update the value function target network: V V+ (1 )V: end for end for 26Figure 14: Our architecture for SAC, which similar to the architecture in Figure 4. (a) Walker2D  (b) Cheetah  (c) Reacher  (d) Swimmer Figure 15: For all the environments we use  = 7:510 5, in comparison to = 310 4, and the rest of the hyper-parameters are the same as reported by Haarnoja et al. (2018), over 6 random seeds. L.2 H YPER -PARAMETER DETAILS The discount factor, , is set to 0.99. The learning rates for the Q-functions,Q, theV-function, V, and the policy, piare set to 310 4. The update parameter of the value function, , is set to 510 3. The batch size is 256. The learning rate for the manifold loss,  , is set to 7:510 5. All the parameters and for learning the manifold representation are the same as described in Appendix I. L.3 E XPERIMENTAL RESULTS We provide the results for our architecture (as described in Figure 14), with manifold learning, in comparison to ‚Äúvanilla‚Äù SAC for the four algorithms. These results are for an architecture and algorithm similar to described Section 5.2 except with SAC algorithm as opposed to DDPG. All the mean discounted rewards are reported in Figure 15 and the corresponding manifold loss is reported in Figure 16. we observe that our agent performs at par with the baseline in case of Walker2D, Cheetah and Reacher and slightly worse in the Swimmer environment. This result was obtained without an extensive hyperparameter tuning by varying the value of  over a very small range. M C OMPARISON WITHOUT MANIFOLD LOSS We provide another comparison where we compare the same bottleneck architecture for the policy network with and without the manifold loss. This demonstrates the efÔ¨Åcacy of the manifold loss (a) Walker2D  (b) Cheetah  (c) Reacher  (d) Swimmer Figure 16: We report the manifold loss as above. Except Cheetah every other domain behaves as expected. 27(a) Walker2D  (b) Cheetah  (c) Reacher  (d) Swimmer Figure 17: We compare the scenario where we use the same architecture for DDPG as illustrated on the left side of Figure 4, with a bottleneck layer of width da+ 1, with, labeled ‚ÄúDDPG‚Äù, and without, labeled ‚ÄúOurs‚Äù, the manifold learning loss. All the results are averaged over 6 random seeds. (a) Walker2D  (b) Cheetah  (c) Reacher  (d) Swimmer Figure 18: We compare using the same architecture, as in Figure 14, for the two implentations of the SAC algorithm with (labeled ""Ours"") and without manifold loss (labeled ""SAC""). described in Equation 3. In Figure 17 we observe that for all the environments our algorithm performs better except for the Reacher environment where both the algorithms perform sub-optimally to the wide network baseline. We attribute the higher variance to changing representation of  . Ansuini et al. (2019) showed that DNNs implicitly learn low-dimensional manifold structure at varying depths when trained with SGD in a supervised manner. Their results are for various popular image classiÔ¨Åcation models like ResNet (He et al., 2016), AlexNet (Krizhevsky et al., 2012) and VGG-16 (Simonyan & Zisserman, 2015). This speaks to the efÔ¨Åcacy of our manifold loss L , that the addition of this loss improves performance over the implicit low-dimensional manifold representation learnt by a Deep ReLu network with a bottleneck layer. We report a similar comparison between SAC algorithms with the same DNN architecture, as in Figure 14, with and without the manifold loss. We observe that with the manifold loss the performance is better for Walker2D, Swimmer and Cheetah environments. For the Reacher environment we observe that SAC algorithm ends up Ô¨Ånding the optimal policy with low variance and with far fewer episodes in both cases. N C OMPUTE REQUIREMENTS For each one of our experiments we perform 6 different runs with varying seeds to obtain the results. Since experiments in Section 5.1 only require trajectories we reuse the trajectories sampled form the ReLU and GELU comparison experiments in Appendix E. For all the experiments in Appendix E we required approximately 180 hours of processing time on NVIDIA GeForce RTX 3090 GPUs which have 24 GB of RAM. We also run two runs of the DDPG algorithm simultaneously on each GPU, owing to the 24 GB RAM availability per GPU, which cuts our run time into half. O S TEPS FOR RUNNING THE CODE AND GPU H OURS All code is in python and was tested for python 3.6. We use the pytorch library for the ease of deÔ¨Åning and training DNNs (Paszke et al., 2019) and pytorch_geometric (Fey & Lenssen, 2019) for all graph operations. We provide the implementation of DDPG using GELU units in the code/rlkit/ folder of our supplementary material To install the environment we recommend installation using the setupy.py Ô¨Åle present in the folder. For running the experiment, there are multiple Ô¨Åles in thecode/rlkit/examples/smooth_ddpg/ddpg *.py, each one uses a different architec- 28ture and environment details of which can be found within the Ô¨Åle. To execute the code, e.g. for ddpg_arch_2.py , run the following command from the rlkit folder: python examples/smooth_ddpg/ddpg_arch_2.py 115 gelu where ""115"" is the seed and ""gelu"" argument means the DNN thus instantiated uses gelu activa- tions. Our code samples trajectories and the location of the folder can be speciÔ¨Åed in the Ô¨Åle path_collector.py The code for dimensionality estimation using neighborhood data is fairly simple and only requires version 1.1.1 of the python package scikit-learn (Pedregosa et al., 2011). It is provided in code/data-processing/dim-estimate . The main Ô¨Åle is dim-estimate.py . It expects a pickle Ô¨Åle which is an array of all states sampled for an agent. The code for cleaning the samples, from runs of the DDPG code as described above, and acquiring them in the required format can be found in the Ô¨Åle run_data_processing.py . The code for the simultaneous dimensionality reduction and policy learning can be found in the folder code/rlkit/examples/manifold_learning/ddpg *.py. The implementation for graph creation and sampling procedure using pytorch_geometric is in the Ô¨Åle code/rlkit/rlkit/torch/manifold/mrl_ddpg.py . The various architectures used for the policy network can be found in the folder code/rlkit/rlkit/archs_dir/manfiold_arch . Finally, the code for obtaining the illustrations in Figure 10 are in the folder code/rlkit/illustration_scripts . For the results and all its ablations in Section 5.2, we ran multiple instances of the modiÔ¨Åed DDPG algorithm and the baseline DDPG algorithms for 6 seeds each for 1000 epochs on cloud instances with Nvidia 3090 Ti GPUs with 24 GB memory, and CPUs with 8 cores and 16 GB RAM. Each run takes about 3 hours each. This means we utilised about 700 GPU hours, including hyperparameter tuning and auxiliary experiments presented in the Appendix. The results in Section 5.1 were obtained from the trajectories sampled from various DDPG runs and took about 20 CPU hours on an 8 core machine. 29"
10,https://arxiv.org,2301.00010,10.48550/arXiv.2301.00010,"Dibyendu Nandy, Dipankar Banerjee, Prantika Bhowmik, Allan Sacha Brun, Robert H. Cameron, S. E. Gibson, Shravan Hanasoge, Louise Harra, Donald M. Hassler, Rekha Jain, Jie Jiang, Laur√®ne Jouve, Duncan H. Mackay, Sushant S. Mahajan, Cristina H. Mandrini, Mathew Owens, Shaonwita Pal, Rui F. Pinto, Chitradeep Saha, Xudong Sun, Durgesh Tripathi, Ilya G. Usoskin",Exploring the Solar Poles: The Last Great Frontier of the Sun,"Despite investments in multiple space and ground-based solar observatories by the global community, the Sun's polar regions remain unchartered territory - the last great frontier for solar observations. Breaching this frontier is fundamental to understanding the solar cycle - the ultimate driver of short-to-long term solar activity that encompasses space weather and space climate. Magnetohydrodynamic dynamo models and empirically observed relationships have established that the polar field is the primary determinant of the future solar cycle amplitude. Models of solar surface evolution of tilted active regions indicate that the mid to high latitude surges of magnetic flux govern dynamics leading to the reversal and build-up of polar fields. Our theoretical understanding and numerical models of this high latitude magnetic field dynamics and plasma flows - that are a critical component of the sunspot cycle - lack precise observational constraints. This limitation compromises our ability to observe the enigmatic kilo Gauss polar flux patches and constrain the polar field distribution at high latitudes. The lack of these observations handicap our understanding of how high latitude magnetic fields power polar jets, plumes, and the fast solar wind that extend to the boundaries of the heliosphere and modulate solar open flux and cosmic ray flux within the solar system. Accurate observation of the Sun's polar regions, therefore, is the single most outstanding challenge that confronts Heliophysics. This paper argues the scientific case for novel out of ecliptic observations of the Sun's polar regions, in conjunction with existing, or future multi-vantage point heliospheric observatories. Such a mission concept can revolutionize the field of Heliophysics like no other mission concept has - with relevance that transcends spatial regimes from the solar interior to the heliosphere.","1   EXPLORING THE SOLAR POLES   THE LAST GREAT FRONTIER OF THE SUN       Primary Author: Dibyendu Nandy1*   Co-authors:  Dipankar Banerjee2, Prantika Bhowmik3, Allan Sacha Brun4, Robert H.  Cameron5, S. E. Gibson6, Shravan Hanasoge7, Louise Harra8, Donald M. Hassler9,  Rekha Jain10, Jie Jiang11, Laur√®ne Jouve12, Duncan H. Mackay13, Sushant S.  Mahajan14, Cristina H. Mandrini15, Mathew Owens16, Shaonwita Pal1, Rui F Pinto4,12,  Chitradeep Saha1, Xudong Sun17, Durgesh Tripathi18, Ilya G. Usoskin19    1*Center of Excellence in Space Sciences India, Indian Institute of Science  Education and Research Kolkata, India , 2Aryabhatta Research Institute of  Observational Sciences, India, 3Department of Mathematical Sciences, Durham  University, UK, 4Dept. d‚ÄôAstrophy sique/AIM, Universit√© Paris et Paris -Saclay, France,  5Max Planck Institute for Solar System Research, Germany, 6NCAR/HAO, USA, 7Tata  Institute of Fundamental Research, India,  8Physikalisch -Meteorologisches  Observatorium Davos/World Radiation Center, Switze rland, 9Southwest Research  Institute, Boulder, USA,  10School of Mathematics and Statistics, University of Sheffield,  UK, 11School of Space and Environment, Beihang University, Beijing, China, 12IRAP,  Universit√© de Toulouse, France, 13University of St Andr ews, UK, 14W. W. Hansen  Experimental Physics Laboratory, Stanford University, Stanford, CA, USA, 15Instituto de  Astronom√≠a y F√≠sica del Espacio, Buenos Aires, Argentina,  16Dept. of Meteorology,  University of Reading,  17Institute for Astronomy, University of Hawai ªi at MƒÅnoa,  18Inter- University Centre for Astronomy and Astrophysics, India, 19University of Oulu, Finland     A white paper submitted for consideration of the Decadal Survey in Solar and Space Physics  2024 -2033 o f the US National Academy of Sciences   2   SYNOPSIS     Despite investments in multiple space and ground -based solar observatories by the  global community, the Sun‚Äôs polar regions remain unchartered territory ‚Äì the last great  frontier for solar observations. Breac hing this frontier is fundamental to understanding  the solar cycle ‚Äì the ultimate driver of short -to-long term solar activity that encompasses  space weather and space climate. Magnetohydrodynamic dynamo models and  empirically observed relationships have es tablished that the polar field is the primary  determinant of future solar cycle amplitude. Models of solar surface evolution of tilted  active regions indicate that the mid -to-high latitude surges of magnetic flux govern the  dynamics leading to the reversal  and build -up of polar field. Our theoretical  understanding and numerical models of this high latitude magnetic field dynamics and  plasma flows ‚Äì that are a critical component of the sunspot cycle ‚Äì lack precise  observational constraints, currently limited  by large projection effects due to our location  in the plane of the ecliptic.  This limitation compromises our ability to observe the  enigmatic kilo -Gauss polar flux patches and to quantitatively constrain the polar field  distribution at high latitudes. By  extension, the lack of these observations handicap our  understanding of how high latitude magnetic fields power polar jets, plumes, and the  fast solar wind that extend to the boundaries of the heliosphere and modulate solar  open flux and cosmic ray flux w ithin the solar system. Accurate observation of the Sun‚Äôs  polar regions, therefore, is the single most outstanding challenge that confronts  Heliophysics. A solar polar exploration mission, in isolation, or in conjunction with multi - vantage point observatio ns across the inner heliosphere, stands to revolutionize the  field of Heliophysics like no other mission concept has ‚Äì with relevance that transcends  spatial regimes from the solar interior to the heliosphere. This white paper argues the  scientific case fo r novel out -of-ecliptic observations of the Sun‚Äôs polar regions, in  conjunction with existing, or future multi -vantage point heliospheric observatories.     THE BIG PICTURE     The genesis of solar -stellar magnetic activity can be traced back to a  magnetohydrodynamic (MHD) dynamo mechanism operating in their interior, where  plasma flows and magnetic fields interact through complex processes to generate large - scale magnetic fields  (Charbonneau 2020; Brun et al. 2015). Magnetic active regions  generated by the solar dynamo emerge through the Sun‚Äôs surface (Jouve,  Brun and  Aulanier 2018) to its outer atmosphere, where subsequent dynamical interactions often  lead to energetic, transien t events such as solar flares and coronal mass ejections  (CMEs) ‚Äì which create severe space weather (Schrijver 2015). The emergence and  evolution of solar surface magnetic fields, mediated via near -surface flux transport  processes result  in the redistribut ion of the surface fields which govern the structuring 3   and dynamics of the large -scale solar corona. Solar wind is born here and propagates  throughout the heliosphere. The combined action of surface field mediated open flux  evolution and turbulent solar wi nd transport modulates the open flux, and consequently  the cosmic ray flux in the heliosphere; these define the ambient space environment of  solar system planets such as the Earth, upon which solar magnetic storms act as  transient perturbations governing s pace weather. Understanding and predicting space  weather is critical for protection of our space -based assets (Schrijver  2015, Daglis et al.  2021). Uncovering the influence of long -term solar magnetic variability (Usoskin 2017)  and its forcing o n planetary  magnetosphere -atmosphere systems have profound  implications for space climate and planetary habitability in the solar system and other  stellar -(exo)planetary systems (Nandy et al. 2021).     While many advances have been made in these directions in the last  couple of  decades, the generated knowledge have also exposed critical shortcomings; the  foremost being our inability to observationally constrain and fully comprehend magnetic  field-associated dynamic in the Sun‚Äôs polar regions and its flux content ‚Äì whos e  influence transcends processes spanning the Sun‚Äôs interior, its atmosphere and the  heliosphere (Petrie 2015).  We note that humanity has never accomplished spatially  resolved precise observations of the polar magnetic fields of any star on a  routine basis . The Solar Orbiter mission is half -step in that direction, but that is not  enough. In what follows, we inspire an expedition to the Sun‚Äôs polar regions focusing on  some of the transformative knowledge this can generate; we do so, with the awareness  that th ere might be new  discoveries and surprises that we cannot even anticipate now.      SUN‚ÄôS POLAR FIELDS AND SUNSPOT CYCLE PREDICTIONS     While the solar cycle governs the occurrence probability of severe space weather  events and the decadal -scale forcing of pla netary environments, predicting future cycles  had remained an outstanding challenge (Petrovay 2018).  Analys es of recent progress  demonstrate that the polar field during cycle minima is the best indicator of the future  solar cycle amplitude and that physic s-based dynamo models based on the Babcock - Leighton mechanism have converged to indicate a weak -moderate cycle 25 (Nandy  2021). The Babcock -Leighton mechanism involves the processes of emergence of  active  regions (tilted bipolar sunspot pairs) and the subs equent decay, dispersal and  large -scale separation of opposite polarity flux via near -surface plasma transport  processes such as meridional circulation, turbulent diffusion and pumping (Jiang et al.  2014). Analytic theory indicates that surface fields are the primary driver of the internal  dynamo (Cameron and Sch√ºssler 2015). In particular, cross -equatorial cancellation and  mid-to-high latitude surges from active latitudes (apparent in Fi gure 1d - e) are the  primary determinants of solar polar field amplitu de. In  turn the solar polar field 4   distribution (which determines  the dipole moment) at solar minimum is the main  contributor to the amplitude of the future sycle. These dynamics are currently poorly  constrained by observations.         Figure 1:  Left: The top panel shows the observed sunspot number time series over the  last century, the middle panel shows the hemispheric polar field variation reconstructed  from polar faculae data and Wilcox Solar Observatory data (blue: north, red: south) and  the bottom panel d epicts the solar butterfly diagram. Right: The top panel depicts the  observed evolution of the radial field on the solar surface (gleaned from Kitt Peak  Vacuum Telescope (KPVT), Michelson Doppler Imager (MDI) and Helioseismic and  Magnetic Imager (HMI) obse rvations) while the bottom panel depicts the radial field  evolution as simulated from a data -driven solar surface flux transport model (Bhowmik  and Nandy 2018). The figure on the left drives home the point that the polar field  amplitude preceding a sunspot  cycle determines the strength of the latter. The figure on  the right illuminates the role of propagating ‚Äútongues‚Äù (surges) of magnetic flux from  mid-to-high latitudes in the reversal and build -up of the Sun‚Äôs polar field.       We do not have direct observ ations of the polar field and associated dipole moment  variation and have to rely on proxies such as the polar faculae for validating predictive  surface flux transport and dynamo models. Even more debilitating is the fact that current  polar field observati ons suffer from large -projection effects. Figure 2  succinctly  demonstrates what we can observe from a location with a direct view of the Sun‚Äôs poles  compared to the compromised view from plane -of-ecliptic.     To summarize, a polar mission capable of imaging  magnetic fields from at least  60 degrees above or below the ecliptic plane can return transformative  information on high latitude magnetic field dynamics, constrain the polar field  5   distribution like never before, leading to the validation of magnetic fiel d evolution  models and accurate data -driven predictions of the sunspot cycle.       Figure 2:  Left: A snapshot of the surface distribution of magnetic fields from a solar  surface magnetic field evolution model depicting a perspective close to the plane -of-the- ecliptic (solar obliquity ignored). Right: The equivalent perspective from the top of the   Sun‚Äôs North pole reveals details of the polar cap missing from the low -latitude  perspective. A mission that can navigate to at least 60 degrees above (or below) the  ecliptic plane would reveal these details, constrain the high latitude dynamics and return   accurate measurements of plasma flows and magnetic field distribution at the poles that  govern solar activity and power fast solar winds.          CONSTRAINTS ON HIGH LATITUDE SOLAR PLASMA FLOWS     In the high beta plasma domain in near -surface layers and wit hin the convection zone,  plasma flows drive the magnetic field dynamics. These flows remain poorly constrained  in the polar regions which high -latitude measurements of the surface velocity field have  the potential to overcome (L√∂ptien et al. 2015). In part icular, Dopplergram data would  allow us to apply helioseismic methods (Christensen -Dalsgaard, 2002; Gizon  and Birch,  2005) to infer solar internal structure and dynamics. Meridional circulation ‚Äì which plays  a critical role in setting the timescale and amp litude of large -scale solar dynamo action  and magnetism (Rempel 2006; Hanasoge, 2022) is rather poorly constrained at the  poles. Significant improvements in flow measurements in the vicinity of the polar cap,  including the polar extent of the meridional ci rculation are possible with the magnetic  feature tracking technique (Mahajan et al. 2021) applied to observations that do not  suffer from projection effects. Such high -latitude observations, when used concurrently  with data from existing observatories such  as the Helioseismic and Magnetic Imager  (HMI), can generate definitive constraints on the polar meridional flow. Such  6   measurements open up the possibility of (stereoscopic) helioseismic imaging of the  plasma flows at deeper layers with greater fidelity; t he deep component of the  meridional flow is thought to play a critical role in the equatorward propagation and  latitude of emergence of sunspots (Nandy and Choudhuri 2002), but its structure and  variability remain ill -constrained. Solar internal rotation m easurements ‚Äì one of the  greatest successes of helioseismology, is quite noisy in the vicinity of the poles.      High latitude flows, polar vortices and associated weak rotation, and hints of  significant subsurface variability of rotation are among outstandi ng issues that  polar observations can resolve; these new constraints on rotation and meridional  circulation in the polar regions can generate powerful constraints on the interplay  of flow and fields that sustain solar dynamo action.     UNRAVELLING THE ENIGMA  OF KILO -GAUSS POLAR FLUX PATCHES         Figure 3:  Hinode SOT observations of unipolar, kilo -Gauss flux patches near the solar  north pole acquired on 2007 September 25 (from Ito et al. 2010). The field of view  extends over 20 degrees of the north polar cap.     Observations by the Hinode Solar Optical Telescope (SOT) have uncovered the  existence of intense small -scale kilo -Gauss flux patches in the high latitude regions of  the Sun, often co -existing within, and having the same polarity as the large -scale polar  cap (Tsuneta et al. 2008, Ito et al. 2010; red regions in Fig ure 3). These unipolar flux  patches in the polar region appear to be different in nature compared to small -scale  mixed polarity flux patches in the quiet Sun, and are believed to sustain open magne tic  fields lines in the solar corona which could power the fast solar winds. These unipolar  7   patches contribute significantly to the overall flux content in the polar cap and therefore  may participate in the dynamo mechanism which converts the poloidal fiel d component  to the toroidal field component of the following cycle. These intense flux concentrations  are also relevant in the context of the solar open flux problem (Linker et al. 2017; Wang  et al. 2022), namely, a mismatch between the expected and observ ed open flux. The  origin of these polar kilo -Gauss flux patches, the evolution of their flux content through  the solar cycle, and their relation to the high latitude propagation of magnetic fields  leading to polar field build up and reversal remain poorly understood.     High latitude vantage point observations of magnetic fields and flows at different  phases of the solar cycle are crucial to deciphering the mystery surrounding the  origin of these polar kilo -Gauss flux patches and in conjunction with in -situ solar  wind cha racterization ‚Äì establish whether they are fundamental to the origin of  fast solar winds from the poles.      DYNAMICS OF THE POLAR ATMOSPHERE: JETS, PLUMES AND WINDS     The Sun‚Äôs poles are often characterized by the presence of coronal holes, open solar  magne tic field lines, intense kilo -Gauss flux tubes, polar plumes and jets (Hanaoka et al.  2018), and sometimes pseudo -streamers. Open coronal field lines, ubiquitous in the  polar regions are excellent pathways for Alfv√©n wave (turbulent dissipation) mediated  solar wind acceleration (Hassler et al. 1999; Krishna Prasad, Banerjee and Gupta 2011;  Morton, Tomczyk and Pinto 2015). The strength of the solar polar field during different  phases of the cycle and at minima influences the boundaries of open and closed fie ld  lines and the tilts of large -scale streamers (Dash et al. 2019). Recent findings hint that  Interchange reconnection in coronal holes may power the solar wind; however, proving  this would require polar magnetic field measurements (Tripathi, Nived and Sol anki  2021; Upendran and Tripathi 2022). All of these polar phenomena play a role in the  acceleration and global structuring of fast solar winds, which leave their imprint in the  heliosphere and near -Earth space weather (Cranmer, Gibson and Riley 2017).     Adequate constraints on the landscape of the polar magnetic fields and transient  phenomena through the solar cycle ‚Äì achieved via an out -of-ecliptic space  mission concept designed for multiple passes over the solar poles ‚Äì would  transform our ability to asse ss and predict the large -scale structuring of the  global solar winds and their heliospheric and space weather consequences, and  bring to the fore magnetic connectivity bridging the solar interior to the farthest  reaches of the heliosphere.      8   SUMMARY RECOMME NDATIONS     NASA Programs:  Addressing the novel science presented here necessitates space  mission whose scope may either be Moderate -scale to Large -scale depending on  whether a single -vantage point, or a multi -vantage point mission concept is planned to  address the science. We believe that a proof -of-concept, single -vantage point mission  with a quick turn -around time for imaging the magnetic fields of the polar regions and  solar atmosphere from at least 60 degrees above the plane -of-the ecliptic during the  high latitude dynamics leading up to the next solar minimum (i.e., 2026 -2032) time  frame, with a nominal mission lifetime of 5 -10 years would be a prudent approach (e.g.,  whitepaper by Hassler et al. 2022). Concurrently, and with the experience of this  missi on assimilated, a multi -vantage point mission concept may be explored to  constrain solar activity and in -situ space environment across the inner heliosphere.      NSF Programs:  Theory, numerical simulations, and data analysis of existing  observations from ground and space that directly contribute to a holistic understanding  of physical processes encompassing the solar interior, atmosphere and the space  environment of solar syste m planets, with a view to establishing causality are necessary  elements for community preparedness and the eventual success of future space  missions. We believe that mid -scale Research Infrastructure and Innovation programs  supported by NSF are therefore c rucial to complement any NASA space mission.  Specifically, we propose supporting large teams or Centers that leverage and assimilate  US and global expertise to develop the theoretical, modeling and data analysis tools  that will stand to complement and enab le transformative scientific returns from the out - of-ecliptic, multi -vantage point space missions envisaged in this white paper.     Globally Coordinated Multi -Vantage Point Observations of the Heliosphere:  We  note that existing missions such as Parker Solar Probe, Solar Orbiter, Solar Dynamics  Observatory, ACE, WIND, DSCOVR and upcoming missions such as CNSA Advanced  Space -based Solar Observatory, ISRO‚Äôs Aditya -L1 and ESA‚Äôs Vigil offer the possibility  of globally coordinated observations in conjunction with a  novel solar polar mission. We  recommend that this be taken full advantage of by cementing mechanisms for data  exchange across mission teams and leveraging modeling and data analysis expertise  across borders. This synergy may catalyze more ambitious, multi agency international  programs for system wide exploration of Heliophysics through the deployment of  multiple spacecrafts to cover the whole 4œÄ steradian of the heliosphere (e.g.,  whitepaper by Raouafi et al. 2022) that may be prohibitively expensive for an y one  nation alone; this should be the ultimate goal for humanity, to together understand the  space we share and use this understanding to guide explorations of other worlds,  beyond our own.  9   References      Bhowmik, P. and Nandy, D. 2018, Nat Commun 9, 5209   https://doi.org/10.1038/s41467 -018-07690 -0  Brun, A.S. et al. 2015., Space Sci. Rev.  196, 303   https://doi.org/10.1007/s11214 -014-0117 -8  Cameron, R. and Sch√ºssler, M. 2015, Science 347, 1333   https://doi.org/10.1126/science.1261470   Charbonneau, P. 2020, Living Rev. Sol. Phys.,  17, 4   https://doi.org/10.1007/s41116 -020-00025 -6  Christensen -Dalsgaard, J. 2002, Reviews of Modern Physics, 74, 1073   https://doi.org/10.1103/Rev ModPhys.74.1073   Cranmer, S.R., Gibson, S.E. and Riley, P. 2017, Space Sci. Rev.  212, 1345   https://doi.org/10.1007/s11214 -017-0416 -y  Daglis, I.A. et al. 2021, Annales Geophysicae, 39, 1013   https://doi.org/10.5194/angeo -39-1013 -2021   Dash, S. et al. 2019, Astrophysical Journal, 890, 37   https://doi.org/10.3847/1538 -4357/ab6a91   Gizon, L.,  and Birch, A. C. 2005, Living Reviews in Solar Physics, 2, 6   https://doi.org/10.12942/lrsp -2005 -6  Hanaoka, Y. et al. 2018, Astrophysical Journal, 860, 142   https://doi.org/10.3847/1538 -4357/aac49b   Hanasoge, S. M. 2022, Living Reviews in Solar Physics, 19, 3   https://doi.org/10.1007/s41116 -022-00034-7  Hassler, D.M. et al. 1999, Science, 283, 5403   https://doi.org/10.1126/science.283.5403.810   Hassler, D.M. et al. 2022, White paper on Solaris mission   SSP Decadal Survey 2024 -2033, US Nationa l Academy of Sciences   Ito, H. et al. 2010, Astrophysical Journal, 719, 131   https://doi.org/10.1088/0004 -637X/719/1/131   Jiang, J. et al. 2014, Space Sci. Rev., 186, 491   https://doi.org/10.1007/s11214 -014-0083 -1  Jouve, L, Brun, A.S., Aulanier, G. 2018, Astrophysical Journal, 857, 83   https://doi.org/10.3847/1538 -4357/aab5b6   Krishna Prasad, S., Banerjee, D., and Gupta, G. 2011, A&A, 528, 4   https://doi.org/10.1051/0004 -6361/201016405   Linker,  J.A. et al. 2017, ApJ, 848, 70   https://doi.org/10.3847/1538 -4357/aa8a70   L√∂ptien, B., et al. 2015, Space Sci. Rev., 196, 251   https://doi.org/10.1007/s11214 -014-0065 -3 10   Mahajan, S.S. et al. 2021, ApJ, 917, 100   https://doi.org/10.3847/1538 -4357/ac0a80   Morton, R., Tomczyk, S. and Pinto, R. 2015, Nat Commun  6, 7813  https://doi.org/10.1038/ncomms8813   Nandy, D.  2021, Sol. Phys., 296, 54   https://doi.org/10.1007/s11207 -021-01797 -2  Nandy, D. et al. 2021, Progress in Earth and Planetary Sciences, 8, 1   https://doi.org/10.1186/s40645 -021-00430 -x  Nandy, D. and Choudhuri, A.R. 2002, Science 296, 1671   https://doi.org/10.1126/science.1070955   Petrie, G.J.D. 2015, Living Rev. Sol. Phys., 12, 5   https://doi.org/10.1007/lrsp -2015 -5  Petrovay, K. 2020, Living Rev. Sol. Phys.,  17, 2   https://doi.org/10.1007/s41116 -020-0022 -z  Raouafi, N.E. et al. 202 2, White paper on Firefly mission   SSP Decadal Survey 2024 -2033, US National Academy of Sciences    Rempel, M. 2006, Astrophysical Journal, 647, 662   https://doi.org/10.1086/498440   Schrijver, C.J. et al. 2015, A dv Space Res., 55, 2745  https://doi.org/10.1016/j.asr.2015.03.023   Tripathi, D., Nived, V. N. and Solanki, S. K. 2021, ApJ, 908, 28   https://doi.org/10.3847/1538 -4357/abcc6b   Tsuneta, S. et al. 2008, Astrophysical Journal, 668, 1374   https://doi.org/10.1086/592226   Upendran, V. and Tripathi, D. 2022, ApJ, 926, 138   https://doi.org/10.3847/1538 -4357/ac3d88   Usoskin, I.G. 2017, Living Rev Sol Phys, 14, 3   https://doi.org/10.1007/s41116 -017-0006 -9  Wang, Y. -M. et al. 2022, ApJ, 926, 113   https://doi.org/10.3847/1538 -4357/ac4491      "
