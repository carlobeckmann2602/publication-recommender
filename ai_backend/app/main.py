import asyncio
from asgiref.sync import sync_to_async
from fastapi import FastAPI, HTTPException, UploadFile
from contextlib import asynccontextmanager
from typing import List, Dict
from .util.misc import create_file_structure
from . import celery as tasks
import os
import aiofiles


@asynccontextmanager
async def lifespan(app: FastAPI):
    # Setup file structure
    create_file_structure(app.upload_path)
    yield


rec_api = FastAPI(
    title="HSD Publication Recommendation Engine",
    description="This is a beautiful description",
    version="0.2.0",
    lifespan=lifespan,
)
# Setup api parameters
rec_api.data_path = os.environ["DATA_PATH"]
rec_api.generated_data_path = f"{rec_api.data_path}/generated_data"
rec_api.upload_path = f"{rec_api.generated_data_path}/upload"


def task_to_async(task, queue: str = tasks.task_default_queue):
    task_timeout = 5 * 60 * 60

    async def wrapper(*args, **kwargs):
        delay = 0.1
        async_result = await sync_to_async(task.apply_async, thread_sensitive=True)(
            args=args, kwargs=kwargs, queue=queue
        )
        while not async_result.ready():
            if delay >= task_timeout:
                raise HTTPException(
                    status_code=404, detail=rf"{task.name} timed out in wait."
                )
            await asyncio.sleep(delay)
            delay = min(delay * 1.5, 2)  # exponential backoff, max 2 seconds
        return async_result.get(timeout=task_timeout)

    return wrapper


@rec_api.post("/summarize/")
async def summarize(file: UploadFile, amount=5, tokenize: bool | None = None):
    """
    Condenses a given textfile to the most important sentences contained in it.
    Also appends the calculated embeddings (vectors) for each sentence
    - **file**: The text file
    - **amount**: The amount of most important sentences
    - **tokenize**: If set to True or False, the underlying recommendation engine will ignore the current saved
                    settings and behave as set

    **return**: The summarization with pattern: {*n*: {token: str, embedding: [float]} *for n in amount*}
    """
    try:
        file_content = await read_file_in_chunks(
            file=file, as_file=file.filename, delete_buffer=True
        )
        return await summarize_text(file_content, amount, tokenize)
    except Exception as e:
        print(e)
        raise HTTPException(status_code=404, detail="Error while uploading the file")


@rec_api.get("/summarize/{text}/")
async def summarize_text(text: str, amount=5, tokenize: bool | None = None):
    """
    Condenses a given text to the most important sentences contained in it.
    Also appends the calculated embeddings (vectors) for each sentence
    - **text**: The text
    - **amount**: The amount of most important sentences
    - **tokenize**: If set to True or False, the underlying recommendation engine will ignore the current saved
                    settings and behave as set

    **return**: The summarization with pattern: {*n*: {token: str, embedding: [float]} *for n in amount*}
    """
    result = await task_to_async(tasks.summarize)(text, amount, tokenize)
    return result


@rec_api.get("/encode/{sentence}/")
async def encode_sentence(sentence: str):
    """
    Converts a given string into an embedding/vector generated by the sentence transformer
    - **sentence**: The given sentence to be vectorized

    **return**: The generated vector
    """
    result = await task_to_async(tasks.encode_sentence)(sentence)
    return result


@rec_api.post("/pca/")
async def encode_sentence(
    embeddings: List[List[float]] = [],
    component_amount: int = 3,
):
    """
    Runs principal component analysis for all given embeddings.
    It returns an embedding created from all used components times a random gaussian
    - **embeddings**: The embeddings/vectors for PCA
    - **component_amount**: The amount of PCs used for the vector generation

    **return**: The generated vector
    """
    if embeddings is None:
        return {}

    result = await task_to_async(tasks.run_pca)(embeddings, component_amount)
    return result


@rec_api.get("/build_tsne/")
async def build_tsne(
    amount: int = None, create_model: bool = False, latent_weight: float = 1
):
    """
    Forces to build new three dimensional coordinates
    - **amount**: Amount of publication from the backend used as basis for t-SNE
    - **create_model**: If set True also an VAE model will be trained based on the data
    - **latent_weight**: The weight the Latent Loss in the VAE model is wieghted
    """
    tasks.build_tsne.apply_async(args=[amount, create_model, latent_weight])
    return {"started": True}


@rec_api.post("/generate_coordinate/")
async def generate_coordinate(embedding: List[float] = []):
    """
    Runs variational autoencoder encode function for given embedding.
    It returns an 3D coordinate created from embedding
    - **embedding**: The embedding/vector of one publication (perform pca before on sentence embeddings)

    **return**: The generated coordinate
    """
    if embedding is None:
        return {}

    result = await task_to_async(tasks.generate_coordinate)(embedding)
    return result


@rec_api.post("/svm/")
async def run_svm(
    positive_embeddings: List[Dict[str, List[float] | str]],
    negative_embeddings: List[Dict[str, List[float] | str]],
    test_embeddings: List[Dict[str, List[float] | str]],
    amount: int = 5,
):
    """
    Trains and runs a support vector machine. The negative and positive embeddings are used for training.
    Then the test embeddings are used to run the SVM and <amount> embeddings are returned.
    - **positive_embeddings**: The embeddings/vectors used as a positive entry for the SVM
    - **negative_embeddings**: The embeddings/vectors used as a negative entry for the SVM
    - **test_embeddings**: The embeddings/vectors used as an input for the trained SVM
    - **amount**: The amount of embeddings/vectors returned.

    **return**: The top <amount> embeddings are returned ins desc order
    """
    if (
        positive_embeddings is None
        or negative_embeddings is None
        or test_embeddings is None
    ):
        return {}

    print(len(positive_embeddings))
    print(len(negative_embeddings))
    print(len(test_embeddings))

    result = await task_to_async(tasks.run_svm)(
        positive_embeddings, negative_embeddings, test_embeddings, amount
    )
    return result


async def read_file_in_chunks(
    file: UploadFile,
    as_file: str,
    delete_buffer=True,
    return_raw=False,
    destination=rec_api.upload_path,
) -> str | bytes:
    if not os.path.exists(destination):
        os.makedirs(destination)
    try:
        async with aiofiles.open(destination + "/" + as_file, "wb") as f:
            all_content = b""
            while contents := await file.read(1024 * 1024):
                await f.write(contents)
                all_content += contents
        if not return_raw:
            all_content = all_content.decode("utf-8")
        return all_content
    except Exception as e:
        raise e
    finally:
        await file.close()
        if delete_buffer:
            os.remove(destination + "/" + as_file)
